{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing :EMA and sensing data\n",
    "    - slice\n",
    "    - merge\n",
    "    - interpolate\n",
    "    - note: labeling in not included in this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process EMA data\n",
    "    - Original data in json file\n",
    "    - step 1 to slice json file to csv file by folders, slice by 24 hours, average activities\n",
    "    - step 2 is to concatenate all csv files into ema_all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA raw data: ['Activity', 'Administration response', 'Behavior', 'Boston Bombing', 'Cancelled Classes', 'Class', 'Class 2', 'Comment', 'Dartmouth now', 'Dimensions', 'Dimensions protestors', 'Dining Halls', 'Events', 'Exercise', 'Green Key 1', 'Green Key 2', 'Lab', 'Mood', 'Mood 1', 'Mood 2', 'PAM', 'QR_Code', 'Sleep', 'Social', 'Stress', 'Study Spaces']\n",
      "\n",
      "E.g., files in /EMA/response/Activity:  ['Activity_u00.json', 'Activity_u01.json', 'Activity_u02.json', 'Activity_u03.json', 'Activity_u04.json', 'Activity_u05.json', 'Activity_u07.json', 'Activity_u08.json', 'Activity_u09.json', 'Activity_u10.json', 'Activity_u12.json', 'Activity_u13.json', 'Activity_u14.json', 'Activity_u15.json', 'Activity_u16.json', 'Activity_u17.json', 'Activity_u18.json', 'Activity_u19.json', 'Activity_u20.json', 'Activity_u22.json', 'Activity_u23.json', 'Activity_u24.json', 'Activity_u25.json', 'Activity_u27.json', 'Activity_u30.json', 'Activity_u31.json', 'Activity_u32.json', 'Activity_u33.json', 'Activity_u34.json', 'Activity_u35.json', 'Activity_u36.json', 'Activity_u39.json', 'Activity_u41.json', 'Activity_u42.json', 'Activity_u43.json', 'Activity_u44.json', 'Activity_u45.json', 'Activity_u46.json', 'Activity_u47.json', 'Activity_u49.json', 'Activity_u50.json', 'Activity_u51.json', 'Activity_u52.json', 'Activity_u53.json', 'Activity_u54.json', 'Activity_u56.json', 'Activity_u57.json', 'Activity_u58.json', 'Activity_u59.json']\n"
     ]
    }
   ],
   "source": [
    "#  take a look the raw data structure in EMA\n",
    "import os\n",
    "dir_rawdata ='C:/study/studentlife/EMA/response/'  # put the raw data in this place, e.g. c:/study/studentlife\n",
    "folder_list = os.listdir(dir_rawdata)\n",
    "print('EMA raw data:', folder_list)\n",
    "print()\n",
    "print('E.g., files in /EMA/response/Activity: ', os.listdir(os.path.join(dir_rawdata, 'Activity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from util import stmp_to_fmt, fmt_to_stmp\n",
    "import numpy as np\n",
    "import random\n",
    "dir_data ='../data/'\n",
    "dir_code ='./'\n",
    "dir_tosave = '../data/ema'\n",
    "\n",
    "################ main function of average the acitivities/value by days#################\n",
    "## concatenate all candicates' acitibity/day by type of activity #########\n",
    "\n",
    "def count_ema_response(file_dir, folder_name): # count all columns exept ['location','resp_time']\n",
    "    count=0\n",
    "    dataset = pd.DataFrame() # creat a empty dataframe\n",
    "    file_list= sorted(os.listdir(file_dir))\n",
    "#     file_list=['Activity_u00.json']\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        user = re.findall('u[0-9][0-9]', file)[0]\n",
    "        data = pd.read_json(file_dir+file, convert_dates= False) # keep timestamp formate\n",
    "        data.fillna(0, inplace =True)\n",
    "        #print(file,' data sieze: ',data.shape)\n",
    "        if data.empty:\n",
    "            continue # if empty go to next file, note; use break will jump out of def\n",
    "        \n",
    "        try:\n",
    "            data= data.drop('location', axis =1) # filter location column\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        len=data.shape[0]\n",
    "        if np.any(data.isnull()):\n",
    "            data=data.dropna(axis=0) # filter null data\n",
    "        data=data.set_index('resp_time')\n",
    "        slice_width = 1*3600*24\n",
    "        for time in range(int(data.index[0]), int(data.index[-1])+1,slice_width):  # timestamp is index, +1 is for data with only one sample\n",
    "            #data_slice = data.loc[time:time+slice_width-1,:]  # data.loc: slice of both left and right boundary is included, so -1\n",
    "            slice_index= data.index.where((data.index>= time) & (data.index< time+slice_width)).dropna()\n",
    "            data_slice =data.loc[slice_index,:]\n",
    "#             acts=(data_slice.sum(axis =0))  # count the activities in the time slot\n",
    "            acts=(data_slice.mean(axis =0,skipna=True, numeric_only= True))  # AVERAGE the activities in the time slot\n",
    "            acts['user']=user\n",
    "            #acts['timestamp']=time \n",
    "#             print(type(user))\n",
    "            acts['user_date']=user+'_'+stmp_to_fmt(time)[:10] # pickup the yearMMDD\n",
    "            acts=acts.to_frame().T\n",
    " \n",
    "            dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "            #end of one file processed\n",
    "        \n",
    "        count+=1 # end of processing a file\n",
    "        # end of one folder processed\n",
    "    \n",
    "    dataset=dataset.set_index('user_date')\n",
    "    col=dataset.columns\n",
    "    new_col=folder_name+'_'+col\n",
    "    dataset.columns = new_col\n",
    "    print(\"Total \",count, \" files are processed, in the fold: \", folder_name )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/study/studentlife/EMA/response/Activity/ Activity\n",
      "Total  48  files are processed, in the fold:  Activity\n",
      "C:/study/studentlife/EMA/response/Administration response/ Administration response\n",
      "Total  26  files are processed, in the fold:  Administration response\n",
      "C:/study/studentlife/EMA/response/Behavior/ Behavior\n",
      "Total  47  files are processed, in the fold:  Behavior\n",
      "C:/study/studentlife/EMA/response/Boston Bombing/ Boston Bombing\n",
      "Total  11  files are processed, in the fold:  Boston Bombing\n",
      "C:/study/studentlife/EMA/response/Cancelled Classes/ Cancelled Classes\n",
      "Total  25  files are processed, in the fold:  Cancelled Classes\n",
      "C:/study/studentlife/EMA/response/Class/ Class\n",
      "Total  49  files are processed, in the fold:  Class\n",
      "C:/study/studentlife/EMA/response/Class 2/ Class 2\n",
      "Total  33  files are processed, in the fold:  Class 2\n",
      "C:/study/studentlife/EMA/response/Comment/ Comment\n",
      "Total  49  files are processed, in the fold:  Comment\n",
      "C:/study/studentlife/EMA/response/Dartmouth now/ Dartmouth now\n",
      "Total  21  files are processed, in the fold:  Dartmouth now\n",
      "C:/study/studentlife/EMA/response/Dimensions/ Dimensions\n",
      "Total  17  files are processed, in the fold:  Dimensions\n",
      "C:/study/studentlife/EMA/response/Dimensions protestors/ Dimensions protestors\n",
      "Total  22  files are processed, in the fold:  Dimensions protestors\n",
      "C:/study/studentlife/EMA/response/Dining Halls/ Dining Halls\n",
      "Total  34  files are processed, in the fold:  Dining Halls\n",
      "C:/study/studentlife/EMA/response/Events/ Events\n",
      "Total  38  files are processed, in the fold:  Events\n",
      "C:/study/studentlife/EMA/response/Exercise/ Exercise\n",
      "Total  46  files are processed, in the fold:  Exercise\n",
      "C:/study/studentlife/EMA/response/Green Key 1/ Green Key 1\n",
      "Total  17  files are processed, in the fold:  Green Key 1\n",
      "C:/study/studentlife/EMA/response/Green Key 2/ Green Key 2\n",
      "Total  20  files are processed, in the fold:  Green Key 2\n",
      "C:/study/studentlife/EMA/response/Lab/ Lab\n",
      "Total  38  files are processed, in the fold:  Lab\n",
      "C:/study/studentlife/EMA/response/Mood/ Mood\n",
      "Total  38  files are processed, in the fold:  Mood\n",
      "C:/study/studentlife/EMA/response/Mood 1/ Mood 1\n",
      "Total  40  files are processed, in the fold:  Mood 1\n",
      "C:/study/studentlife/EMA/response/Mood 2/ Mood 2\n",
      "Total  39  files are processed, in the fold:  Mood 2\n",
      "C:/study/studentlife/EMA/response/PAM/ PAM\n",
      "Total  49  files are processed, in the fold:  PAM\n",
      "C:/study/studentlife/EMA/response/QR_Code/ QR_Code\n",
      "Total  47  files are processed, in the fold:  QR_Code\n",
      "C:/study/studentlife/EMA/response/Sleep/ Sleep\n",
      "Total  49  files are processed, in the fold:  Sleep\n",
      "C:/study/studentlife/EMA/response/Social/ Social\n",
      "Total  49  files are processed, in the fold:  Social\n",
      "C:/study/studentlife/EMA/response/Stress/ Stress\n",
      "Total  49  files are processed, in the fold:  Stress\n",
      "C:/study/studentlife/EMA/response/Study Spaces/ Study Spaces\n",
      "Total  42  files are processed, in the fold:  Study Spaces\n",
      "Total  26  folders process!\n",
      "Done! Files saved in  :  ../data\n"
     ]
    }
   ],
   "source": [
    "##### A. generate dataset by types for all folders #######\n",
    "'''\n",
    "input: raw data in json format under studentlife/ema/response/\n",
    "output: csv files under ../data/ema/ folder \n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from util import stmp_to_fmt, fmt_to_stmp\n",
    "import numpy as np\n",
    "import random\n",
    "dir_data ='../data/'\n",
    "dir_code ='./'\n",
    "dir_tosave = '../data/ema'\n",
    "\n",
    "################ main function of average the acitivities/value by days#################\n",
    "## concatenate all candicates' acitibity/day by type of activity #########\n",
    "\n",
    "def count_ema_response(file_dir, folder_name): # count all columns exept ['location','resp_time']\n",
    "    count=0\n",
    "    dataset = pd.DataFrame() # creat a empty dataframe\n",
    "    file_list= sorted(os.listdir(file_dir))\n",
    "#     file_list=['Activity_u00.json']\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        user = re.findall('u[0-9][0-9]', file)[0]\n",
    "        data = pd.read_json(file_dir+file, convert_dates= False) # keep timestamp formate\n",
    "        data.fillna(0, inplace =True)\n",
    "        #print(file,' data sieze: ',data.shape)\n",
    "        if data.empty:\n",
    "            continue # if empty go to next file, note; use break will jump out of def\n",
    "        \n",
    "        try:\n",
    "            data= data.drop('location', axis =1) # filter location column\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        len=data.shape[0]\n",
    "        if np.any(data.isnull()):\n",
    "            data=data.dropna(axis=0) # filter null data\n",
    "        data=data.set_index('resp_time')\n",
    "        slice_width = 1*3600*24\n",
    "        for time in range(int(data.index[0]), int(data.index[-1])+1,slice_width):  # timestamp is index, +1 is for data with only one sample\n",
    "            #data_slice = data.loc[time:time+slice_width-1,:]  # data.loc: slice of both left and right boundary is included, so -1\n",
    "            slice_index= data.index.where((data.index>= time) & (data.index< time+slice_width)).dropna()\n",
    "            data_slice =data.loc[slice_index,:]\n",
    "#             acts=(data_slice.sum(axis =0))  # count the activities in the time slot\n",
    "            acts=(data_slice.mean(axis =0,skipna=True, numeric_only= True))  # AVERAGE the activities in the time slot\n",
    "            acts['user']=user\n",
    "            #acts['timestamp']=time \n",
    "#             print(type(user))\n",
    "            acts['user_date']=user+'_'+stmp_to_fmt(time)[:10] # pickup the yearMMDD\n",
    "            acts=acts.to_frame().T\n",
    " \n",
    "            dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "            #end of one file processed\n",
    "        \n",
    "        count+=1 # end of processing a file\n",
    "        # end of one folder processed\n",
    "    \n",
    "    dataset=dataset.set_index('user_date')\n",
    "    col=dataset.columns\n",
    "    new_col=folder_name+'_'+col\n",
    "    dataset.columns = new_col\n",
    "    print(\"Total \",count, \" files are processed, in the fold: \", folder_name )\n",
    "    return dataset\n",
    "\n",
    "dir_data = dir_rawdata\n",
    "ii=0\n",
    "folder_list= os.listdir(dir_data)\n",
    "for folder_name in folder_list: #'activity. behaviour, class ...'\n",
    "    file_dir = os.path.join(dir_data,folder_name)+'/' # e,g, 'C:/study/studentlife/EMA/response/Activity'\n",
    "    print(file_dir, folder_name)\n",
    "    data=count_ema_response(file_dir, folder_name)\n",
    "    data.to_csv(dir_tosave+folder_name+'.csv')\n",
    "    ii +=1\n",
    "\n",
    "print('Total ', ii, ' folders', 'process!')\n",
    "print(\"Done! Files saved in  : \", dir_tosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge success ! Data size of merged dataset is (49, 116)\n"
     ]
    }
   ],
   "source": [
    "########### B. concat all data in file ############\n",
    "'''\n",
    "input: csv files under ../data/ema folder \n",
    "output:  ema_all.csv file which contains all slices data of ema by users\n",
    "'''\n",
    "\n",
    "ema_files_position = dir_tosave\n",
    "files = os.listdir(ema_files_position)\n",
    "# files = ['Activity.csv','Class.csv'] # for testing\n",
    "data_all = pd.DataFrame()\n",
    "for file in files:\n",
    "    this_data= pd.read_csv(os.path.join(ema_files_position,file), index_col=0)\n",
    "    data_all= data_all.merge(this_data, how='outer', left_index=True, right_index=True )\n",
    "print('Merge success ! Data size of merged dataset is', data_all.shape )\n",
    "dir_tosave_ema = '../data' # home win10\n",
    "data_all.to_csv(dir_tosave_ema+ 'ema_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process sensing data\n",
    "    - slice, sum/calculate duration, concatenate\n",
    "    - process 'activity' and 'audeo' by sum\n",
    "    - process 'conversation','dark','phonecharge','phonelock' by calculating diration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensing raw data: ['activity', 'audio', 'conversation', 'dark', 'phonecharge', 'phonelock', 'sensing_data_info_summary.csv']\n",
      "\n",
      "E.g., files in /Sensing/Audio/:  ['audio_u00.csv', 'audio_u01.csv', 'audio_u02.csv', 'audio_u03.csv', 'audio_u04.csv', 'audio_u05.csv', 'audio_u07.csv', 'audio_u08.csv', 'audio_u09.csv', 'audio_u10.csv', 'audio_u12.csv', 'audio_u13.csv', 'audio_u14.csv', 'audio_u15.csv', 'audio_u16.csv', 'audio_u17.csv', 'audio_u18.csv', 'audio_u19.csv', 'audio_u20.csv', 'audio_u22.csv', 'audio_u23.csv', 'audio_u24.csv', 'audio_u25.csv', 'audio_u27.csv', 'audio_u30.csv', 'audio_u31.csv', 'audio_u32.csv', 'audio_u33.csv', 'audio_u34.csv', 'audio_u35.csv', 'audio_u36.csv', 'audio_u39.csv', 'audio_u41.csv', 'audio_u42.csv', 'audio_u43.csv', 'audio_u44.csv', 'audio_u45.csv', 'audio_u46.csv', 'audio_u47.csv', 'audio_u49.csv', 'audio_u50.csv', 'audio_u51.csv', 'audio_u52.csv', 'audio_u53.csv', 'audio_u54.csv', 'audio_u56.csv', 'audio_u57.csv', 'audio_u58.csv', 'audio_u59.csv']\n"
     ]
    }
   ],
   "source": [
    "#  take a look at the raw data structure in Sensing\n",
    "import os\n",
    "dir_rawdata ='C:/study/studentlife/sensing/' # put the raw data in this place, e.g. c:/study/studentlife\n",
    "folder_list = os.listdir(dir_rawdata)\n",
    "print('Sensing raw data:', folder_list)\n",
    "print()\n",
    "print('E.g., files in /Sensing/Audio/: ', os.listdir(os.path.join(dir_rawdata, 'audio')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/study/studentlife/sensing/activity/\n",
      "C:/study/studentlife/sensing/audio/\n",
      "C:/study/studentlife/sensing/conversation/\n",
      "C:/study/studentlife/sensing/dark/\n",
      "C:/study/studentlife/sensing/phonecharge/\n",
      "C:/study/studentlife/sensing/phonelock/\n",
      "total files:  294\n",
      "['activity', 'audio', 'conversation', 'dark', 'phonecharge', 'phonelock']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file name</th>\n",
       "      <th>userID</th>\n",
       "      <th>columns</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>duration(days)</th>\n",
       "      <th>instance_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activity_u00.csv</td>\n",
       "      <td>u00</td>\n",
       "      <td>timestamp, activity inference</td>\n",
       "      <td>2013-03-27 00:00:01</td>\n",
       "      <td>2013-05-31 21:03:33</td>\n",
       "      <td>65</td>\n",
       "      <td>461991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>activity_u01.csv</td>\n",
       "      <td>u01</td>\n",
       "      <td>timestamp, activity inference</td>\n",
       "      <td>2013-03-27 00:01:39</td>\n",
       "      <td>2013-05-29 09:32:25</td>\n",
       "      <td>63</td>\n",
       "      <td>474862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>activity_u02.csv</td>\n",
       "      <td>u02</td>\n",
       "      <td>timestamp, activity inference</td>\n",
       "      <td>2013-03-27 00:01:48</td>\n",
       "      <td>2013-05-31 23:42:22</td>\n",
       "      <td>65</td>\n",
       "      <td>527518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>activity_u03.csv</td>\n",
       "      <td>u03</td>\n",
       "      <td>timestamp, activity inference</td>\n",
       "      <td>2013-03-27 00:02:13</td>\n",
       "      <td>2013-05-22 18:44:33</td>\n",
       "      <td>56</td>\n",
       "      <td>411402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>activity_u04.csv</td>\n",
       "      <td>u04</td>\n",
       "      <td>timestamp, activity inference</td>\n",
       "      <td>2013-03-27 00:02:57</td>\n",
       "      <td>2013-05-23 08:28:37</td>\n",
       "      <td>57</td>\n",
       "      <td>462516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>phonelock_u54.csv</td>\n",
       "      <td>u54</td>\n",
       "      <td>start,end</td>\n",
       "      <td>2013-03-27 18:51:36</td>\n",
       "      <td>2013-05-30 05:16:36</td>\n",
       "      <td>63</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>phonelock_u56.csv</td>\n",
       "      <td>u56</td>\n",
       "      <td>start,end</td>\n",
       "      <td>2013-03-27 00:18:53</td>\n",
       "      <td>2013-05-31 12:46:25</td>\n",
       "      <td>65</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>phonelock_u57.csv</td>\n",
       "      <td>u57</td>\n",
       "      <td>start,end</td>\n",
       "      <td>2013-03-27 00:00:05</td>\n",
       "      <td>2013-05-31 16:47:29</td>\n",
       "      <td>65</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>phonelock_u58.csv</td>\n",
       "      <td>u58</td>\n",
       "      <td>start,end</td>\n",
       "      <td>2013-03-27 00:07:50</td>\n",
       "      <td>2013-05-29 07:51:03</td>\n",
       "      <td>63</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>phonelock_u59.csv</td>\n",
       "      <td>u59</td>\n",
       "      <td>start,end</td>\n",
       "      <td>2013-03-27 02:21:18</td>\n",
       "      <td>2013-05-31 03:19:50</td>\n",
       "      <td>65</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             file name userID                        columns  \\\n",
       "0     activity_u00.csv    u00  timestamp, activity inference   \n",
       "1     activity_u01.csv    u01  timestamp, activity inference   \n",
       "2     activity_u02.csv    u02  timestamp, activity inference   \n",
       "3     activity_u03.csv    u03  timestamp, activity inference   \n",
       "4     activity_u04.csv    u04  timestamp, activity inference   \n",
       "..                 ...    ...                            ...   \n",
       "289  phonelock_u54.csv    u54                      start,end   \n",
       "290  phonelock_u56.csv    u56                      start,end   \n",
       "291  phonelock_u57.csv    u57                      start,end   \n",
       "292  phonelock_u58.csv    u58                      start,end   \n",
       "293  phonelock_u59.csv    u59                      start,end   \n",
       "\n",
       "              start_time             end_time duration(days) instance_no  \n",
       "0    2013-03-27 00:00:01  2013-05-31 21:03:33             65      461991  \n",
       "1    2013-03-27 00:01:39  2013-05-29 09:32:25             63      474862  \n",
       "2    2013-03-27 00:01:48  2013-05-31 23:42:22             65      527518  \n",
       "3    2013-03-27 00:02:13  2013-05-22 18:44:33             56      411402  \n",
       "4    2013-03-27 00:02:57  2013-05-23 08:28:37             57      462516  \n",
       "..                   ...                  ...            ...         ...  \n",
       "289  2013-03-27 18:51:36  2013-05-30 05:16:36             63         165  \n",
       "290  2013-03-27 00:18:53  2013-05-31 12:46:25             65         137  \n",
       "291  2013-03-27 00:00:05  2013-05-31 16:47:29             65         292  \n",
       "292  2013-03-27 00:07:50  2013-05-29 07:51:03             63         266  \n",
       "293  2013-03-27 02:21:18  2013-05-31 03:19:50             65         145  \n",
       "\n",
       "[294 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### 1. take a look at the whole data info in \"sensing\" folder #######\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import util\n",
    "import numpy as np\n",
    "from util import stmp_to_fmt, fmt_to_stmp\n",
    "'''\n",
    "input: raw data in json format under studentlife/sensing/\n",
    "output: csv files under ../data/sensing/ folder \n",
    "'''\n",
    "\n",
    "\n",
    "dir_rawdata ='C:/study/studentlife/sensing/' # put the raw data in this place, e.g. c:/study/studentlife\n",
    "dir_code = './'\n",
    "dir_tosave = '../data/sensing/' \n",
    "dir_data = dir_rawdata\n",
    "\n",
    "def get_file_profile(sub_folder_dir,  file_name):\n",
    "    dataset = pd.DataFrame() # creat a empty dataframe\n",
    "    user = re.findall('u[0-9][0-9]', file_name)\n",
    "    data = pd.read_csv(sub_folder_dir+file_name)\n",
    "    col=list(data.columns)\n",
    "    time = data.iloc[:,0]\n",
    "    data_size = data.shape[0]\n",
    "    days = int((data.iloc[-1,0]-data.iloc[0,0])/3600/24)\n",
    "\n",
    "#     data_info_all =pd.DataFrame(columns =['file name','userID','columns','start_time','end_time','duration(days)','instance_no'])\n",
    "    data_info= [file_name]+user+[','.join(col[:])]+[stmp_to_fmt(int(data.iloc[0,0]))]+[stmp_to_fmt(int(data.iloc[-1,0]))]+[days]+[data_size]\n",
    "    return data_info\n",
    "\n",
    "sub_folder_list =[]\n",
    "folder_list= os.listdir(dir_data)\n",
    "\n",
    "i=0\n",
    "data_info_all =pd.DataFrame(columns =['file name','userID','columns','start_time','end_time','duration(days)','instance_no'])\n",
    "for sub_folder_name in folder_list:  # folder of sensing\n",
    "    sub_folder_list = sub_folder_list+[sub_folder_name]\n",
    "    sub_folder_dir = os.path.join(dir_data,sub_folder_name)+'/'\n",
    "    print(sub_folder_dir)\n",
    "    file_list = os.listdir(sub_folder_dir)\n",
    "    for file_name in file_list:\n",
    "        data_info= get_file_profile(sub_folder_dir,  file_name)\n",
    "        a_series = pd.Series(data_info, index = data_info_all.columns)\n",
    "        data_info_all=data_info_all.append(a_series, ignore_index=True)\n",
    "        i+=1\n",
    "#         print(file_name)\n",
    "\n",
    "print('total files: ',i)\n",
    "print(sub_folder_list)\n",
    "data_info_all.to_csv(dir_data+\"sensing_data_info_summary.csv\")\n",
    "data_info_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 1.2  create a data dictionary for slicing ###########\n",
    "# start_date = '2013-03-26 00:00:00'\n",
    "# end_date ='2013-05-31 23:59:59'\n",
    "# start_date_stmp= fmt_to_stmp(start_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "# end_date_stmp= fmt_to_stmp(end_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "# # print(start_date_stmp, ' to ',end_date_stmp)\n",
    "\n",
    "# date_dict={}\n",
    "# i=0\n",
    "# for time_stmp in range(start_date_stmp, end_date_stmp, 3600*24):\n",
    "#     date=stmp_to_fmt(time_stmp)\n",
    "#     i+=1\n",
    "#     date_dict[date]=time_stmp, time_stmp+24*3600-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  2.1. process(slice) activity's and audio's: sum activities ######\n",
    "########### not include wifi , bluetooth, gps. wifi_location#########\n",
    "'''\n",
    "input: csv files under ../data/sensing/audio/ folder \n",
    "output: audio.csv\n",
    "'''\n",
    "def act_count_sliced(dir,file):\n",
    "    user = re.findall('u[0-9][0-9]', file)[0]\n",
    "    file= dir+file\n",
    "    data= pd.read_csv(file, index_col= 0) # index_col= 0: use the first column as the index\n",
    "#     data.fillna(0, inplace =True)  # if there is null in data set , add this\n",
    "    if np.any(data.isnull()):\n",
    "        data=data.dropna(axis=0) # filter null data\n",
    "    \n",
    "    slice_width = 1*3600*24\n",
    "#     time_list=[]\n",
    "    dataset=pd.DataFrame()\n",
    "    for time in range(data.index[0], data.index[-1]+1,slice_width):  # timestamp is index\n",
    "    #     time_list.append(time)\n",
    "        data_slot = data.loc[time:time+slice_width-1,:]  #note: data.loc: slice of both left and right boundary is included, so -1\n",
    "        acts=(data_slot.sum(axis =0))  # count the activities in the time slot\n",
    "        \n",
    "        #acts['timestamp']=time \n",
    "        acts['user']=user\n",
    "        acts['user_date']=user+'_'+stmp_to_fmt(time)[:10]\n",
    "        acts=acts.to_frame().T\n",
    "        dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "        #end of one file processed\n",
    "        \n",
    "    dataset=dataset.set_index('user_date')\n",
    "    dataset=dataset.fillna(0)\n",
    "#     print(acts)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# dataset =pd.DataFrame()\n",
    "# dir_temp=(dir_data+'activity/')\n",
    "# file= 'activity_u00.csv'\n",
    "# data_s = act_count_sliced(dir_temp,file)\n",
    "# dataset = pd.concat([dataset,data_s], axis =0) \n",
    "# dataset\n",
    "\n",
    "dataset =pd.DataFrame()\n",
    "file_dir = dir_data+'activity/'\n",
    "file_list= sorted(os.listdir(file_dir))\n",
    "print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "print(\"counting........\")\n",
    "for file in file_list:\n",
    "    data_= act_count_sliced(file_dir, file)\n",
    "    dataset = pd.concat([dataset,data_], axis =0) \n",
    "col=dataset.columns\n",
    "new_col=folder+'_'+col\n",
    "dataset.columns = new_col\n",
    "print(\"done!\")\n",
    "dataset.to_csv(dir_tosave+'activity.csv')\n",
    "\n",
    "dataset =pd.DataFrame()\n",
    "file_dir = dir_data+'audio/'\n",
    "file_list= sorted(os.listdir(file_dir))\n",
    "print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "print(\"counting........\")\n",
    "for file in file_list:\n",
    "    data_= act_count_sliced(file_dir, file)\n",
    "    dataset = pd.concat([dataset,data_], axis =0) \n",
    "col=dataset.columns\n",
    "new_col=folder+'_'+col\n",
    "dataset.columns = new_col\n",
    "print(\"done!\")\n",
    "dataset.to_csv(dir_tosave+'audio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########  2.2 process (slice) 'conversation','dark','phonecharge','phonelock': calculate duration  ################\n",
    "'''\n",
    "input: csv files under ../data/sensing/'conversation','dark','phonecharge','phonelock' folder \n",
    "output: csv files of 'conversation','dark','phonecharge','phonelock\n",
    "'''\n",
    "def duration_count_sliced(dir,file): #for col 0 is stat_time, col 1 is end_time\n",
    "    user = re.findall('u[0-9][0-9]', file)[0]\n",
    "    file= dir+file \n",
    "    data= pd.read_csv(file, index_col=0) # index_col= 0: use the first column as the index\n",
    "    if np.any(data.isnull()):\n",
    "        data=data.dropna(axis=0) # filter null data\n",
    "    duration = data.iloc[:,0]-data.index # end_time - start_time\n",
    "    data.insert(1,'duration(sec)',duration) # add column \n",
    "    slice_width = 1*3600*24\n",
    "\n",
    "#     time_list=[]\n",
    "    dataset=pd.DataFrame()\n",
    "    for time in range(int(data.index[0]), int(data.index[-1])+1,slice_width):  # timestamp is index, +1 is for data with only one sample\n",
    "#         data_slice = data.loc[time:time+slice_width-1,:]  # data.loc: slice of both left and right boundary is included, so -1\n",
    "        slice_index= data.index.where((data.index>= time) & (data.index< time+slice_width)).dropna()\n",
    "        data_slice =data.loc[slice_index,:]\n",
    "\n",
    "        acts=(data_slice.iloc[:,1:].sum(axis =0))  # calculate the total duration in the sliced period, after column 1\n",
    "        #acts['timestamp']=time \n",
    "        acts['user']=user\n",
    "        acts['user_date']=user+'_'+stmp_to_fmt(time)[:10]        \n",
    "        acts=acts.to_frame().T\n",
    "    #     time_list.append(time)\n",
    "        dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "    dataset=dataset.set_index('user_date')\n",
    "    dataset=dataset.fillna(0)\n",
    "    return dataset\n",
    "\n",
    "# dataset =pd.DataFrame()\n",
    "# dir_temp=(dir_data+'phonecharge/')\n",
    "# file= 'phonecharge_u39.csv'\n",
    "\n",
    "# data_s = duration_count_sliced(dir_temp,file)\n",
    "# dataset = pd.concat([dataset,data_s], axis =0) \n",
    "# dataset\n",
    "\n",
    "folder_list= ['conversation','dark','phonecharge','phonelock']\n",
    "# folder_list= ['phonelock']\n",
    "for folder in folder_list:\n",
    "    dataset =pd.DataFrame()\n",
    "    file_dir = dir_data+folder+'/'\n",
    "    file_list= sorted(os.listdir(file_dir))\n",
    "    print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "    print(\"counting........\")\n",
    "    for file in file_list:\n",
    "#         print(file)\n",
    "        data_= duration_count_sliced(file_dir, file)\n",
    "        dataset = pd.concat([dataset,data_], axis =0) \n",
    "    col=dataset.columns\n",
    "    new_col=folder+'_'+col\n",
    "    dataset.columns = new_col\n",
    "    print(\"done!\")\n",
    "    dataset.to_csv(dir_tosave+folder+'.csv')\n",
    "    \n",
    "##### seperately generate dataset by types for all folders, done ! #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 3. concatenate all sensing data in one file ############\n",
    "'''\n",
    "input: csv files of 'activity', audio', 'conversation','dark','phonecharge','phonelock'\n",
    "output:sensing_all.csv\n",
    "'''\n",
    "files_position = dir_tosave #'C:/study/Dropbox/research/depression/data/studentlife/sensing/'\n",
    "files = os.listdir(files_position) #['Activity.csv','Class.csv', 'Class 2.csv',...]\n",
    "# files = ['Activity.csv','Class.csv', 'Class 2.csv'] # for testing\n",
    "data_all = pd.DataFrame()\n",
    "for file in files:\n",
    "#     print(file)\n",
    "    this_data= pd.read_csv(files_position+file, index_col='user_date')\n",
    "#     print(this_data)\n",
    "#     data_all=pd.concat([data_all,this_data], axis=1,ignore_index=False)\n",
    "    data_all= data_all.merge(this_data, how='outer', left_index=True, right_index=True )\n",
    "print('Merge success ! Data size of merged dataset is', data_all.shape )\n",
    "data_all\n",
    "dir_tosave_sensing= '../data/'\n",
    "data_all.to_csv(dir_tosave_sensing+ 'sensing_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge EMA and sensing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset_ema shape:  (3025, 107)\n",
      "2. Dataset_sensing shape:  (3075, 12)\n",
      "3. Merge success ! shape of merged dataset is (3407, 119)\n",
      "\n",
      "4.1 Removed columns of non-data, merged dataset shape:  (3407, 87)\n",
      "4.2 Removed columns of non-data, EMA dataset shape:  (3025, 81)\n",
      "4.2 Removed columns of non-data, sensing dataset shape:  (3075, 6)\n"
     ]
    }
   ],
   "source": [
    "#################  1.1 merge sensing and ema  ####################\n",
    "######### ema_all_v5.csv, sensing_all_v2.csv  ###################\n",
    "\n",
    "dir_data = '../data/processed_data/'\n",
    "\n",
    "dataset_ema = pd.read_csv(dir_data+ 'ema_all_v5.csv', index_col=0)  #v2 is the sliced dataset\n",
    "print ('1. Dataset_ema shape: ',dataset_ema.shape)\n",
    "\n",
    "dataset_sensing = pd.read_csv(dir_data+ 'sensing_all_v2.csv', index_col=0)  #v2 is the sliced dataset\n",
    "print ('2. Dataset_sensing shape: ',dataset_sensing.shape)\n",
    "\n",
    "######### clean data ###########\n",
    "dataset= dataset_sensing.merge(dataset_ema, how='outer', left_index=True, right_index=True )\n",
    "print('3. Merge success ! shape of merged dataset is', dataset.shape )\n",
    "\n",
    "def drop_notnum(dataset):\n",
    "    cols=dataset.columns\n",
    "    col_notnum=[]\n",
    "    for col in cols:\n",
    "        if str(dataset[col].dtype)=='object':\n",
    "               col_notnum.append(col)\n",
    "    # print(col_notnum)\n",
    "    dataset=dataset.drop(col_notnum, axis=1)\n",
    "    return dataset\n",
    "\n",
    "dataset=drop_notnum(dataset)  #abc=dataset.dropna(axis=1, how='all') # drops columns those with all NaN\n",
    "dataset_ema= drop_notnum(dataset_ema)\n",
    "dataset_sensing = drop_notnum(dataset_sensing)\n",
    "print('')\n",
    "print('4.1 Removed columns of non-data, merged dataset shape: ', dataset.shape)\n",
    "print('4.2 Removed columns of non-data, EMA dataset shape: ', dataset_ema.shape)\n",
    "print('4.2 Removed columns of non-data, sensing dataset shape: ', dataset_sensing.shape)\n",
    "\n",
    "dataset.to_csv(dir_data+\"studentlife_data_full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
