{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing :EMA and sensing data\n",
    "    - slice\n",
    "    - merge\n",
    "    - interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "dir_data ='../data/processed_data/'\n",
    "dir_code ='./'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process EMA data\n",
    "    - Original data in json file\n",
    "    - step 1 to slice json file to csv file by folders, slice by 24 hours, average activities\n",
    "    - step 2 is to concatenate all csv files into ema_all_v5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from util import stmp_to_fmt, fmt_to_stmp\n",
    "\n",
    "# dir_data='D:/research/dataset/studentlife/EMA/response/' # lab win10\n",
    "dir_data= 'C:/study/studentlife/EMA/response/' #home win10\n",
    "\n",
    "dir_code = './'\n",
    "dir_tosave = '../data/processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ main function of average the acitivities/value by days#################\n",
    "## concatenate all candicates' acitibity/day by type of activity #########\n",
    "import numpy as np\n",
    "def count_ema_response(file_dir, folder_name): # count all columns exept ['location','resp_time']\n",
    "    count=0\n",
    "    dataset = pd.DataFrame() # creat a empty dataframe\n",
    "    file_list= sorted(os.listdir(file_dir))\n",
    "#     file_list=['Activity_u00.json']\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        user = re.findall('u[0-9][0-9]', file)[0]\n",
    "        data = pd.read_json(file_dir+file, convert_dates= False) # keep timestamp formate\n",
    "        data.fillna(0, inplace =True)\n",
    "        #print(file,' data sieze: ',data.shape)\n",
    "        if data.empty:\n",
    "            continue # if empty go to next file, note; use break will jump out of def\n",
    "        \n",
    "        try:\n",
    "            data= data.drop('location', axis =1) # filter location column\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        len=data.shape[0]\n",
    "        if np.any(data.isnull()):\n",
    "            data=data.dropna(axis=0) # filter null data\n",
    "        data=data.set_index('resp_time')\n",
    "        slice_width = 1*3600*24\n",
    "        for time in range(int(data.index[0]), int(data.index[-1])+1,slice_width):  # timestamp is index, +1 is for data with only one sample\n",
    "            #data_slice = data.loc[time:time+slice_width-1,:]  # data.loc: slice of both left and right boundary is included, so -1\n",
    "            slice_index= data.index.where((data.index>= time) & (data.index< time+slice_width)).dropna()\n",
    "            data_slice =data.loc[slice_index,:]\n",
    "#             acts=(data_slice.sum(axis =0))  # count the activities in the time slot\n",
    "            acts=(data_slice.mean(axis =0,skipna=True, numeric_only= True))  # AVERAGE the activities in the time slot\n",
    "            acts['user']=user\n",
    "            #acts['timestamp']=time \n",
    "#             print(type(user))\n",
    "            acts['user_date']=user+'_'+stmp_to_fmt(time)[:10] # pickup the yearMMDD\n",
    "            acts=acts.to_frame().T\n",
    " \n",
    "            dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "            #end of one file processed\n",
    "        \n",
    "        count+=1 # end of processing a file\n",
    "        # end of one folder processed\n",
    "    \n",
    "    dataset=dataset.set_index('user_date')\n",
    "    col=dataset.columns\n",
    "    new_col=folder_name+'_'+col\n",
    "    dataset.columns = new_col\n",
    "    print(\"Total \",count, \" files are processed, in the fold: \", folder_name )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A. generate dataset by types for all folders #######\n",
    "import os\n",
    "\n",
    "ii=0\n",
    "folder_list= os.listdir(dir_data)\n",
    "for folder_name in folder_list: #'activity. behaviour, class ...'\n",
    "    file_dir = dir_data+folder_name+'/' # e,g, 'C:/study/studentlife/EMA/response/Activity'\n",
    "    data=count_ema_response(file_dir, folder_name)\n",
    "    data.to_csv(dir_tosave+folder_name+'.csv')\n",
    "    ii +=1\n",
    "\n",
    "print('Total ', ii, ' folders', 'process!')\n",
    "print(\"Done! Files saved in  : \", dir_tosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### B. concat all data in file ############\n",
    "ema_files_position = dir_tosave\n",
    "files = os.listdir(ema_files_position)\n",
    "# files = ['Activity.csv','Class.csv'] # for testing\n",
    "data_all = pd.DataFrame()\n",
    "for file in files:\n",
    "    this_data= pd.read_csv(ema_files_position+file, index_col=0)\n",
    "    data_all= data_all.merge(this_data, how='outer', left_index=True, right_index=True )\n",
    "print('Merge success ! Data size of merged dataset is', data_all.shape )\n",
    "dir_tosave_ema = '../data/processed_data/' # home win10\n",
    "data_all.to_csv(dir_tosave_ema+ 'ema_all_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 0. activity ###################\n",
    "# folder_name='Activity'\n",
    "# file_dir = dir_data+folder_name+'/'\n",
    "# count_ema_response(file_dir, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process sensing data\n",
    "    - slice, sum/calculate duration, concatenate\n",
    "    - process 'activity' and 'audeo' by sum\n",
    "    - process 'conversation','dark','phonecharge','phonelock' by calculating diration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import util\n",
    "import numpy as np\n",
    "from util import stmp_to_fmt, fmt_to_stmp\n",
    "\n",
    "\n",
    "dir_data= 'C:/study/studentlife/sensing/' #home \n",
    "dir_code = './'\n",
    "dir_tosave = '../data/processed_data/sensing/' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. take a look at the whole data info in \"sensing\" folder #######\n",
    "import os\n",
    "\n",
    "def get_file_profile(sub_folder_dir,  file_name):\n",
    "    dataset = pd.DataFrame() # creat a empty dataframe\n",
    "    user = re.findall('u[0-9][0-9]', file_name)\n",
    "    data = pd.read_csv(sub_folder_dir+file_name)\n",
    "    col=list(data.columns)\n",
    "    time = data.iloc[:,0]\n",
    "    data_size = data.shape[0]\n",
    "    days = int((data.iloc[-1,0]-data.iloc[0,0])/3600/24)\n",
    "\n",
    "#     data_info_all =pd.DataFrame(columns =['file name','userID','columns','start_time','end_time','duration(days)','instance_no'])\n",
    "    data_info= [file_name]+user+[','.join(col[:])]+[stmp_to_fmt(int(data.iloc[0,0]))]+[stmp_to_fmt(int(data.iloc[-1,0]))]+[days]+[data_size]\n",
    "    return data_info\n",
    "\n",
    "sub_folder_list =[]\n",
    "folder_list= os.listdir(dir_data)\n",
    "\n",
    "i=0\n",
    "data_info_all =pd.DataFrame(columns =['file name','userID','columns','start_time','end_time','duration(days)','instance_no'])\n",
    "for sub_folder_name in folder_list:  # folder of sensing\n",
    "    sub_folder_list = sub_folder_list+[sub_folder_name]\n",
    "    sub_folder_dir = dir_data+sub_folder_name+'/'\n",
    "    file_list = os.listdir(sub_folder_dir)\n",
    "    for file_name in file_list:\n",
    "        data_info= get_file_profile(sub_folder_dir,  file_name)\n",
    "        a_series = pd.Series(data_info, index = data_info_all.columns)\n",
    "        data_info_all=data_info_all.append(a_series, ignore_index=True)\n",
    "        i+=1\n",
    "#         print(file_name)\n",
    "\n",
    "print('total files: ',i)\n",
    "print(sub_folder_list)\n",
    "# data_info_all.to_csv(dir_data+\"sensing_data_info_summary.csv\")\n",
    "data_info_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 1.2  create a data dictionary for slicing ###########\n",
    "# start_date = '2013-03-26 00:00:00'\n",
    "# end_date ='2013-05-31 23:59:59'\n",
    "# start_date_stmp= fmt_to_stmp(start_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "# end_date_stmp= fmt_to_stmp(end_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "# # print(start_date_stmp, ' to ',end_date_stmp)\n",
    "\n",
    "# date_dict={}\n",
    "# i=0\n",
    "# for time_stmp in range(start_date_stmp, end_date_stmp, 3600*24):\n",
    "#     date=stmp_to_fmt(time_stmp)\n",
    "#     i+=1\n",
    "#     date_dict[date]=time_stmp, time_stmp+24*3600-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  2.1. process(slice) activity's and audio's: sum activities ######\n",
    "########### not include wifi , bluetooth, gps. wifi_location#########\n",
    "\n",
    "def act_count_sliced(dir,file):\n",
    "    user = re.findall('u[0-9][0-9]', file)[0]\n",
    "    file= dir+file\n",
    "    data= pd.read_csv(file, index_col= 0) # index_col= 0: use the first column as the index\n",
    "#     data.fillna(0, inplace =True)  # if there is null in data set , add this\n",
    "    if np.any(data.isnull()):\n",
    "        data=data.dropna(axis=0) # filter null data\n",
    "    \n",
    "    slice_width = 1*3600*24\n",
    "#     time_list=[]\n",
    "    dataset=pd.DataFrame()\n",
    "    for time in range(data.index[0], data.index[-1]+1,slice_width):  # timestamp is index\n",
    "    #     time_list.append(time)\n",
    "        data_slot = data.loc[time:time+slice_width-1,:]  #note: data.loc: slice of both left and right boundary is included, so -1\n",
    "        acts=(data_slot.sum(axis =0))  # count the activities in the time slot\n",
    "        \n",
    "        #acts['timestamp']=time \n",
    "        acts['user']=user\n",
    "        acts['user_date']=user+'_'+stmp_to_fmt(time)[:10]\n",
    "        acts=acts.to_frame().T\n",
    "        dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "        #end of one file processed\n",
    "        \n",
    "    dataset=dataset.set_index('user_date')\n",
    "    dataset=dataset.fillna(0)\n",
    "#     print(acts)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# dataset =pd.DataFrame()\n",
    "# dir_temp=(dir_data+'activity/')\n",
    "# file= 'activity_u00.csv'\n",
    "# data_s = act_count_sliced(dir_temp,file)\n",
    "# dataset = pd.concat([dataset,data_s], axis =0) \n",
    "# dataset\n",
    "\n",
    "dataset =pd.DataFrame()\n",
    "file_dir = dir_data+'activity/'\n",
    "file_list= sorted(os.listdir(file_dir))\n",
    "print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "print(\"counting........\")\n",
    "for file in file_list:\n",
    "    data_= act_count_sliced(file_dir, file)\n",
    "    dataset = pd.concat([dataset,data_], axis =0) \n",
    "col=dataset.columns\n",
    "new_col=folder+'_'+col\n",
    "dataset.columns = new_col\n",
    "print(\"done!\")\n",
    "dataset.to_csv(dir_tosave+'activity.csv')\n",
    "\n",
    "dataset =pd.DataFrame()\n",
    "file_dir = dir_data+'audio/'\n",
    "file_list= sorted(os.listdir(file_dir))\n",
    "print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "print(\"counting........\")\n",
    "for file in file_list:\n",
    "    data_= act_count_sliced(file_dir, file)\n",
    "    dataset = pd.concat([dataset,data_], axis =0) \n",
    "col=dataset.columns\n",
    "new_col=folder+'_'+col\n",
    "dataset.columns = new_col\n",
    "print(\"done!\")\n",
    "dataset.to_csv(dir_tosave+'audio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########  2.2 process (slice) 'conversation','dark','phonecharge','phonelock': calculate duration  ################\n",
    "def duration_count_sliced(dir,file): #for col 0 is stat_time, col 1 is end_time\n",
    "    user = re.findall('u[0-9][0-9]', file)[0]\n",
    "    file= dir+file \n",
    "    data= pd.read_csv(file, index_col=0) # index_col= 0: use the first column as the index\n",
    "    if np.any(data.isnull()):\n",
    "        data=data.dropna(axis=0) # filter null data\n",
    "    duration = data.iloc[:,0]-data.index # end_time - start_time\n",
    "    data.insert(1,'duration(sec)',duration) # add column \n",
    "    slice_width = 1*3600*24\n",
    "\n",
    "#     time_list=[]\n",
    "    dataset=pd.DataFrame()\n",
    "    for time in range(int(data.index[0]), int(data.index[-1])+1,slice_width):  # timestamp is index, +1 is for data with only one sample\n",
    "#         data_slice = data.loc[time:time+slice_width-1,:]  # data.loc: slice of both left and right boundary is included, so -1\n",
    "        slice_index= data.index.where((data.index>= time) & (data.index< time+slice_width)).dropna()\n",
    "        data_slice =data.loc[slice_index,:]\n",
    "\n",
    "        acts=(data_slice.iloc[:,1:].sum(axis =0))  # calculate the total duration in the sliced period, after column 1\n",
    "        #acts['timestamp']=time \n",
    "        acts['user']=user\n",
    "        acts['user_date']=user+'_'+stmp_to_fmt(time)[:10]        \n",
    "        acts=acts.to_frame().T\n",
    "    #     time_list.append(time)\n",
    "        dataset=pd.concat([dataset, acts], axis =0)  #append the user infor\n",
    "    dataset=dataset.set_index('user_date')\n",
    "    dataset=dataset.fillna(0)\n",
    "    return dataset\n",
    "\n",
    "# dataset =pd.DataFrame()\n",
    "# dir_temp=(dir_data+'phonecharge/')\n",
    "# file= 'phonecharge_u39.csv'\n",
    "\n",
    "# data_s = duration_count_sliced(dir_temp,file)\n",
    "# dataset = pd.concat([dataset,data_s], axis =0) \n",
    "# dataset\n",
    "\n",
    "folder_list= ['conversation','dark','phonecharge','phonelock']\n",
    "# folder_list= ['phonelock']\n",
    "for folder in folder_list:\n",
    "    dataset =pd.DataFrame()\n",
    "    file_dir = dir_data+folder+'/'\n",
    "    file_list= sorted(os.listdir(file_dir))\n",
    "    print(\"total \", len(file_list),\" files in the folder of \" , file_dir)\n",
    "    print(\"counting........\")\n",
    "    for file in file_list:\n",
    "#         print(file)\n",
    "        data_= duration_count_sliced(file_dir, file)\n",
    "        dataset = pd.concat([dataset,data_], axis =0) \n",
    "    col=dataset.columns\n",
    "    new_col=folder+'_'+col\n",
    "    dataset.columns = new_col\n",
    "    print(\"done!\")\n",
    "    dataset.to_csv(dir_tosave+folder+'.csv')\n",
    "    \n",
    "##### seperately generate dataset by types for all folders, done ! #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 3. concatenate all sensing data in one file ############\n",
    "\n",
    "files_position = dir_tosave #'C:/study/Dropbox/research/depression/data/studentlife/sensing/'\n",
    "files = os.listdir(files_position) #['Activity.csv','Class.csv', 'Class 2.csv',...]\n",
    "# files = ['Activity.csv','Class.csv', 'Class 2.csv'] # for testing\n",
    "data_all = pd.DataFrame()\n",
    "for file in files:\n",
    "#     print(file)\n",
    "    this_data= pd.read_csv(files_position+file, index_col='user_date')\n",
    "#     print(this_data)\n",
    "#     data_all=pd.concat([data_all,this_data], axis=1,ignore_index=False)\n",
    "    data_all= data_all.merge(this_data, how='outer', left_index=True, right_index=True )\n",
    "print('Merge success ! Data size of merged dataset is', data_all.shape )\n",
    "data_all\n",
    "dir_tosave_sensing= 'C:/study/Dropbox/research/depression/code/data/' # home win10\n",
    "data_all.to_csv(dir_tosave_sensing+ 'sensing_all_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge EMA and sensing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset_ema shape:  (3025, 107)\n",
      "2. Dataset_sensing shape:  (3075, 12)\n",
      "3. Merge success ! shape of merged dataset is (3407, 119)\n",
      "\n",
      "4.1 Removed columns of non-data, merged dataset shape:  (3407, 87)\n",
      "4.2 Removed columns of non-data, EMA dataset shape:  (3025, 81)\n",
      "4.2 Removed columns of non-data, sensing dataset shape:  (3075, 6)\n"
     ]
    }
   ],
   "source": [
    "#################  1.1 merge sensing and ema  ####################\n",
    "######### ema_all_v5.csv, sensing_all_v2.csv  ###################\n",
    "\n",
    "dir_data = '../data/processed_data/'\n",
    "\n",
    "dataset_ema = pd.read_csv(dir_data+ 'ema_all_v5.csv', index_col=0)  #v2 is the sliced dataset\n",
    "print ('1. Dataset_ema shape: ',dataset_ema.shape)\n",
    "\n",
    "dataset_sensing = pd.read_csv(dir_data+ 'sensing_all_v2.csv', index_col=0)  #v2 is the sliced dataset\n",
    "print ('2. Dataset_sensing shape: ',dataset_sensing.shape)\n",
    "\n",
    "######### clean data ###########\n",
    "dataset= dataset_sensing.merge(dataset_ema, how='outer', left_index=True, right_index=True )\n",
    "print('3. Merge success ! shape of merged dataset is', dataset.shape )\n",
    "\n",
    "def drop_notnum(dataset):\n",
    "    cols=dataset.columns\n",
    "    col_notnum=[]\n",
    "    for col in cols:\n",
    "        if str(dataset[col].dtype)=='object':\n",
    "               col_notnum.append(col)\n",
    "    # print(col_notnum)\n",
    "    dataset=dataset.drop(col_notnum, axis=1)\n",
    "    return dataset\n",
    "\n",
    "dataset=drop_notnum(dataset)  #abc=dataset.dropna(axis=1, how='all') # drops columns those with all NaN\n",
    "dataset_ema= drop_notnum(dataset_ema)\n",
    "dataset_sensing = drop_notnum(dataset_sensing)\n",
    "print('')\n",
    "print('4.1 Removed columns of non-data, merged dataset shape: ', dataset.shape)\n",
    "print('4.2 Removed columns of non-data, EMA dataset shape: ', dataset_ema.shape)\n",
    "print('4.2 Removed columns of non-data, sensing dataset shape: ', dataset_sensing.shape)\n",
    "\n",
    "dataset.to_csv(dir_data+\"studentlife_data_full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
