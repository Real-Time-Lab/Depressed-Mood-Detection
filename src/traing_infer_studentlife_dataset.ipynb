{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea85abf",
   "metadata": {},
   "source": [
    "## Traing and inference\n",
    "  - IV: prediction\n",
    "  - V: plotting\n",
    "  - VI: Generate the probability density and entropy data by various augmentation factors\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858a9a5",
   "metadata": {},
   "source": [
    "## I. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69588ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### prediction with student life dataset sliced #############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "dir_data = '../data/processed_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79252c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHQ-9 shape, with selected user (post):  (38, 5)\n",
      "Dataset shape : (3407, 87)\n"
     ]
    }
   ],
   "source": [
    "##########  0. import phq/label and ema/sensing data #######################\n",
    "## select user with 'post' score available \n",
    "\n",
    "phq = pd.read_csv(dir_data+ 'phq9_score.csv')  # shape of 84,15\n",
    "phq = phq.loc[phq['type']=='post']\n",
    "phq=(phq.loc[:,['uid','type','depression severity','phq score','dp_grade', 'dp_class']]).set_index('uid')\n",
    "phq.rename_axis('user', inplace=True)  # change index from 'uid' to 'user'\n",
    "\n",
    "print(\"PHQ-9 shape, with selected user (post): \", phq.shape)\n",
    "\n",
    "## import EMA/sensing data\n",
    "dataset = pd.read_csv(dir_data+'studentlife_data_full.csv', index_col=0)#set the col0 as index, index as \"u00_2013-03-24\"\n",
    "\n",
    "print ('Dataset shape :',dataset.shape)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc3580",
   "metadata": {},
   "source": [
    "## II. Build model and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d50596",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## 2. build model ##########################\n",
    "from abc import ABC, abstractmethod\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import SimpleRNN, LSTM, Embedding, Dense, Dropout, BatchNormalization,LeakyReLU, MaxPooling2D, Conv2D,Flatten\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support,  accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import dates as mdates\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for Machine Learning Class\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train_predict(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "        \n",
    "    def load_predict(self, X, predict_type):\n",
    "        pass\n",
    "    \n",
    "    def accuracy(self, predict, label):\n",
    "        predict = predict.ravel()\n",
    "        label = label.ravel()\n",
    "        return np.sum(predict == label)/len(label) \n",
    "\n",
    "class RNN_model(BaseModel):\n",
    "    def __init__(self, win_width, feature_num, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.model = Sequential()\n",
    "        self.model.add(SimpleRNN(units=128, input_shape=(win_width, feature_num),activation=\"relu\")) \n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Dense(units=64, activation=\"relu\")) \n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Dense(units=32, activation=\"relu\")) \n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Dense(units= 2, activation=\"linear\")) # \n",
    "        self.model.compile(optimizer='adam',loss='mse', metrics=['mae', 'acc']) \n",
    "        self.model.summary()  \n",
    "      \n",
    "        \n",
    "    def train_predict(self, X, y,X_test, predict_type):\n",
    "        print('On training...')\n",
    "#         history = self.model.fit(X,y,batch_size=30,epochs=self.epochs, verbose = 0) \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1000) # to set patience\n",
    "        history=self.model.fit(X,y,batch_size=128,validation_split=0.33, epochs= self.epochs, verbose = 0, callbacks=[es]) \n",
    "#         history=self.model.fit(X,y,batch_size=128,validation_split=0.33, epochs= self.epochs, verbose = 0) \n",
    "#         self.model.save(save_dir+'RNN_'+predict_type+'.h5')\n",
    "        print('Training done !')\n",
    "        return self.model.predict(X_test), history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def load_predict(self, X, predict_type):\n",
    "        try:\n",
    "            self.model.load_weights('RNN_'+predict_type+'.h5')\n",
    "        except IOError:\n",
    "            raise Exception('IoError when reading dayline data file: ' + 'RNN_model.h5')\n",
    "            \n",
    "class LSTM_model(BaseModel):\n",
    "    def __init__(self, win_width, feature_num, epochs):\n",
    "        self.epochs= epochs\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(units=128, input_shape=(win_width, feature_num), activation='relu', return_sequences= True))  # input_shape=(timesteps ,data_dim)\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.05))  # to set dropout rate\n",
    "        self.model.add(LSTM(units = 64, activation='relu', return_sequences= True, return_state= False))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(LSTM(units = 32, activation='relu', return_sequences= False, return_state= False))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.05)) # to set dropout rate\n",
    "\n",
    "        self.model.add(Dense(units = 2, activation = \"relu\"))\n",
    "#         self.model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "        self.model.compile(optimizer='adam',loss='mse', metrics=['mae', 'acc']) \n",
    "       \n",
    "    \n",
    "    def train_predict(self, X, y, X_test,predict_type):\n",
    "        self.model.summary()\n",
    "        print('On training...')\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1200) # to set patience\n",
    "        history=self.model.fit(X,y,batch_size=128,validation_split=0.33, epochs= self.epochs, verbose = 0, callbacks=[es]) \n",
    "#         history=self.model.fit(X,y,batch_size=128,validation_split=0.33, epochs= self.epochs, verbose = 0) \n",
    "\n",
    "        #         self.model.save(save_dir+'LSTM_'+predict_type+'.h5')\n",
    "        print('Training done !')\n",
    "        return self.model.predict(X_test), history\n",
    "   \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def load_predict(self, X, predict_type):\n",
    "        try:\n",
    "            self.model.load_weights('LSTM_'+predict_type+'.h5')\n",
    "        except IOError:\n",
    "             raise Exception('IoError when reading dayline data file: ' + 'LSTM_model.h5')\n",
    "        return self.model.predict(X)\n",
    "   \n",
    "  \n",
    "def accuracy(predict, label):\n",
    "    predict = predict.ravel()\n",
    "    label = label.ravel()\n",
    "    return np.sum(predict == label)/len(label) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60970699",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## functions: data processing #####################\n",
    "########################################################\n",
    "\n",
    "def dataExtract(data, win_width =10, stride =1, forward_step= 0, label = 'label', normalize = False): # output 3 dimensions array\n",
    "    sample_num = data.shape[0]\n",
    "\n",
    "    y=data.loc[:,label].values\n",
    "    X = data.drop(columns = [label]) # drop 'label column, use remain for feature\n",
    "    feature_num = X.shape[1]\n",
    "    if normalize == True:\n",
    "        X = minmaxNormalize(X)\n",
    "#         X = Normalize(X)\n",
    "    X_new = np.array([[]]*feature_num).T.reshape(0,win_width,feature_num) # 3D empty array\n",
    "    y_new = []\n",
    "    for i in range(sample_num-forward_step,0, -stride):\n",
    "        X_win = X.values[i-win_width:i,:] \n",
    "        if X_win.shape[0] <win_width:\n",
    "            break \n",
    "        X_win = X_win.reshape(1,win_width,feature_num) # reshape to 3D for catanecating\n",
    "        X_new = np.vstack((X_new,X_win)) \n",
    "        y_new.append(y[i+forward_step-1]) \n",
    "    X_new = X_new[::-1,:,:] \n",
    "    y_new = np.array(y_new[::-1]) \n",
    "    return X_new, y_new # retrun array instead of dataframe\n",
    "\n",
    "def minmaxNormalize(X, axis = 0): # default axis set to 0\n",
    "    minum = np.min(X, axis = axis)\n",
    "    maxum = np.max(X, axis = axis)\n",
    "    return (X-minum)/(maxum-minum)\n",
    "\n",
    "def Normalize(X):\n",
    "    return  X/np.linalg.norm(X, ord = 2, axis = 0)\n",
    "\n",
    "def PCA(X, k, axis=1):  # feature in rows axis =0, in cols axis =1, X in np.array formate\n",
    "    if axis==0:  #features by rows\n",
    "        n = X.shape[0]\n",
    "        mean = np.mean(X, axis = 0).reshape(1,-1) # features number X data number \n",
    "        Xm = (X- mean)\n",
    "        COV =  Xm @ Xm.T/(n-1)\n",
    "        eig_value, eig_vector = np.linalg.eig(COV) \n",
    "        index = np.argsort(-1*eig_value) # rank from the largest value to the smallest one， descend\n",
    "        P = eig_vector[:,index[0:k]]\n",
    "        return P.T @ Xm\n",
    "    else:  #axis == 1, features by cols\n",
    "#         X=dataframe.values\n",
    "        n = np.array(X).shape[1]\n",
    "        mean = np.mean(X, axis = 1).reshape(-1,1) # features number X data number \n",
    "        Xm = (X- mean)\n",
    "        COV =  Xm.T@Xm/(n-1)\n",
    "        eig_value, eig_vector = np.linalg.eig(COV) \n",
    "        index = np.argsort(-1*eig_value) # rank from the largest value to the smallest one， descend\n",
    "        P = eig_vector[:,index[0:k]]\n",
    "        return Xm@P\n",
    "    \n",
    "\n",
    "def slice_sum_by_id(data,id_list, win_width, stride, forward_step, label):\n",
    "    x_f=np.array([])\n",
    "    y_f=np.array([])\n",
    "#     id_list =np.unique(data['ID'])\n",
    "    for id in id_list:\n",
    "        #data_id= (data.where(data['ID']==id)).dropna()\n",
    "        data_id= data[data.index==id]\n",
    "        x_sliced, y_sliced = dataExtract(data_id, win_width =win_width, stride =stride, \\\n",
    "                                         forward_step= forward_step, label = label, normalize = False)\n",
    "        if not x_f.any():  # initialize 3d array\n",
    "            x_f=np.empty(shape=x_sliced.shape)  #sliced size for each id =90-(win_width+stride+forward_step)+2\n",
    "            y_f=np.empty(shape=y_sliced.shape)\n",
    "            init=x_sliced.shape[0]\n",
    "        x_f= np.append(x_f,x_sliced, axis=0)\n",
    "        y_f= np.append(y_f,y_sliced, axis=0)\n",
    "    x_f=x_f[init:,:,:]  # take off the first chunk created by np.empty : size of init\n",
    "    y_f=y_f[init:,]\n",
    "    return x_f, y_f\n",
    "\n",
    "''' create a diction of score:grade '''\n",
    "dp_grade ={0:[0,4], 1:[5,9],2:[10,14],3:[15,19],4:[20,27]} \n",
    "dp_score2grade={}\n",
    "for key, value in dp_grade.items(): \n",
    "    for i in range(value[0], value[1]+1):\n",
    "        dp_score2grade[i]=key\n",
    "        \n",
    "# normalize dataset and fill nan with -1\n",
    "def minmax_normalize(df): # this min_max norm can apply dataset with NULL value\n",
    "    for col in df.columns:\n",
    "        df[col]= (df[col]-df[col].min())/(df[col].max()-df[col].min())\n",
    "#     df.dropna(how='any', inplace=True, axis=1) # drop columns with null falue\n",
    "    return df\n",
    "\n",
    "def normalize_list(la): # normalize a 1 dimension list\n",
    "    if sum(la)==0:\n",
    "        lb =la # doesn't consider all la element =0\n",
    "    else:\n",
    "        lb=[]\n",
    "        [lb.append(v/sum(la)) for v in la]  \n",
    "    return lb\n",
    "\n",
    "def entropy(lst):\n",
    "    result= 0;\n",
    "    lst = normalize_list(lst)\n",
    "    for p in lst:\n",
    "        if p == 0:\n",
    "            pass\n",
    "        else:\n",
    "            result+=(-p)*np.log2(p)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e3b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' function of 2nd round classification by user catogory'''\n",
    "def classifier_slice2uid(df_predicted, trigger=0.1): # 2nd round classification by user catogory, binary\n",
    "    users= np.unique(df_predicted.index)\n",
    "    score_user ={}\n",
    "    class_user ={}\n",
    "    for user in users:\n",
    "        class_slice = df_predicted.loc[user][0].to_list()\n",
    "        score= sum(class_slice)/len(class_slice)\n",
    "        score_user[user]=score\n",
    "#     print(score_user)\n",
    "    for user, score in score_user.items():\n",
    "        class_user[user]= int(score-trigger+1) # convert the value >threshold to be 1 \n",
    "#     predictions_user = np.array(list(class_user.values()))  # transfer to np array format, need: dict.values -> list -> np.array\n",
    "#     print(\"predictions_user:\", predictions_user)\n",
    "#     print(\"class_user: \", class_user)\n",
    "    predictions_user = pd.DataFrame(class_user.values(), index = class_user.keys()) # transfer diction to dataframe\n",
    "    return predictions_user , score_user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acabe61",
   "metadata": {},
   "source": [
    "## III. data cleaning and train/test data processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84971088",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 0. pick up features\n",
    "'''\n",
    "\n",
    "\n",
    "# 0. all features\n",
    "col_slct= dataset.columns # ALL features, 78 cols after manully filter \"null\" and \"green\" columns\n",
    "\n",
    "# # 1. EMA 6 + sensing 4\n",
    "col_slct=['Mood_happyornot','PAM_picture_idx', 'Mood_sad','Stress_level','Sleep_social','Sleep_rate',\n",
    "            'dark_duration(sec)','audio_ audio inference',\n",
    "            'phonecharge_duration(sec)','activity_ activity inference'] \n",
    "\n",
    "# # 2. EMA  6 features\n",
    "# col_slct=['Mood_happyornot','PAM_picture_idx', 'Mood_sad','Stress_level','Sleep_social','Sleep_rate']\n",
    "\n",
    "# # # # 3. sensing 4 features\n",
    "# col_slct=['dark_duration(sec)','audio_ audio inference',\n",
    "#           'phonecharge_duration(sec)','activity_ activity inference'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b4a43e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''1. Data cleaning:\n",
    "    - select columns\n",
    "    - normalization, or\n",
    "    - PCA (on demand)\n",
    "'''\n",
    "## normalization\n",
    "dataset_col = dataset[col_slct].copy()\n",
    "data_norm= minmax_normalize(dataset_col)\n",
    "\n",
    "\n",
    "## process PCA ############\n",
    "# dataset_np = data_norm.iloc[:,1:].to_numpy()  # transfer to numpy without index of 'user_date'\n",
    "# K=2\n",
    "# dataset_np=PCA(dataset_np, K, axis= 1)\n",
    "# dataset_np.shape\n",
    "# dataset_pca=pd.DataFrame(dataset_np, columns= np.arange(K), index= data_norm.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91da02b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset before labeling is: (3407, 10)\n",
      "After labeling and removing non-qualified users, dataset shape is: (2678, 11)\n"
     ]
    }
   ],
   "source": [
    "''' 2. Data cleaning:\n",
    "    - index: user_date to user\n",
    "    - add lables\n",
    "    - clean data\n",
    "    '''\n",
    "\n",
    "########### 1.2 add label(dp_class) by merging phq ###############\n",
    "########## add uer and set as index for merging phq ###########\n",
    "\n",
    "\n",
    "# dataset_1=dataset_col   # no pca, no normalization\n",
    "# dataset_1= dataset_pca # with both normalization + pca\n",
    "dataset_1 = data_norm # with normalization , no pca\n",
    "\n",
    "dataset_1['user']=dataset_1.index  # copy 'user' column with index values\n",
    "def str_slice(str):\n",
    "    return str[:3]\n",
    "dataset_1['user']=dataset_1['user'].apply(str_slice)\n",
    "dataset_1 = dataset_1.reset_index().set_index('user') \n",
    "dataset_1 = dataset_1.drop(['user_date'], axis=1)# keep the index columns and set 'user' col as index\n",
    "dataset_1.fillna(0, inplace= True)  # interpolate 0 into missing data (NaN)\n",
    "print(\"Dataset before labeling is:\", dataset_1.shape)\n",
    "\n",
    "## add label of dp_class\n",
    "dp_class_list=[]\n",
    "for user in dataset_1.index: \n",
    "    if user in phq.index.to_list(): #if the user of a instance is in the selected phq_user list\n",
    "        dp_class_list.append(phq.loc[user]['dp_class'])\n",
    "    else:\n",
    "        dp_class_list.append(None) # for removing those users are not in the selected list\n",
    "dataset_1['dp_class']=dp_class_list\n",
    "\n",
    "dataset_1= dataset_1.dropna(axis = 0, how='any') # drop off user without post score\n",
    "\n",
    "# dataset_1.to_csv(dir_data+\"studentlife_datafull_cleaned.csv\")\n",
    "\n",
    "print(\"After labeling and removing non-qualified users, dataset shape is:\", dataset_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc6a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. Traing and testing dataset processing:\n",
    "    - Split training and testing dataset\n",
    "    - Data augment : pick the id with depression ,which stays in train dataset for augmenting'''\n",
    "\n",
    "################## process training and testing data #######################\n",
    "\n",
    "random_seed= 3 # 1 depression :42, 0 ,3 ; 0 depression: 2,5, 6\n",
    "agm_factor = 2 # add 4 times depressed user data, randomly\n",
    "trigger =0.2 # threshold for 2nd round classification\n",
    "split_rate =0.8\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_test(dataset, agm_factor=agm_factor, random_seed = random_seed):\n",
    "    random.seed(random_seed)\n",
    "    id_depression_test=[]\n",
    "    id_depression_all=['u16','u17', 'u18','u23','u33','u52','u53'] # users who have depression\n",
    "    \n",
    "    \n",
    "#     #### for test with data version 1\n",
    "#     id_depression_all=['u17', 'u18','u23','u33','u52'] # users who have depression\n",
    "#     ####\n",
    "    \n",
    "    id_list =list(np.unique(dataset.index))\n",
    "    id_nonedepression_all = list(set(id_list)-set(id_depression_all))\n",
    "\n",
    "    # id depression    \n",
    "    id_depression_test = random.sample(id_depression_all,2)\n",
    "    id_depression_train = list(set(id_depression_all)- set(id_depression_test))\n",
    "\n",
    "    # id nonedepression\n",
    "    split = round(split_rate*len(id_nonedepression_all))\n",
    "    id_nonedepression_train= random.sample(id_nonedepression_all, split)\n",
    "    id_nonedepression_test = list(set(id_nonedepression_all)-set(id_nonedepression_train))\n",
    "\n",
    "    id_test = sorted(id_depression_test + id_nonedepression_test)\n",
    "    id_train = sorted(id_depression_train + id_nonedepression_train)\n",
    "\n",
    "    id_agm =id_train+ random.sample(id_depression_train, agm_factor)\n",
    "#     random.shuffle(id_agm)\n",
    "\n",
    "#     print(id_test)\n",
    "#     print(id_agm)\n",
    "\n",
    "    X_train= dataset.loc[id_agm,:].iloc[:,:-1] # X_train shape[0] will increase after augmenting\n",
    "    y_train= dataset.loc[id_agm,:].iloc[:,-1]\n",
    "\n",
    "    X_test= dataset.loc[id_test,:].iloc[:,:-1]\n",
    "    y_test= dataset.loc[id_test,:].iloc[:,-1]\n",
    "\n",
    "    y_train_dummy = pd.get_dummies(y_train).values\n",
    "    y_test_dummy = pd.get_dummies(y_test).values\n",
    "    \n",
    "#     print('Total ', len(list(np.unique(X_train.index))), 'training users, and ', len(list(np.unique(X_test.index))) ,' test users.')\n",
    "#     print(\"Shape:\",X_train.shape,  y_train_dummy.shape, X_test.shape, y_test_dummy.shape )\n",
    "#     print(\"Size of features selected:\", len(col_slct))\n",
    "    return X_train, y_train, X_test , y_test, y_train_dummy, y_test_dummy\n",
    "\n",
    "## split train/test  and dummy y\n",
    "X_train, y_train, X_test , y_test, y_train_dummy, y_test_dummy = generate_train_test(dataset_1, agm_factor=agm_factor, random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438fb49",
   "metadata": {},
   "source": [
    "## Prediction stage-1\n",
    "    - store the result to file: dir_data+\"result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c4edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif']=[u'simHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "\n",
    "# ===========================machine learning algorithms to predict and plot====================#\n",
    "def ml_predict_score_user(X_train, y_train_dummy):\n",
    "    ### 1. Random Foreest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "#     print(\"=============================================================================================\")     \n",
    "#     print(\"1. RF\")\n",
    "\n",
    "    RF = RandomForestClassifier(n_estimators=10,oob_score= True) # after tuning, n_estimators =100 is too large\n",
    "    RF.fit(X_train,y_train_dummy) ###\n",
    "    print('1', y_train_dummy.shape)\n",
    "    pred = RF.predict(X_test) ####\n",
    "    predictions_1 = pred.argmax(axis = 1) ####\n",
    "\n",
    "    predicted_df = pd.DataFrame(predictions_1, index =X_test.index) \n",
    "    predictions_temp, score_user_1  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "    predictions_1 =  predictions_temp.iloc[:,-1].to_list() \n",
    "    y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "\n",
    "    mse_1 = np.sum((y_test-list(score_user_1.values()))**2/len(y_test))\n",
    "#     print(score_user_1)\n",
    "#     print(\"mse of score by users: \",mse_1)\n",
    "\n",
    "\n",
    "\n",
    "#     ###2. Logistic Regression Classifier \n",
    "#     from sklearn.linear_model import LogisticRegression\n",
    "# #     print(\"=============================================================================================\")        \n",
    "# #     print(\"2. LR\")\n",
    "\n",
    "#     clf = LogisticRegression(penalty='l2')\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_2 = pred.argmax(axis = 1) ####\n",
    "\n",
    "    \n",
    "    \n",
    "#     predicted_df = pd.DataFrame(predictions_2, index =X_test.index) \n",
    "#     predictions_temp, score_user_2  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_2 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_2 = np.sum((y_test-list(score_user_2.values()))**2/len(y_test))\n",
    "# #     print(score_user_2)\n",
    "# #     print(\"mse of score by users: \",mse_2)\n",
    "\n",
    "    ### 3. Decision Tree Classifier    \n",
    "    from sklearn import tree\n",
    "#     print(\"=============================================================================================\")    \n",
    "#     print(\"3. Decision Tree\")\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train,y_train_dummy)  ####\n",
    "    pred = clf.predict(X_test) ####\n",
    "    predictions_3 = pred.argmax(axis = 1) ####\n",
    "\n",
    "    predicted_df = pd.DataFrame(predictions_3, index =X_test.index) \n",
    "    predictions_temp, score_user_3  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "    predictions_3 =  predictions_temp.iloc[:,-1].to_list() \n",
    "    y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "    mse_3 = np.sum((y_test-list(score_user_3.values()))**2/len(y_test))\n",
    "\n",
    "#     print(score_user_3)\n",
    "#     print(\"mse of score by users: \",mse_3)\n",
    "\n",
    "#     ###4. Gradient Boosting Decision Tree\n",
    "#     from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "# #     print(\"=============================================================================================\")    \n",
    "# #     print(\"4. Gradient Boosting Decision Tree\")\n",
    "\n",
    "#     clf = GradientBoostingClassifier()\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_4 = pred.argmax(axis = 1) ####\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_4, index =X_test.index) \n",
    "#     predictions_temp, score_user_4  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_4 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_4 = np.sum((y_test-list(score_user_4.values()))**2/len(y_test))\n",
    "# #     print(score_user_4)\n",
    "# #     print(\"mse of score by users: \",mse_4)\n",
    "\n",
    "#     ###5. AdaBoost Classifier\n",
    "#     from sklearn.ensemble import  AdaBoostClassifier\n",
    "# #     print(\"=============================================================================================\")     \n",
    "# #     print(\"5. AdaBoost\")\n",
    "\n",
    "#     clf = AdaBoostClassifier()\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_5 = pred.argmax(axis = 1) ####\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_5, index =X_test.index) \n",
    "#     predictions_temp, score_user_5  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_5 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_5 = np.sum((y_test-list(score_user_5.values()))**2/len(y_test))\n",
    "# #     print(score_user_5)\n",
    "# #     print(\"mse of score by users: \",mse_5)\n",
    "\n",
    "#     ### 6. GaussianNB\n",
    "#     from sklearn.naive_bayes import GaussianNB\n",
    "# #     print(\"=============================================================================================\")     \n",
    "# #     print(\"6. GaussianNB\")\n",
    "\n",
    "#     clf = GaussianNB()\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_6 = pred.argmax(axis = 1) ####\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_6, index =X_test.index) \n",
    "#     predictions_temp, score_user_6  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_6 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_6 = np.sum((y_test-list(score_user_6.values()))**2/len(y_test))\n",
    "# #     print(score_user_6)\n",
    "# #     print(\"mse of score by users: \",mse_6)\n",
    "\n",
    "#     ### 7. Linear Discriminant Analysis\n",
    "#     from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# #     print(\"=============================================================================================\")  \n",
    "# #     print(\"7. Linear Discriminant Analysis\")\n",
    "\n",
    "#     clf = LinearDiscriminantAnalysis()\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_7 = pred.argmax(axis = 1) ####)\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_7, index =X_test.index) \n",
    "#     predictions_temp, score_user_7  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_7 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_7 = np.sum((y_test-list(score_user_7.values()))**2/len(y_test))\n",
    "\n",
    "# #     print(score_user_7)\n",
    "# #     print(\"mse of score by users: \",mse_7)\n",
    "\n",
    "#     ### 8. Quadratic Discriminant Analysis\n",
    "#     from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# #     print(\"=============================================================================================\")   \n",
    "# #     print(\"8. Quadratic Discriminant Analysis\")\n",
    "\n",
    "#     clf = QuadraticDiscriminantAnalysis()\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_8 = pred.argmax(axis = 1) ####\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_8, index =X_test.index) \n",
    "#     predictions_temp, score_user_8  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_8 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_8 = np.sum((y_test-list(score_user_8.values()))**2/len(y_test))\n",
    "\n",
    "# #     print(score_user_8)\n",
    "# #     print(\"mse of score by users: \",mse_8)\n",
    "\n",
    "\n",
    "#     ### 9. SVM Classifier \n",
    "#     from sklearn.svm import SVC\n",
    "#     clf = SVC(kernel='rbf', probability=True)\n",
    "# #     print(\"=============================================================================================\") \n",
    "# #     print(\"9. SVM\")\n",
    "\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     clf.fit(X_train,y_train_dummy)  ####\n",
    "#     pred = clf.predict(X_test) ####\n",
    "#     predictions_9 = pred.argmax(axis = 1) ####\n",
    "\n",
    "#     predicted_df = pd.DataFrame(predictions_9, index =X_test.index) \n",
    "#     predictions_temp, score_user_9  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "#     predictions_9 =  predictions_temp.iloc[:,-1].to_list() \n",
    "#     y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "#     mse_9 = np.sum((y_test-list(score_user_9.values()))**2/len(y_test))\n",
    "\n",
    "# #     print(score_user_9)\n",
    "# #     print(\"mse of score by users: \",mse_9)\n",
    "\n",
    "\n",
    "    ### 10. knn Classifier \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "#     print(\"=============================================================================================\") \n",
    "#     print(\"10. KNN\")\n",
    "\n",
    "    knn=KNeighborsClassifier(n_neighbors= 5)\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train,y_train_dummy)  ####\n",
    "    pred = knn.predict(X_test) ####\n",
    "    predictions_10 = pred.argmax(axis = 1) ####\n",
    "\n",
    "    predicted_df = pd.DataFrame(predictions_10, index =X_test.index) \n",
    "    predictions_temp, score_user_10  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "    predictions_10 =  predictions_temp.iloc[:,-1].to_list() \n",
    "    y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "    mse_10 = np.sum((y_test-list(score_user_10.values()))**2/len(y_test))\n",
    "\n",
    "#     print(score_user_10)\n",
    "#     print(\"mse of score by users: \",mse_10)\n",
    "    \n",
    "    return score_user_1,score_user_2,score_user_3,score_user_4,score_user_5, score_user_6, score_user_7, score_user_8,score_user_9, score_user_10,\\\n",
    "    y_test, mse_1,mse_2,mse_3,mse_4,mse_5,mse_6,mse_7,mse_8,mse_9,mse_10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53516e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2148, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:546: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score_user_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e5b70da5f267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscore_user_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mscore_user_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_10\u001b[0m\u001b[1;33m,\u001b[0m        \u001b[0my_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_10\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;33m=\u001b[0m \u001b[0mml_predict_score_user\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_dummy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# train and predict with ML ================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0malgorithm_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"BLANK\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"RF\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LR\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"DT\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"GBDT\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AB\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"GNB\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LDA\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"QDA\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"SVM\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"KNN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-6697a328292d>\u001b[0m in \u001b[0;36mml_predict_score_user\u001b[1;34m(X_train, y_train_dummy)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;31m#     print(\"mse of score by users: \",mse_10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscore_user_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_user_9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_user_10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse_10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score_user_2' is not defined"
     ]
    }
   ],
   "source": [
    "# ###############  start iteration by augmentation factor ###################\n",
    "# #######################################################################\n",
    "import time\n",
    "\n",
    "p_ml =  pd.DataFrame()\n",
    "# nondepression_idnum_train=len(np.unique(y_train.index))-len(id_dptrain) # none depression id number in training\n",
    "nondepression_idnum_train=len(np.unique(y_train.index))\n",
    "\n",
    "for j in range(6):   # j stands for the augmenting factor \n",
    "    X_train, y_train, X_test, y_test, y_train_dummy, y_test_dummy \\\n",
    "    = generate_train_test(dataset_1, agm_factor=j, random_seed = random_seed) # generate train dataset\n",
    "    \n",
    "    score_user_1,score_user_2,score_user_3,score_user_4,score_user_5, score_user_6, score_user_7, score_user_8,\\\n",
    "    score_user_9,score_user_10,        y_t, mse_1,mse_2,mse_3,mse_4,mse_5,mse_6,mse_7,mse_8,mse_9,mse_10 \\\n",
    "    = ml_predict_score_user(X_train, y_train_dummy) # train and predict with ML ================\n",
    "    \n",
    "    algorithm_list = [\"BLANK\",\"RF\",\"LR\",\"DT\",\"GBDT\",\"AB\",\"GNB\",\"LDA\",\"QDA\",\"SVM\",\"KNN\"]\n",
    "    for i in range(1,len(algorithm_list)): # total 10 algorithms, starts from 1\n",
    "#         print('score_user_{}='.format(i))\n",
    "#         var=eval(\"score_user_\"+str(i))\n",
    "#         print(var)\n",
    "        p_temp=[] \n",
    "        p_temp.append(int(time.time()))\n",
    "        p_temp.append(j)\n",
    "        p_temp.extend(list(eval(\"score_user_\"+str(i)).values()))  # predicted\n",
    "        p_temp.extend(y_t)  # ground truth\n",
    "        p_temp.append(eval(\"mse_\"+str(i)))\n",
    "        p_temp.append(entropy(list(eval(\"score_user_\"+str(i)).values()))) # entropy under prediction with algorithm i\n",
    "        p_temp.append(algorithm_list[i])\n",
    "        p_temp.append(len(col_slct))\n",
    "        p_temp.append(random_seed)\n",
    "        p_series =  pd.Series(p_temp)\n",
    "        p_ml = p_ml.append(p_series, ignore_index=True)\n",
    "\n",
    "user_prob_col, user_gth_col, user_prd_col, =[], [], []\n",
    "[(user_prob_col.append(u+'_prob'), user_gth_col.append(u+'gth'), user_prd_col.append(u+'_prd')) for u in score_user_1.keys()] # e.g. [u01_prd, ...]\n",
    "\n",
    "p_col = [\"time\",\"aug_factor\"]+user_prob_col + user_gth_col +[\"mse\",\"entropy\",\"model\",\"feature num\",\"random seed\"]\n",
    "p_ml.columns= p_col\n",
    "p_ml['feature num']=p_ml['feature num'].astype(int) #format the feature type\n",
    "p_ml['aug_factor']=p_ml['aug_factor'].astype(int)\n",
    "p_ml.to_csv(dir_data+\"result.csv\", mode=\"w\", index= False)\n",
    "print(\"Stage1 prediction result saved in:\", dir_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6201f89",
   "metadata": {},
   "source": [
    "## Prediction stage-2\n",
    "    - predict with probability lists by using student_t\n",
    "    - store the result to file: dir_data+\"result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bede9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict with probability lists by using student_t\n",
    "\n",
    "# STUDENT-t\n",
    "def student_t(data):\n",
    "    n=len(data)\n",
    "    mu= np.mean(data)\n",
    "    S= np.std(data,ddof=1)#variance\n",
    "    t_av= 1.440 #https:// 90% confidence, 1 side, en.wikipedia.org/wiki/Student%27s_t-distribution\n",
    "#     t_av =0.906 # 80%\n",
    "    t_av= 1.943 #  95% confidentce, 1 side\n",
    "    X_left = mu-t_av*S/np.sqrt(n)\n",
    "    X_right = mu +t_av*S/np.sqrt(n)\n",
    "#     print(X_left,\",\",X_right)\n",
    "    result=[]\n",
    "    for i in range(len(data)):\n",
    "        if data[i] >= X_right:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "data_result = pd.read_csv(dir_data+'result.csv')\n",
    "data_prd =pd.DataFrame(columns = user_prd_col, dtype=object)\n",
    "\n",
    "for i in range(len(data_result.index)):\n",
    "    prd_stage2 =  student_t(data_result[user_prob_col].iloc[i,:])\n",
    "    data_prd.loc[len(data_prd.index)] = prd_stage2 # add prediction to the row end of dataframe\n",
    "\n",
    "result= pd.merge(data_result, data_prd, left_index=True, right_index= True)\n",
    "result.to_csv(dir_data+\"result.csv\", mode=\"w\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c4a8a5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "446fbf8e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Evaluate and plot\n",
    "    - accuracy, recall, precision, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb5c8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics calculation\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\n",
    "data_result = pd.read_csv(dir_data+ \"result.csv\")\n",
    "gth = data_result[user_gth_col]\n",
    "prd = data_result[user_prd_col]\n",
    "metrics = pd.DataFrame(columns = ['acc','recall','precision', 'f1', 'roc_auc'], dtype = object)\n",
    "for i in range(len(data_result.index)):\n",
    "    acc= accuracy_score(gth.iloc[i,:], prd.iloc[i,:])\n",
    "    recall= recall_score(gth.iloc[i,:], prd.iloc[i,:])\n",
    "    precision = precision_score(gth.iloc[i,:], prd.iloc[i,:]) \n",
    "    f1= f1_score(gth.iloc[i,:], prd.iloc[i,:])\n",
    "    roc_auc = roc_auc_score(gth.iloc[i,:], prd.iloc[i,:])\n",
    "#     print(acc, recall, precision, f1, roc_auc)\n",
    "    metrics.loc[len(metrics.index)] = [acc, recall, precision, f1, roc_auc]\n",
    "result= pd.merge(data_result, metrics, left_index=True, right_index=True)\n",
    "result.to_csv(dir_data+\"result.csv\", mode=\"w\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3aebedd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fcd6ce06dc8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'font.sans-serif'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'simHei'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'axes.unicode_minus'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions_10' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif']=[u'simHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "roc_curve(y_test,predictions_10)\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "# plt.xlim(0,1) \n",
    "# plt.ylim(0.0,1.1) \n",
    "plt.title('ROC',fontsize=14,fontweight='bold')\n",
    "plt.xlabel('False Postive Rate')\n",
    "plt.ylabel('True Postive Rate')\n",
    "plt.plot(fpr_10,tpr_10,linewidth=2, linestyle=\"-\",color='red')\n",
    "\n",
    "plt.subplot(122)\n",
    "ticks=['Normal','Depression']\n",
    "conf_mat = confusion_matrix(y_test, predictions_10)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=ticks, yticklabels=ticks,cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix',fontsize=14,fontweight='bold')\n",
    "plt.ylabel('Actual',fontsize=12)\n",
    "plt.xlabel('Prediction',fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d617e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47e79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47af535d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,346\n",
      "Trainable params: 3,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.99      0.85       406\n",
      "           1       0.38      0.02      0.04       142\n",
      "\n",
      "    accuracy                           0.74       548\n",
      "   macro avg       0.56      0.50      0.44       548\n",
      "weighted avg       0.65      0.74      0.64       548\n",
      "\n",
      "AC 0.7372262773722628\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyoAAAEWCAYAAACJ7mM4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABqm0lEQVR4nO3deXxU9b3/8ddnJnvYF0FEBAVFwR2tK0YqitZqVVSuVopLtb9avba3rVrbaler3by1t1YEcWmLYt1q3TeEtqLiLm64gLLKEgjZt8/vj+9JCMkkmYSJM5D308cxkzPnfM9nToaZ8znfzdwdERERERGRTBJLdwAiIiIiIiLNKVEREREREZGMo0RFREREREQyjhIVERERERHJOEpUREREREQk4yhRERERERGRjKNERWQbZWYTzeycdMchIiIi0hWUqEinmdk0M/MES1EKyh5uZp2a5CeKYfjWxtDOMYrMrLQrj5GEi4F7G34xsyOi175rGmMSEUk5M+trZveaWZmZvWpmh6So3B5m9o+o3FVmdkSKyv28vofczH4T/b5X9PttXXnc6FhLUvFdL9IeJSqyNf4G9AX2jX7fLfr9Xyko+5OorM7oG+2/3TKzU4An3b28yepjm/0UEdle3AYMAQ4E/gT8w8wKU1DuNGAnYBRQBHyYgjLh8/0eGtPsZ1LM7JqtSGr2ITXf9SJtUqIinebu1e6+ASiJVpW4+wZ3r01B2fVR2Z3Zd4O7129tDJnKzGLAecAtzZ46FpiLEhUR2Y5EtcQnARe7+7vufgtQDhyTguIHAG+6+4qo7JUpKPPz/B6qY3OCMjb6vcu5e0kqvutF2qNERbqMmd0W3bE5y8zeNbPLmjx3WFR9X25mL5rZXs32bdH0q2GdmR1iZu+YWYmZXZvguC2q3KN1x5rZS2ZWamZ3mJlFz/U2s4ei8u42syfM7O6tfO1jzexfZrbRzB4xs6FNnjvGzBY1ee1jknmuibOAv7t7dZP9+gDjgF8DE8ws3uS5fczs39HrXmBm+7f3XNSkYEmCczg8etypv20bxzvHzBY12S4v2ubApE+6iGyvDgeqgNearPs/oBha/7xt+Bwzs5PMbKmZrTezb0XPfTX6jrka+Fr0+bYkei7R909jUycz2z/6bCuPPq/HNw+4le+hvmY228yKo8/II5ttn/A7qh3LgGwz60VIWF5qUuaO0fkoNbOPLNTEt/nao+enmdlcMzsgOq9/T/D6WjT9MrN7zGxG9HhnC83pxiXxGkRapURFutqxwGXA5UT9KaIP33uA+4BdgeeA33SgzD8AXwOmApdb8n0y/gBcBUwETgeOjtZ/H3Bgb0LztX8Dl3Qgni2YWQ/gCeBJQrO45cCDFmpCAP4C3AnsDixgy9fe1nOYWRZwZrRdU8cAS6PjZgNfiLbvBTwexbIH8CzhvLf5XJI69Ldt53gPALua2ejo94nASnd/uQPxiMj2aQiwrmkNhbv/2t3nJfF525/wGXUC8GPgt2aWB9xNaJ51HTA7erxPkvHcBLwIjCR8Xt+c5H53AIWE5mt/Bh4xs52aPN/ad1R73gT2ItSovNFk/e8INSy7A98Fbo++Q5J57YOAvwIzgZ8kGce3gcnRzamfAXe4+8Ik9xVJKCvdAch2bxSwu7sXN1t/IOFu2FigJ+HCNVk/c/cXAcxsFbAz8FES+/2fuz8R7fdqtB/A/sAD7r7UzJ4Ehrj7Zx2Ip7kvE5ol/NTd3cwuBdYCBxOSjwogB9hAuNBv+u+wrecAzgdud/fm1fvHAv9x91oze6nhd+BLQJm7XxO97l8Cr5tZdjvPJaOjf9tWj+fum8zsUWAy8HPgK8BdScYhItu3bFpv0tTW5y1AD+Ab7r7IzBYDNwKD3H0psMHMKoGGZszJqohiqiBc7P++vR3MbEfgRGCYu38K3GxmU4BzgF9Fm7X2HdWeN4ADgN7ASmB4tP5bhHMzFBhM+DzeMTp+e699NHCkuyfdD8Xdl5nZLwgJ2VBgz2T3FWmNalSkq93e/ELW3R24lFBlfRMwEIgn2Lc1zzV5XA0kUz3e1n4fAIdYaC51EPB2B2JJZGfg4+h14u4VhC+PYdHzZwNHEF7//OiYtPdcdBfwBJqM9NXEROB0M9sAHMbmfio7Ax83bOTum9z9Lnevaee5LZhZQYJjdvRv297x7gZOi+6EfhklKiISbAD6NF1hZvPM7GLa/7wtdvc3o+camssm+53RVNPPwG8Sahw+AF4Bjkti/52B2ihJaPBhkzih899tbwCn0fK7awLwHvAPwg05SP679uWOJClNzCDUbN2T4CaWSIcpUZGu1mIIXzM7CvhvYKy7H0yoWk6au5e0v1VCre33LnAqUEmozWjeSb2jPgFGNPxiZvnAjsDS6II/292PJXTifJRQ9U5bz0W+CdzU8IXcpPzdCXfQJgL7Af8FHGyh38qnwC5Nts0ys9fNbFQ7zzlbfj4k6ivS0b9tW8cD+CehicJUYLW7L0JEBF4FeprZyCbrRhJuiLT6eRut6sz3hUdlxaKfOxNuujSsG0RogjsAuAG4u5WbOU19AmRZk/6KhOaxS5v83tnvtjcISUljs6/oxtYdwDfdfU/C53Jz9bSeDHV2+P0fE26yfdXMRrS3sUh7lKhIOvQifBH0MbPDgN/SuTtcqXIl4UtnDHBMsyF/22JmNrTZUkC44C4ws6vNbBdCu+N3CZ0c48DjZjYV2IHwuhuad7X6XNQO+3B3fyxBHMcCn7r7fHdfAjxESLomRLH0NLMfRW2hf0BoHrC0neeWAzua2S7Ra0q2jXJbf9u2joe7l0Xb/A7VpojIZv8hJCv/Z2YjzexyIBd4mrY/bzvrM6AGODSqaf9d9DtRP5m/EvpjDGXLz/BWufuqKNb/M7MRZnYRoca8eX/Dzng7iq9p/5QcwjkqMLM9ophhy+/aD4AvRN9d+21tYmFmBxBGpJxK6Ldz09aUJwJKVCQ9HgUeBF4mfJjNAIaY2aA0xXM/MIfwIV9lZm8k+YFdQKglaLqc5e6lhKYAxxI6OQ4FTo6GXN5EqPG4glDtfxbhg522niN8Kd7QShwTgacafomaN8wDjo3KnBQt70UxnRQNLd3Wcx8A/0sYWOBf0eNktPq3bet4TfZv6OSpREVEgMYmpV8m9FN5EzgDmOTupW193m7F8SoIHfDnEJp2PQmsaLLJFEJ/uveAa4Dzk7zB9TVCv5ZXgf8HnODuyzsbZ5N4q6NY3miyrgT4HqHT/lOE74SNbG4CBuH1vURI7B4nzCfTKVFCNx243t2XETrTjzWzsztbpgiANWtFItKtRM2mniV8ya0E+gG3AnPc/Y/pjK0pMxuzvTeFikZvOxS41N2/kO54REREJL006pd0d0sJd5KeJgxjuQF4jM3V5Blhe09SIg8S2n5PSXcgIiIikn6qURERERERkYyjPioiIiIiIpJxlKiIiIiIiMhWM7NB0YSlmNlMM3vezH7Y5PkW69qyzfRRGTBggA8fPjzdYYiIdGsvv/zyWncfmO44MlH+/t9SW2rpsPUvZsy4LbINyc9OzbQOHfncqnj1j8kc8zdAvpmdCsTd/VAzuzWaM23v5uvcfXFbhW0zicrw4cNZuHBhusMQEenWzGxp+1uJiMj2xswuBC5ssmq6u09v8vwEoAxYBRQRhsAGeAI4gjA8dvN120eiIiIiIiIiKWTJ9wKJkpLpiZ4zsxzgR8ApwANAIWHyaID1wAGtrGuTEhURERERke4oFk9VSVcAf3L3DWYGUArkR8/1IPSLT7Su7fBSFZ2IiIiIiGxDzJJf2nYMcLGZzQX2A75MaNoFsC+wBHg5wbo2dUmNipnNBPYCHnb3nyd4fgTwR6AX8KK7/09XxCEiIiIiIq3oQNOvtrj7+MYiQ7JyEjDfzIYAxwOHAJ5gXZtSXqPStJc/sGvUy7+564CfufuRwFAzK0p1HCIiIiIi0obU1ag0cvcidy8hdKhfABzt7hsTrWuvrK6oUSmi/R79uwOvRI8/A3p3QRwiIiIiItKaFNWoJOLuxWzOCVpd15auiK55j/5BCbb5O3C1mX0ZmAQ8naggM7vQzBaa2cI1a9Z0QagiIiIiIt1UF9SopFJX1Ki026Pf3X9uZkcA3wNud/fSRAU1HQZt3LhxmkhLRLpcRW0Fn276lLx4Hv3y+lGYXYil8AO6rr6ODVUbKK4sxnH65vWlT24fsmLtfxxX1Fbw8caPWVW2inGDx9Erp1dSx6yuq+b94vd5a+1bxCzGGXucsbUvQ0REtgepG/WrS3RFotLQo38BoUf/e61s9xowDPivLohBRJpZX7met9a+xaK1iyipLmFIjyFhKRxCXlZeq/vlZ+XTN68vufHcxnU19TVsqNzA+sr1FFcVU1xZzPrK9Y3b58XzyMvKI9akSjlmMfKz8hufjzf5cMyKZTGoYBA58ZxW46itr2VD1QY2Vm2koraicVleupwPN3zIhxs+ZGXZSob3Hs7Y/mPZe8DeDOkxhMraSirrKqmorSDLssjPDsePWYxVZatYXrqcFaUrWFqylA82fMDy0uU4m++LZMey2aFgB0b0HsFuvXdjtz67ATTut6p8FWU1ZVTUVlBZWwlA37y+YcntS1VdVThPleE8bajasEX5AIbRO7c3hdmF5MXzyM/KJyee05gguTury1ezonRF4775WfmcMOIEpoyewuh+o1lXsY6PNn7ERxs+YnX5aoqrillfsZ5V5atYXLyYmvoaAPYesLcSFRERCbqw6VcqdEWi8gBb9uifYmY/d/cfNtvue8Dv3L28C2LYwh2L7uCe9+/p6sOIZKzy2nI+K/8MCBfFeVl5VNRWdKiMwuxCeuf0prSmlJLqkpTHaBgDCwayU4+dyM/Kb7zwL68tp7iyuM1j9szuyW59dmPvAXvz4cYPuWXFLdR7fdLHzo3nsnPPnRkzYAwnjTyJ4b2GU11XHRKwqvWsKlvFxxs/5sWVL1JdXw2ExGtQwSAGFw6mf17/kIBFCV9DYvLxho/JzcqlX14/duuzG31zQwLTL68f/fL6gdGYwKyvXN+YfJXXllNdV9305DB2wFhOHnkyI/uMpE9uHx7+6GEe/uhh7l18Lz2ze7KpZlPj5jGL0Se3D/3y+jEgfwBf3eurjO0/lrEDxrJj4Y4d/MuIiMh2K01NupKV8kTF3UuiUbwmAte7+yrg9QTbXZ3qY7dmh4Id2LPfnp/X4UQyTlYsi9377s7YAWPZq/9e5Gfls7FqI8vLlrOydOWWF8VNOE5FbUXjhfTGqo0UZhfSL69fY61Bw0V337y+xIg1XmxX1lVuUXNQW19LVW1V4/NNE4mquqrNtRtlKyitLiU/K5/eBb3Dz9zejcfondubgqwC8rJCzcPgwsEMzB+4RfOs8ppy3l3/Lmsq1mxRi1PrtY3Hr6uvY3DhYIb0GEL/vP5JNe+qq69jeely4rE4OxTsQHYseyv+KlvnoMEH8e0Dv80/PvwHH2/8ONT49NmNXXvvyg4FO2xRmyUiIpJQhn9XdMk8Kh3t0d/VJo2YxKQRk9IdhkhG6ZPXhz55fRjTf0xqy6VPSsvrjILsAg4YdEDKy43H4gzrNSzl5XZW79zenLPXOekOQ0REtlXdMVEREREREZEMF+9+nelFRERERCTTdbc+KiIiIiIisg1Q0y8REREREck4qlEREREREZGMoxoVERERERHJOKpRERERERGRjBPTqF8iIiIiIpJp1PRLREREREQyjpp+iYiIiIhIxlGNioiIiIiIZBwlKiIiIiIiknFS2JnezPoBBwKvuvvaVJSZ2WmUiIiIiIh0DbPklzaLsb7AP4GDgWfNbKCZfWJmc6Nl72i7mWb2vJn9MJnwVKMiIiIiItIdpa7p1z7Ad9x9QZS0nAfMdvfLGw9ldioQd/dDzexWMxvl7ovbKlQ1KiIiIiIi3VEHalTM7EIzW9hkubChGHd/LkpSxhNqVSqAE83sxagWJQsoAuZEuzwBHNFeeKpRERERERHphqwDwxO7+3RgehtlGXAmUAy8Chzj7ivN7A7gBKAQWB5tvh44oL1jdkmNSnvtz8ysr5k9EmVjN3dFDCIiIiIi0joLNSVJLe3x4GLgDWCIu6+MnloIjAJKgfxoXQ+SyENSnqg0bX8G7GpmoxJsdg7wV3cfB/Q0s3GpjkNERERERFpnMUt6abMcs8vNbGr0ax/gz2a2r5nFga8ArwMvs7m5177Akvbi64qmX0W0bH/WvKPMOmCsmfUBdgY+TVRQ1PbtQoBhw4Z1QagiIiIiIt1TR5p+tWM6MMfMLgDeAsYDfwUM+Ie7P2VmvYD5ZjYEOB44pL1CuyJRSab92b+ALwGXAu9E27XQtC3cuHHjPOWRioiIiIh0U6lKVNy9GJjYbPU+zbYpMbOiaLvr3X1je+V2RR+VZNqfXQ18w91/CrwLnNsFcYiIiIiISCtS2UclGe5e7O5z3H1VMtt3RaKSTPuzvsDeUbu1LwCqLRERERER+TxZB5Y06IpE5QHgHDP7HXAGsMjMft5sm2sJTbo2Av2A2V0Qh4iIiIiItOLzrlHpqJT3UUnQ/mwVoad/021eBMak+tgiIiIiIpKcWCyz537vkgkfow41c9rdUERERERE0iJdNSXJ0sz0IiIiIiLdUWbnKUpURERERES6I9WoiIiIiIhIxlGiIiIiIiIiGcdiSlRERERERCTDqEZFREREREQyjhIVERERERHJOEpUREREREQk4yhRERERERGRzJPZeYoSFRERERGR7igWi6U7hDYpURERERER6YbU9EtERERERDJPZucpSlRERERERLqjTK9RyeyGaSIiIrJN2aFfT56ffTkAN119FnNv/x8uv+C4LZ5/auZlaYpOtgW1tbVMOqaI86edw/nTzmHx+++lO6TtlpklvSRRVj8zm2hmA1IVnxIVERERSZlrv30K+bnZnDxhX+KxGEVf+y0jdhrAbsMG0qdnPrf89BwK8nPTHaZksMXvv8ekE77EzNvuZOZtdzJq9z3SHdJ2K1WJipn1Bf4JHAw8a2YDzWymmT1vZj9ssl2LdW1RoiIiItKMme1lZleY2Y8blnTHtC046qDdKausZvW6TYwfN4p7n3wFgKcXvMth++1GXb1zzhW3sqmsMs2RSiZ7443XmPfcXM6eMplrfvQDamtr0x3SdstilvTSjn2A77j7L4DHgQlA3N0PBXY1s1Fmdmrzde0V2iWJSnvZkpn9PzObGy2vmdnNXRGHiIhIJ80BlgHPNVlaMLMLzWyhmS2sXbvo84wv42Rnxbny65P40f8+CEBBfg4rPtsIwPqNZQzq35NNZZWUlCpJkbaNGbM3N8+YxV/v+ju1tbX8a37Cf36SAqmqUXH359x9gZmNJ9SqHEf4HAV4AjgCKEqwrk0pT1SSyZbc/SZ3L3L3ImA+cEuq4xAREdkKq4HZ0Zfvc+6e8ErJ3ae7+zh3H5c1YMznHGJm+e55E5k+Zz4bSysAKCuvIj83G4AeBbkZ32lXMsfue4xm4MAdANhrzFg+Wbo0zRFtvzqSqDS9MRMtFzYry4AzgWLAgeXRU+uBQUBhgnVt6ooalSKSzJbMbCdgkLsvbOX5xhOyZs2alAcqIiLSitcJ7az/n5lNNbOp6Q4o0034wmguOnM8j9/y3+yzx06cMH5vDttvNwD23n0nPlmxPs0Ryrbiqiu/x3vvvktdXR3PPvMUu+8xOt0hbbfMkl+a3piJlulNy/LgYuAN4DAgP3qqByHnKE2wrk1dMTxx82zpgDa2vRi4qbUnoxMwHWDcuHGeqgBFRETa8Xq0OBk/00BmmHj+DY2PH7/lv5l82c08detl7LhDb449bC+O+tpv0hecbFMu+sbFXPn9/8GBo4omcMihh6U7pO1Wqmo6zexyYKW73wH0AX5FqKxYAOwLvEdoTtt8XZu6IlFJKlsysxhwNHBVF8QgIiKyNR4ELgf2BN4Cfp3ecLYtx339f8PPC/6XLx4ymt/d9uQWfVManhdJZOSo3bnn/ofSHUa3EGu/k3yypgNzzOwCwmfmA8A8MxsCHA8cQrjxM7/ZurbjS1V0TbzM5uZe+wJLWtnuSOAFd1dNiYiIZJrbgXcIycpi4I70hrNt2rCpgnuffJXV6zalOxQRSaAjTb/a4u7F7j7R3ce7+zfdfSOhO8gC4Gh33+juJc3XtRdfV9SoPMCW2dIUM/u5uzcfAew4YF4XHF9ERGRr9Y2aMAC8F90lFBHZrqSwRqUFdy9mc7/1Vte1JeWJiruXmFkRMBG43t1XEdr5Nt/uB6k+toiISIo0DJ3/AqF5wqtpjkdEJOUyfTC+rqhR6XC2JCIikknc/VIz+xKwF/CAuz+S7phERFIt04cN75JERUREZFvn7g8DD6c7DhGRrpLheYoSFRERERGR7igW64pxtVJHiYqIiEjEzH7n7t8xs2cJQ2lCmEfF3X1CGkMTEUk51aiIiIhsI9z9O9HPo9Mdi4hIV1MfFRERERERyTgZnqcoUREREWnOzHoCvYFNwKnAU+7+aXqjEhFJrUyvUcnsHjQiIiLpcR+wG3ADMBK4O63RiIh0gVTNTN9VlKiIiIi0lO3uzwE7uvtVQH26AxIRSbVYzJJe0kFNv0RERFr61MxeBW43s3OAFekOSEQk1TK96ZcSFRERkWbc/Rwz6+fu681sKDA73TGJiKRahucpavolIiLSXNSZvsDMegMTgR3THJKISMqZWdJLOihRERERaUmd6UVku6fO9CIiItsedaYXke2eOtOLiIhse9SZXkS2e+pMLyIiso1RZ3oR6Q4yPVFJuumXmZ1hZrldGYyIiEgmsPDtfZiZnQ/sDOyQ5pBERFJue+qjsifwrJndbGaHt7Whmc00s+fN7IftbPcnM/tyB2IQERH5PNwNHA1cRPiu/Et6wxERSb3tZtQvd/+Jux8G/A24w8wWm9m05tuZ2alA3N0PBXY1s1GJyjOzI4HB7v5Q50IXERHpMgPd/X+AUnf/Nxp8RkS2Q6mqUTGz3mb2qJk9YWb3m1mOmX1iZnOjZe9ou6QqMxok3UfFzM4AzgZ6ANcB9wKPALc127QImBM9fgI4AljcrKxs4BbgETM72d0fbOWYFwIXAgwbNizZUEVkK9TU1LBs2TIqKyvTHYqkUV5eHkOHDiU7OzvdoaTLYjO7FdjRzK4G3k93QCIiqZbC0bzOBn7n7k+a2U3AFcBsd7+8YYOmlRlmdquZjXL3xa0VCB3rTL8X8G13/6jJAc9NsF0hsDx6vB44IME2U4G3geuBS8xsmLvf2Hwjd58OTAcYN26cdyBWEemkZcuW0bNnT4YPH57xneyka7g769atY9myZYwYMSLd4aSFu19oZicD7wLvAT9Nc0giIikX68D3fNMKhMj06Fodd/9Tk/UDgU+BE83saOBNQjPaItqpzGgRX9LRhVqUflGg55tZjru/nWC7UiA/etyjlWPsT3hxqwjtfo/uQBwi0oUqKyvp37+/kpRuzMzo379/t69Vc/cH3f366KdulonIdqcjTb/cfbq7j2uyTG9Znh0K9AWeBI5x94OBbOAEWlZmDGovvo4kKncDY6LHg4C/trLdy4QMCWBfYEmCbT4Ado0ejwOWdiAOEeliSlKku78HzOy1dMcgItLVUtmZ3sz6ATcC5wFvuPvK6KmFwCiSq8zYQkcSlb7ufjuAu/8SGNDKdg8A55jZ74AzgEVm9vNm28wEjjazecA3gd90IA4R2c699tprvPbaa53e/7LLLuv0vkVFRZ3eV7Yrt5nZpekOQkSkK8Us+aUtZpYD3ANc6e5LgTvNbF8ziwNfAV4nucqMLXSkj8oyM7sceBE4CPgs0UbuXmJmRcBE4PqoedfrzbbZBJzegWOLSDfSkKTst99+ndr/hhtuSFks0m2dTOhIfxZQAbi7T0hzTCIiKZXCzvTnE/qlX2VmVwHPAncCBvzD3Z8ys17AfDMbAhwPHNJeoR1JVKYROtBMJnQu/FprG7p7MZs7y4jINuonDy3i7RUlKS1zryG9uPrLY1p9/sorr+T+++8H4M477+Tpp58GQk3HQQcdxBtvvMHjjz9OaWkpkydPpqysjJEjRzJr1qzGMoqKipg7dy4A11xzDTU1NcyfP5+SkhIee+wxBg8enFSsVVVVTJs2jRUrVjB06FBmzZpFXV0dp59+OiUlJfTv35977rmHmpqaFuuysjry8SqZxt3Vd1JEtntGahIVd78JuKnZ6p8026Z5ZcbG9srtyDwqVcBdhE71DxI6xIuIpNS1117LFVdcwRVXXNGYpAAsWLCAQw89lMcffxyAlStXcskll/DUU0+xZMkSVq9e3WqZH3zwAfPmzePUU0/lmWeeSTqWW265hbFjx/Lcc88xatQobr31Vt5++21isRjz5s3j3HPPpbS0NOE62baZWczMTjGzb5vZl627d9oRke1Sqpp+Jcvdi919TtTiql0dmUdlJjCC0JO/HHA2tzMTke1QWzUfn7exY8dy6qmnNv6enZ3NjBkzmDVrFuvXr6eioqLVfadOnQqE+Ziqq6uTPubbb7/deMxDDjmERx99lIsuuoixY8dy7LHHMmrUKCZNmsQBBxzQYp1s8+4iNHF+gzBazX8BZ6U1IhGRFMv0ezAd6Uw/EphEGLHrKKC+SyISkW4vPz+f8vJyIMzpAdCjR48ttpk5cyaTJ09m9uzZFBYWtllee8+3ZsyYMSxYsAAINTpjxozh9ddf5/DDD+eJJ56guLiY+fPnJ1wn27wd3P1b0XCc/w/YMd0BiYikWqpmpu8qHUlUyoEvAnFCR/i+XRKRiHR7EydO5L777uPwww9v9aJ/4sSJXHvttUyYEPo3L1++POF2W+OCCy5g0aJFjB8/nsWLFzNt2jSGDx/OH/7wBw477DBWrVrFuHHjEq6TbV65mV1hZhOjjqEbzWx8uoMSEUmlmFnSSzpYsnNYmVkh4Y5SDaFn/5Pu/rndNhw3bpwvXLjw8zqcSLf1zjvvsOeee6Y7DMkAid4LZvayu2/3mZiZXZ1gtbt7qzPU5+//LU0KKR22/sU/pjsE2QblZ6emF/zkWa8k/bn193MP+NyzlaT7qLh7GaHZF8CPuyYcERGR9HP3n5jZWGAn4BPgU3fXKAkisl3J8C4qyTf9MrNHuzIQERGRTGFmNxKG1rwW2BX4W3ojEhFJvUxv+tWRgf7fNLOT3f3BLotGRERkK5hZuzX+bTXfamJvdy8ys2fc/WEz+34KwhMRySgZXqHSoUTlIOASM3sTKEOz9IqISOZJ1ffumijp6WtmXwOSGvNfRGRbkunDE3ekj4pm6RURkYzm7j9pf6ukTAUuBJ4HegPnpqhcEZGMkaqJHLtKR/qoTG2+dGVgIiLJKCoqarHusssuS7jtNddcw9y5c1st67XXXuO1115LurzOShSzZAYzGx8NQ3wQ8Cph4sfXgO1+pDMR6X5iMUt6SYeONP1qiDCfMPHjWuCOlEckIrKVbrjhhk7t15Ck7LfffikpTzKPmY109w/a2KSh9cBRhOH4Xwb2A3oCR3ZtdCIin6/tqenX7U1+/bOZ/akL4hGRTPLoFbDqzdSWOXhvOP5XrT79i1/8gjFjxvCVr3yFa6+9lpEjR3L88cczefJkysrKGDlyJLNmzWrzEEVFRY01J8XFxZx++unU1dXh7hQVFVFaWtqivCuvvJL7778fgDvvvJOnn346YXlVVVVMmzaNFStWMHToUGbNmsUvf/lLampqmD9/PiUlJTz22GMMHjw4qdORqLy6ujpOP/10SkpK6N+/P/fccw81NTUt1mVldeReU/djZl8Ffg3s0GR1KaEpV0INTcfM7Gl3P65JWc90VZwiIumyPTX9Gt9kmQzs1YVxiUg3dfrpp/Poo2E09Hnz5nHCCSewcuVKLrnkEp566imWLFnC6tWrky5v+vTpnHjiiTz77LNkZ2cDJCzv2muv5YorruCKK67YIklp7pZbbmHs2LE899xzjBo1iltvvRWADz74gHnz5nHqqafyzDPJX9MmKu/tt98mFosxb948zj33XEpLSxOuk3b9HDgAeA4YAXwLuC3JfevN7FIzO8rMvtlF8YmIpJWZJb2kQ0duxzXtTF8FXJziWEQk07RR89FVdt99d5YtW0ZJSQl9+vShsLCQ7OxsZsyYwaxZs1i/fj0VFRVJl/fxxx9z5plnAjBuXOhmsDXlvf3225x66qkAHHLIITz66KP07duXqVNDt71hw4ZRXV29VeVddNFFjB07lmOPPZZRo0YxadIkDjjggBbrpF1ZwAbgWeBw4GZgOfDfSex7OqEz/RTChI+nd02IIiLpk+EVKsnXqADXAw9H1eJrgMVdE5KIdHcHH3wwN9xwAyeddBIAM2fOZPLkycyePZvCwsIOlTVs2DAWLVoEbO6D0lp5+fn5lJeXA+DuCcsbM2YMCxYsAGDBggWMGTMGoMNxtVXe66+/zuGHH84TTzxBcXEx8+fPT7hO2jUTeAh4APgZcCfh+6td7r4ByHH3/+fu17r7uq4KUkQkXeIxS3pJh44kKncDY6LHg4C/trahmc00s+fN7IetPJ9lZp+Y2dxo2bsDcYjIdu7000/nhhtu4MQTTwRg4sSJXHvttUyYEKZuWr58edJlXXjhhdx7770UFRVRUlLSZnkTJ07kvvvu4/DDD281EbjgggtYtGgR48ePZ/HixUybNq2zL7PV8oYPH84f/vAHDjvsMFatWsW4ceMSrpO2ufvVwLnu/iYwDXgF+HIHitBcYSKyXcv0pl/W2l3DFhuazXf3I5v8/myiuVXM7FTgJHefZma3Ate6++Jm2xwAnOnulycb6Lhx43zhwoXJbi4infTOO++w5557pjsMyQCJ3gtm9rK7bxNZUjTMcAvuPi/J/RN+z7Umf/9vJfeFKtLE+hf/mO4QZBuUn52aVlsX/X1R0p9bN08e87lnKx3po7LMzC4HXgQOBj5rZbsiYE70+AngCFo2EzsEONHMjgbeBC5y99rmBZnZhYQ2wgwbNqwDoYqIiNAw+aMBOwG7Av8iDD0sItLtxTJ8eOKONP2aBpQDk4Ey4GutbFdI6KwIsJ7QTKy5l4Bj3P1gIBs4IVFB7j7d3ce5+7iBAwd2IFQREenu3P3oaCly91GEZl+vd6CI97ooNBGRjGCW/NJ2OdbbzB41syfM7H4zy0nUFaS97iHNdXTCx+fd/UYzOx+ob2W7UsKkkAA9SJwMveHuVdHjhcCoDsQhIiLSYe7+iJl9N5ltzWwH4Dkzm9pk/zYnOb591g+2MkLpjjL8hrZs51LY9+Rs4Hfu/qSZ3UQYMTHu7oea2a1mNgrYu/m65t1DmutIjcockutM/zKhuRfAvsCSBNvcaWb7mlkc+Aodu8Ml8vmrr4eK4nRHISIdYGazoi/DhuVJkr9B9xiwG+EmXcMiIrJdiZslvZjZhWa2sMlyYUM57v4nd38y+nUg8FVadgUpSrCuTR2pUenbMDu9u//SzJ5tZbsHgPlmNgQ4HphiZj9396ZVPD8F/kb44P+Huz/VgTgy10fPwbBDICs33ZFs3+rroWoj5Pdtf9sPn4Gl/4EJSdUwtu7VO+GxK+Hbb0FBv60rS0Q+L3Ob/V5CSECSscndf57acEREMktHRh129+nA9La2MbNDgb6EioqmXUEOoGX3kAPajS/58EJnejM7OupUn7AzvbuXEDKmBcDR7v56syQFd3/L3fdx973d/aoOxJC5PnkB7jgJXpqR7ki2fy/fCr8fC2Vr29/2pZkw79dQmtTUCa378BmoKYMlmruiq/3xj3+kqKiI/Px8ioqKuP/++ztcxmWXXZb6wDJFbRWsfhtqKtMdScZz99ubLsCjJO43mch8M5ttZseb2fjWRhATEdmWxSz5pT1m1g+4ETiPxF1BkukesmV8HXgt09iyM/3U1jZ092J3n+PuqzpQ/rbttb+En+8+nN44uoMPn4XqUnj7wfa3XRm1KlyS1GikibnDpy+Exx891/lyJCnf+ta3mDt3LjvttBNz587llFNO6XAZN9xwQ+oDyxSVJVBXBVWb0h1JxjOzR5qvAp5Pcvca4F3gIOBowg04EZHtSqrmUTGzHOAe4Ep3X0ririDJdA/ZQtJNv6LO7zea2U7AJMIMv2cku3/GKl0D838LR18Jeb07V0Z1Obx1P8Rz4ZPnoWwdFPZPbZzbqrK1UDggdeW5w7KXwuNF98NB57dx7HWw8dPw+ON5MPa0zh1z4zLYtBIsFsrpRq578TreXf9u4ifr68DrIJ7ToTJH9xvN5QcnPYVSo6KiIg466CDeeOMNHn/8cUpLS5k8eTJlZWWMHDmSWbNmbbHt3LlzAbjmmmuoqalh/vz5lJSU8NhjjzF48OAW5Scqr7KykmnTprFs2TL69OnDnDlziMViLdZdf/31FBUVUVRUxG233QbAtGnTkoo50TGuu+469txzT6ZMmcI111zD6NGjmTJlSgi0OkpQais6fA67of0SrEt2zoBfEu4K7gm8BdyWmpBERDJHCiecP5/QlOsqM7sKmAWc06QryCGEz9/5zda1HV97G0TDix1jZr8xs9cIH9jDgD919pVklMd/AC/cBK/+pfNlvPvPcPHwxR+D18P7CZpAJzmxJu7w8HdTf+e+rgaWvxz6dzRXWQJzpqa+NmjZy/DrkfDKnakrc8MnULoaeu8MS/4Fm9qotFsV1abk99u6BGPZi+Hn2NNg3WIoWdH5siAkttuMVt637lBbCXXVIVn5HCxYsIBDDz2Uxx9/HICVK1dyySWX8NRTT7FkyRJWr17d6r4ffPAB8+bN49RTT+WZZ55JuE2i8qZPn86+++7Lv/71L0477TTeeuuthOsS8fp6Fjz/PAfvvXebMScqb+rUqfztb38D4PHHH+fkk0+OCnWoKg2Pa5SotMbM/tvMPgYGmtlHDQuwCkh2dr1bCc3EHiXMwTKr7c1FRLY9qRqe2N1vcve+0XDwRVFz2yI2dwXZmKB7yMb24kumRmUtkAv8H/BF4O/ufnUS+2W+Jf+CN+eAxeHVv8Ih3+zcOIGv/RX6DAv7L7gpXPDvf/bm5+tq4dbjYMAo+MpNbR/j4+fgpVvgnX/AxS8k12G8LVWloSP48/8Xahf2OxtOuhFi8fB8bTXc/dVw3KXPw4ijILdHx47hnvg1vfV3wOGxK2DEeOi7y9a9Fthcm/LFq+G+C2DRA3DINxJv29Ds66ALYN71sOFT6LNzx4/56YuQXRD+vm/eE5Kefad0Knzefzyc78m3wp5f7ti+7z0G2Xmwa1Hnjt0Jl+92OoyohX67QTz6uHCH9R9CVVloSJPXJzV/23aMHTuWU089tfH37OxsZsyYwaxZs1i/fj0VFa1fuE+dGlqqDhs2jOrq6oTbJCrv3Xff5bTTQk3ctGnTALjttttarHvkkc0tjCoqKsjPz6eupIS9Ro7ky1/4Au6OmSV9DDNj06ZNzJ07l7Fjx5KfHzXpra0MiWEsK3rcyr+9Bu4hmex+A3zcBjxImKC46czy69y9NMkydnb3c6LHj5uZ2n2KyHYnqwvHx3b3YjaP8tXqurYk00dlF0J/lD6EGX3HmtllZrZP8qFmoLqaUHPRexgc+zP4bBGsfK3j5WxcFmo/9j0LYjEYfULoeN30rvmi+2D5Qnh9Nsz7TdvlvTQDcnuFJlOPd3KkqsqN4YL4sSvh92NCotB7Zzjw3JBU3f+NkDy5wz++FZKUQ74JZZ/Bf25M/jgbPoW/ngG/HR2O2ZQ7vP0P2GkcYPDgxYlrczrq0xchuxDGnAKDxoZz25qVr4cEcq/obnRnO8J/+gIMOQB23A8K+m9dbde//zdcOD5wMaz/OPn9qsvgvq/D3edAycqOHbO2KsTc0fNfWw1VJVBTHmqS6mrC+rK1oX9E7yGhtqqiOLyfuliPHlsm0DNnzmTy5MnMnj2bwsLCNvdt73mqNjHz5j+1KG/06NG89FJIjn/5y18yY8aMhOtycnJYsyYM2PDYY4/h7tStXUuPwkK8pob6kpJWY05UHsCUKVM477zzGpOshjgBKBgQam/rqmhT5Qb47O1uV/sS3blbAjzp7kubLMkmKQArzOxKM5sQNWNY1jXRioikT6pqVLpKu4lK1DH+bnc/z933BCYQamLaueLOcC/cDGvegeOvC7UM8dxQq9JcZQksfhKe+gnMPA7+eHAY4avB63cBvvkO+x4nhLbjH0WjN9fXhVGndtgL9jkTnv15602sNi6Hdx+BcefB4f8dOuh/8HTyr2n5KzD9aLhuOPztjJD0DD8Czn8SznsUvnwDTPhRqEW693x48sfwxt1w9A9h0rXhgv4/N7bdnArCBe8L0+FPh4Qkp3QVvNEsOV7+CpQsC7UZk34ZkoQXb07+tbTm0xdgpwPC3f0xp4TfN3yaeNuVr8OO+4ZzX9C/c82/qsth1Zuw88EhER1+ZCgn2aZ8W8TzBiz9NxwcDTv+93NDEpGMN/8ekobqMnisA/07ytbBHSeHEele7mDLlcoN4WfvnUNytXZx+PdQsjwk0wUDov5HDhXrO1Z2CkycOJFrr72WCRMmALB8eTTiYV11SKrWf7S5j1Jb6uuheAkTD9qDa3/5iy3K+/rXv84rr7xCUVERr7zyCuecc07CdSeddBI33ngj3/jGN+jfvz9eXU19ZSWWnY3l5FC7dl2rMScqD2Dy5MmYGUcc0WSY+erS0CeooT9deyN/VWwIPxv+lt2Mu59tZgUAZjY4msQxWVcQvuv+QRhGc27qIxQRSa+YWdJLOrTZ9CuakfcZd2+8k+TubwJvsi0nKiUrYO61MOo42OP4kCbueWJo1nPcLzY3kyheArdMgPJ1oanFjvuF5hZ3nASTZ4V9X/sb7HIE9BsR9hl+BOT2DgnH6C+FDt9r34fTb4PdJ4WLvfsuhAuegh323DKul28Ld0nHnQs9BsM7D8FDl8E3n0+uOdZjV4YLsyO/CyOOhKEHQXb+ltuM/2640HnyR+H3A88N6yA0p3r3kXBuvvy/iY9RXQZ/mQyf/Ad2mwAn3gBzzoGFs0JS0vBGfvuBcM72mBSaBr3zT3jqGhh5TGgC11z5+pAQ7j15c7O0Fscuh9VvwWGXht/HngrP/Cyc48Mv3XLbyo3hQnW/s1omGB35x7biVaivhZ2/EH4fMT68tvUfQf/dki8HQqKWXQBH/yCUc/dXQ7J4/HVt7+cOC2eGhGvsaeE1v/doeP+1Ze0H8LfTQwI8YHd45ufhnCXbnLCiGLLyQzKSlReae63/MPxd+wwL5zE7P9Rwla2FwoGdv+VSWxnu+tdUhkQ/u4APFm85WW1D5/gG48eP37J/SG0lrHkXaiqYe/cfQ+2D13PNVVeEJnNsblq15etcD/W1jD90HG8983cYuMcW78E5c1rWUDdfN3bsWObN25wIV3/yCfVlZcydP5+64mJqVq6kvry8ZcytlLdo0SLOPfdcfvCDH2weaaWhf0pe7/D3gHDO8vu0fE0QPkuqQk0OlRuh546Jt9uOmdnZhP6UvYHDCQPCXObuyTQ7eAC4D7g4+r0TdydERDJbumpKktVejcps4Pho/pSBn0dAXeKDp+HJq+HRy+Gh/4bZ/xXuuB7/q81/of3ODncd34vamtfVwL0XhJ9n3wtXfAJffxq+/ky4YLz7bHjke+HCbb+zNh8rng27HwvvPxr2nfdrGDga9jw5XNRN+SvkFIYYms4DUlsNr9wOo46FvsPDhdXJfwyJx9M/bf81LlsIny6AI/8HJlwVLoSbJykNDr809FM55Jtwwm82n4P+u4VRtF65Az5rZaSnJ38cRjY76Y/w1ftC34QDzw1N5xr6j7iHPja7FoWLYjM46Q8hnnvODRfOTW1cDrdOgvsvDE3TWtM8aei3KwzZP3Hzr1XRxeCO+4WfI8aHmoD1H7VefiINwxIPPSj8bOgf8nEHm3+VrYM37gk1b/l9Q/+UQ74JL/y5/WGWV7wSaofGnReStB32Cs0WGzpVJ7L0PzDzmHCB+rWHQmJduQGevTa5eGurQpOvhqQmtwf0HxkSlz67hPd5g8IBoQlSdUda1TRRthY+eyfcGChdFS6+N62EDUvDxXYy6utCU7raaug5JPyb22Gv8FzFutb3c4eyNeF19ds1vI6S5a1vn0wo1dXUlZQQ79sPi8WI9+mDxWLUrmsjjmbGjBnDiy++yPnnNxnVrrYi9E/J7RmS76y8tpt0VZWG85ddGLarraa+uhqv7fpmehnkl8A+AO5+LzAuWpeMTe7+8ybzsNzRVUGKiKRLKudR6ZL42nrS3Wvc/RbC5C1TzOw7ZtbJMXzTaOl/Qif312aHGoPy9eEudr9dN2+zaxH02mlz86+5vwoX3l++AUYdE5ILCBdlX3sIdvti6PSeXbi5D0SD0V8KtTBP/DDc4R3/vXBhAdBrCJz519C86vYvb56I8N2HwmhWB399cznDDgnNhF68uf2JJJ//Y6jJ2f+ryZ2TA6aG5l4NHaQbjP8+5PSApxKMl/DhMyGOQy+GA87ZnODsPTnsszBqWrTqzXDRuedJm/ftORhOnQHFH8Ofj4DFT4X16z4MSUrJCug/Kpz31pqzNIy+1ZA0AIw5NSQw6z7cctuGjvSDo65UI44KP5smGLVV4TW11b9i2UvhAr1huOl+u4b3SUf7qbxye7gIbmj2BXDMT0Ki9ejlbcfw0q3hfbbPmZCVE2q7SpbDs79IvH1tFdx1VmjudsFTMOwLMHhsSHRemhEmC2xPQ5Ohpnfrcwphh9GQ12vLbfP6hAEpkpmAs7m62vC3z+kBA/aAwVFTvZ47hhqd9R+HJKSBe8u+Nu6hr1htZUjyew4KSXE8OzRRKy9uvaledWnYr8fAkAD0GBT+7VYUby67pqJDI7XVrVsHZsT79wPA4nHifftRt7GE+lY68ielITHNiWpXs/LaHqK4qgQw6L0TAF65kZpPP6Xq44/xzjRd3HY1bZdYCmS3tmEzmvBRRLZ78ZglvaRDUhM+unu5u98IzAAuMLNLGtr9bhMm/BB+9Blc+Ql8bzF8+83QvKqpWBz2/S/48Olw53v+b8NFf6K5N3J7wH/NhsMuCbUXzZtljTwmNK964c/h4ntMswnrdj4Izro7XITdfiJsWh1mUO+zS0iAmpr4E9j9eHj4f+C5Xye+4CpeGu7KH/i1cLG1NQr7w5HfCUMsP/zdcIcawoXrAxeHi8kJzTr55/aEvU8PNRsVxSEWi8PoE7fcbtQxcOFz4SL0r6eF8m+dFGZ8n/YQfOm34QJ84a2JY/v0pTD6VNM5ahrObfNalZWvh+ZzPaNJqPvvFu60N/RTqa0KTa/uPAVmTQrNpJprmOixoQYHQnI24qjQ5ybZzul1tSFBGHHUls39snJCM71NK+GDJxPvW1EMb90L+5y+OUHY+eBQ8/XCn0O/l+Y+fCbsd9y1WybjR18Vynjs8vb72FQUh2ZqyYwWFYuFpKhy4+YO9001JBLrPtwy6QAoXRlqCXoPhZyCUJZZSGx77xwuttd9EN7ja94Nf9dVb4TkpqGs8nWh+VbPwS2TqIL+UF+zuQlUc6WfhaZseVHNUc/B4XVv+DQ001z1Rjju2vdCQtMOr6ujrriYeK/exLI3Xw+HpMWpW78VfXmqov4pWdG8Ndn5UFdN3aYSqqMEpOqDD6h87z1q164Nf4/cnuH1xHOoW7+e+ooKsgYObHfiru3IHwkJx/fM7HvAcyQ/PHHDhI8HowkfRWQ7tU3XqDTn7iXu/ltCk7BLzSyFM/l1oWS/lPc7KzSVuO+CcGF7/PWtbxvPhmN/HmoXmsvtGZoaQVSbkqDPxa5HwVf/HuYFmXFM6GR90Pmba14aZOfDmXfCPlNCR/zHf9DyAvmFm8NkhF+4KLnX2Z5DL4FDvxVqjG47ITTNeuyKUONzyp8TNykbd264kHv9rpCoDD888aSXA0aGJnQHTA3lx7Ph3MdCzcKuR4Warfm/aTnrtnuoUdn54C3X99k59BFaeNvmpAo2d6RvYBb+Jh/PCzU2c6bC4idCDce6D0Itzws3b3lu138ULoKbH3PE+LD+s0XJnM0wz07JcvhCgmGUdz8OCncIze0SeW12uGs+rtnElhN+FJorvTi95T5v3RdqOZoPY1zQLyQrH88LTfNaU1cTjtmRobEL+gMekq6mSZB7SCrK1oRkoXjp5udrKkItTMGAxO+pwgHQd0R4X1WVgGWFfjB5fcJ78bO3w/E2LotqQ1pO4kher5CIlCdodtVQbsGAzf/uLBZqZWLx8FlQ0D8kTFhIagjzo3htbeNSX11NXWkptWvXUrNsGV5fT1b/Ld/7sZwc4r16UVdcTG1xMV7Xxtwz7uE92qRZl9fX49WlW96IyM6nrsao/uRT6kvLoN5D5/3sbGpWraK2vCb0ZzGjPt6TmpIa4j17Eu+97VWKd5a7/xr4LmE+lN2B3wNjktz3J82WJNrgiohsW6wD/6VD0jPTm1kM6AGUA3sD/+fum9reaxvTfzcYdmiYGHHyrZube3XGoReHoVvbmg19+BHw1XtD5/R4Lux/TuLt4tlh/pX8vrDgT6Hfygm/DbUFlRvDRe6YU8Jd6VSIZ4VBBYYeFIYV/tOhULURjro8jLiVyI77hiF85/82XJS2NrcJhIvSk24MtTADdg93sRtM+DHMmBCa6h31/c3ri5eEcps2+2pwxLdDDc3rf4MDp4VmOmvfazlPyYjx8MZdIfla/jKc+PvQHOrI/4F/XAKPfj90Uj9tRrhI/rShqVmCRAXCBf/gvVt/nRASn+f/L9SW7X5cy+fj2WHOnX//IQw73KtJh2f3ULs09CDYsdlo4Pl9Qsf4t+4LTfgaLmBrKkI/qzGnbL7z3tSB54ZBGx67MryORMlITTmQGxKCZGXnhSSibE1IGPtG/VhKV4dhrwsHQDwvjAS3cVl4r5YsDzVvbXXyzu+zeYSrpjccqgeGBHrTKohlQ5/hiW9IWAzP70t98TrqSpZALIbl5hLLy8NqNoSP3sIt77d4PAfvOwqLx7F4dJOhpoL6TeuoK4tRt7EEb6U2zeJxsvr3J1bQMvHK2mEHqpcupWb5cmpWrCDeowfxHgXE8rOwuurQNLCmAmor8fp66muNeutBfW2M+ooKYjEjOy+38e5SfV2MmtIsLCtG7m4jsazwce719VR/uJiaMrD6HGLu1Gyowgyy+vXoNrUpZtaXUBNyDGG0yp5AP8LcKiIiQvpqSpLVkRqVe4DxhDtSFwD3d0lE6XbaTDjv8S3vxnfGbhPgtFta9gFpbpfD4IIn4ew54Y53a2KxcEE68adhjpQ/jgtDBL98G1RvSlyzs7XGfAW+/mzoVzP0oNBMqS3jzg0XqhiMTmIywxHjt0xSAIYeGJqM/efG0JeoQUNH/ea1GwAjvxhqZOb/LtQGrF4U7oY3/xuOODL8XP5yGERg3Hnh956D4aw5YQSzT56Hm8eHZmafvhD6OAwcvWU5vXcK6/5zY9iuLfN/G2qCjvyf1kcz2/+c0Pyp+UACHz4T5i9pXpvS4ICvhWZzb927ed3iJ0O/i7GnJt4nnhWSxNLV8M9vJ24CVlMekvREiU5beu0Uah+qS2HNe1EisTIkQ72Ghn4ghTvgpWupX/0hXrEpnPtm/0a+8IUvsDga8evB++9n2tSp1FdWUl9TszlByCkMo8f1242iM7+5RRleV8d/X3IJ9eXl1Hy2hqqVpVSXxqmvKOenv/41zzzySGgqtWoTVRuyqV6+itp166hdv56XHnuMFx94gKr336fynXeofO89qpcu5ZJv/5iqjVnUFm8g1rMX2TvuuHkZMoSc4cPJ22MPckePJnvHxIlXLC+P3N13J3fXXcnq14/6slKqV66m6uNl1KxeRX3ZRuqqoboyn6qNuVRvyqK2JNSsZBXmhEG8lq+l5rPPqK+qovrT5WCQ07+gMUkBsFiMnN6GxaFm2QpqV6+mvrKKrEInVtvJAQ+2IWZ2vZm9DCwETop+9nX3ndz9NHdvZ5g9EZHuI9ObfiVdowL0d/d/mtm33H2Smf27y6JKscr33qP6ww/b37Cp1x9pf5tUezeZY46C3a8Ps83fHA0xPGA/eG1lWLrC8B+HC/8nnmp7u9qesKJfuFs+/+XOH8+LYPHT8OsLwmhs2fnwygOwvA+8vBQswdwYsWPhjf+Fm68J/U8+yYO3iuHjZuc058shvrU7wSPNz/cgGHFNqAG55iuhf0a/PeCxx1oer+95IVG55iuw33/BrhNoUSu66k341+9h56Nh1cAEx2uiYn+45zbYtGeoGagsCXP3xIbCp/mwIsG+DmwYCX+7GVZH00MsuAU+2wHeKd88gl0iPc6ER++FNT+C4YdtXr/hU+qHjaCuLh82bmx9/1ZlQ+4QKFuDV6yFeAFelYevWIHX1OCVlXhtNlAJZGPlG4n1qCWWlwf19XhtHRMPO4xHZ89m59PP4Il772XCvvtS1eTfr8XikBXHsrKwrCy8qpqqDz8MTbHq6qC+nl9ddBFVH4UR3mI9epCdVUksx8nq14+cwf3I7Qv1VXXUZ/WmvqKcupLwWl979VUsN5cDx4/H6+rwqiq8sopfX3kl8TzIildgg1smV8kyM6yggFj1OrLqq6nPHkBtWQ21ZeVRF5gaLBYj1qsX8V69iMVrsJJl4NVk5eVQU9Ob2s8+o/azNVg8Rk6/bGLebC6eulqstoycHQZS/dkmateuJdajB/FehKZuHR2ie9uzE7Aj8AmwMlpSMNusiMj2J9Nr2S3Z0V/M7CGgDlgE/Bu41N0ndWFsWxg3bpwvXLiwU/t+9vsbWHdzCiYbFOkGav7vj4waFAYhWDdrFtUfL9nqMq1hWttYjNzdd2fQxefhlktdZTVeXr65psSMBW+8wf/edhtzbr2VA489lqcffpjc7GzOmDqVsrIydttlF275zW/CMLu1tUz86ld58u67Q+ISj0NWFseccgrPPPIIlpvLxvJyTj/1K9RVlePANd+5kHEHHsjki75PWUUVI0eOZOaf/8wPfvhDHnjoIQB22mknnn5682SrRUVFzH3yMVjzDlXZfZl26ZWsWLGCoUOHMuvWmfzy2l9RU1PD/PnzKSkp4bHHHmPw4JZ9ZkpLS5l8ysmUlaxn5KjdmfWXu6msrORrU6eybOlS+vTty5y//514VhbTpk1j2bJl9Ondizk3/Yrrb7qDouNO5MgDDuDWP/+ZeO/enDflBIqOP4WDDivijTff5PHHH6f0s0+YfOYUympgtxG7cfPPfkZd//6cd97XWLb0Y/oMGMScv9/Hddddx5577smUKVO45pprGD16NFOmTNki3nfeeYc999xyvicze9ndx231m6KLmdlewBcJzb7GAx8BLwAvuPudXXHMOa+t6FbDqUlqnDR2SLpDkG1QXlZqOo38bt5HSX9ufWf8rp97VtOR24KnA3u5+ytmti9wZhfFlHL9pp5D75OSaIokmWX1Inj1L6EJlHtoWnbI/2t9+w+fgUevCH0jhuwf5qHprPr6MMDBTgeG0aja2m7hTHhxRqhRGToujNL2xuzQ5+SM26PO2O2orYJZJ4Q+Uj12CK/7mB+3HDmtuYqNcNuXYMzJYc6Yx6+Cr/wpxNGeklVhGOO+w0J/lKX/gR334cP+/cgdORKArN59qM1vZT6e9lj0P9uyC16sIJ/4jiNC+WzunG6xGMTjjB89mvOvuorP3Cns2ZMdhg9n8eLFXPqd73DMMccwadIkinNyGLRzOK+x/Hxyhw/f8tBZWcR7hRHApt9wAyeedBKXTZnIxCnfgJ6DWVnTg0v++9uN5a3ZsIFf/frXjB4T+lknnBgyOw/yenPLTTczdq89mT3jRq655hpu/f3PoKKMDz5ewbx58/jpT3/KM888w1lnndWiiJWffMwl53yFY744gUlf/RarV6/m7rvvZr/99+fuOXOYNWsWi95+mwULFrDvvvty1113MWvWLN5aVdk4umC8Vy+yd4hq0LLyWfDKm1z67e/x69+EOXhXLv2AS84/m2Mmn8+k449nQ34+d8+axb77H8hd//tjZj3wDG+99RZTp07l29/+NlOmTOHxxx/n8ssv78xfOWO5+9vA24RJHmPAgYSk5atAlyQqIiLbmnTNOJ+sjiQq1cAHZpZF6JDYwZnz0ierf/8Wo/DINmC33eCwk8Kwtm/dG4aL7tXGnacRI2DxTFjzDux9cNh/a4waleR2v4KJF8Cbc+CNu2Hhj8P68+bA7kXJH2/8GWEY48/q4Ivnw5f+O7n9Dv0SLH4EstfCTv3hyMmt94fZwm6Q9Vu493yoK4AzfwEHfx177/3QFAsY/OMfJR9/J1kshuVs7g8Tj8c58MADue666zj22GMByM7OZsaMGcyaNYv169dTUdHG/CHNfPzxx5x55pkwaC/GHToecnuSnZPTufJ6DOLt9z/k1OMnwMZPOeTAfXj02efp2yOHqScdBRUbGDZsGNWJ5kupryW7bBUzZj/ArAefbTzuu+++y2mnhUE3GhKk2267rcW6R5o0HayoqCA/Px+y8xm7x26cemI0UEPVJrLrK5lx10PMeiDBMXJ6MO20SbDDnpgZmzZtYu7cuYwdOzaUt51y93rgpWhRHxURkUi37ExvZjPN7Hkz+2Fr20TbDTKzVzsQg3RH/XcLI4C1laRAGHBgfNThv7XRybrKgJFw9A/g0tfg/CfhnAcSj/LVlgOmhk71O40LAyd0ZL/KjWEulr1OTjJJiew9GabMhm8uCCO1dWTfLjRp0iT+/Oc/M2lSaF06c+ZMJk+ezOzZsyks7NhofMOGDWPRokUQy+K1119vs7z8/HzKy8PkjgmbxeYUMmaffVnw1gcwYHcWvLOcMQceCgUDKCzsGSY0bToIRFNl65j51zlMPvMsZt91d+NxR48ezUsvhUEZfvnLXzJjxoyE63JyclizJkwQ+1hDv6msPHoUFoThpMvXw7oPmXn3Q0w+87+2eG2N5fXeiV9O/zszZoQJZKdMmcJ5553H1KlTO3RORURk+9DQMjuZJR06kqj0d/d/AqPc/Wwg4e03MzsViLv7ocCuZtbWbenftFaOSKeMPQ2mPth+k6muYhZGJtvt6I7vO3hsiP3se5KbaLHB8CM3T+w4ppXRvtoy+oQwnHAGOe644+jRowdf+EKYbHPixIlce+21TJgwAYDly5cnXdaFF17IvffeS1FRESUlJW2WN3HiRO677z4OP/xw5s+fn7C8C771XRZ9uJzxxxzP4g8+CDUe8WzoMzTMu1K1KfHElxXFTJxwFNf+5vdbHPfrX/86r7zyCkVFRbzyyiucc845CdeddNJJ3HjjjXzjG9+gf0MNcSwOxKBsHWxYCjmFTDzpDK697teJjzHxeF557XXOOScMhT558mTMjCOOOCLp8ykiItuPGJb0kg4p70xvZn8AHnP3R8xsCpDv7rMSbDcBOAMY7e5FrRzzQuBCgGHDhh24dOnSpGIV6XZe/WtoevbV+1tOGtpBiTpQS5Jqq8NEoD0GbzknTk1FmOG+YZjmVFr/UahRy+sd5pNJ8u+/aNEizj33XC666CLOPz/xENjbcmf6dFBneukMdaaXzkhVZ/o//WdJ0p9b3zxs+HbRmb4QaLjduR5o0QbHzHKAHwGnAA+0dkB3nw5MhzDqVwdiFele9j87LJJeWTlh8s2K9WGOmIa68ori8DO/T+qP2WMw5PQMk2p2oG5+zJgxvPii5j4UEenOsjK8k0pHbr3WAuPM7PfAQUBZK9uVsrk5V49WjnEF8Cd339CB44uIZL6C/lBXHZqAQRixrqI4JDDx7NQfL6cg1NJk+MgtIiKSebanPiqzCJNoPUaYUKtFc67Iy0BDg+d9gSUJtjkGuNjM5gL7mdmMDsQhIl0s2SahkkBub7B4qFUBqC4LiUt+v/TG1UF6D4iIbP9iZkkvyYgGypofPc4ys0/MbG607B2tT2rQLehY06+h7n5O9PhxM3uule0eAOab2RDgeGCKmf3c3RuDcffxTV7QXHe/oANxiEgXysvLY926dfTv3z/jZ6zNSLEY5PeF8nXQuzZq9hULfUi2Ee7OunXryIuGqRYRke1TKr/mzawvcDuhGwjAPsBsd7+8yTaNg26Z2a1mNsrdF7dWZkcSlZVmdiVhZt9DgGWJNnL3EjMrAiYC17v7KuD11gptrSO9iKTH0KFDWbZsWeNQuNIJtdVQuho+q4bKkjCK28b30x1Vh+Tl5TF06NB0hyEiIl2oI02rmg5yFZke9SdvUEfow/5g9PshwIlmdjTwJnARUATMiZ5/gtAKKyWJyjTg68BphJG/FrS2obsXNwlCRLYh2dnZjBgxIt1hbNvc4aZvwMZlUFUC/3UX7HFkuqMSERHZQkdmpm86yFUrz5cATVtjvAQc4+4rzewO4ASSGHSrqaQTFXevBv6v4XczexG4Mdn9RUS6DTPY72x44qrQDGy3L6Y7IhERkRY6kqh0whvuXhU9XgiMIrlBtxpt3YQLIiKS2D5nQjwHxpwShi0WERHJMNaBpRPuNLN9zSwOfIXQFSSZQbcatVujYmZnJVoNbFtD2IiIfJ56DISvPwN9dkl3JCIiIgl18Zg5PwX+Rsgb/uHuT5lZL7YcdOuQtgpIpunXqFbW39mRSEVEup3Be6c7AhERkVZ1xeieDQNluftbhJG/mj7XfNCtjW2V1W6i4u4/6WygIiIiIiKSmdLRB6Qjg251ZNQvERERERHZTnRxZ/qtpkRFRERERKQbyvSJnZWoiIiIiIh0Q5k+/K8SFRERERGRbkg1KiIiIiIiknEyO01RoiIiIiIi0i3FVaMiIiIiIiKZJsPzFCUqIiIiIiLdkWV44y8lKiIiIiIi3ZBqVEREREREJOPEMrxGJW3DJ5tZPzObaGYD0hWDiIiIiEh3ZZb8kg5dkqiY2Uwze97MftjK832BfwIHA8+a2cCuiENERERERBKLmSW9pCW+VBdoZqcCcXc/FNjVzEYl2Gwf4Dvu/gvgceCAVMchIiIiIiKti1nyS1ri64Iyi4A50eMngCOab+Duz7n7AjMbT6hVeT5RQWZ2oZktNLOFa9as6YJQRURERES6J+vAf+nQFYlKIbA8erweGJRoIzMz4EygGKhJtI27T3f3ce4+buBAtQ4TEREREUmV7thHpRTIjx73aO0YHlwMvAGc1AVxiIiISJqVl5bwwRsLKSvZmO5QZBuyccMGnv/PvykuXp/uULZr3bFG5WU2N/faF1jSfAMzu9zMpka/9gE2dEEcIiIi8jkr3bCeGVdfCkBF6Sb+ct0PWPbBu9z6029TVrKB4s9WcuevrmDG1Zfy6B1/SnO0kolKNm7kkou/wVtvvsEF536N9euVrHSVTO+j0hXzqDwAzDezIcDxwBQz+7m7Nx0BbDowx8wuAN4i9GURERGRbVhF6Sbu/dOvqK6sBGDVJx9y/DnfZOfd96KybBMrPnqfl599hKJTp7Lz7ntx9w0/4eNFrzFizH7pDVwyyvvvv8d3v38F++y7HyUlJbzz9iIOP+LIdIe1XUrXaF7JSnmNiruXEDrULwCOdvfXmyUpuHuxu0909/Hu/k1391THISIiIp8vi8U487Ifk1tQAMCIvfZj5933Ysnbr7Psg3fZefcxrFu5jB13DQOCFvbqS2V5aTpDlgw07qCD2Wff/Xh54Uu89eYb7Lvf/ukOabtlHVjSoUvmUYkSkTnuvqoryhcREelKZraXmV1hZj9uWFrZrnF0yqfu/cvnHWbGySsoJK+gxxbr3J03n3+W/B49iGdlMeYLR/Hs32/n3Zf/w+LXX2TXvQ9MU7SSydydxx99hF69epGV1RUNgARSP4+KmQ0ys/lNfm8xt2J78y1uEV+nXpWIiMj2bQ6wDHiuydJC09Epjzntq59nfNsMM+PL51/GoGG78e7Cf1N02jnsvt8XePmZh9l//HHk5uW3X4h0O2bGD350NaN234O5zz6T7nC2W6msUYkmdL+dMAJwwrkVk5xvsZESFRERkZZWA7Ojeb+ec/eEiYq0bd6Ds3n1uccBqCwrbaxtGTx8JBvWfsZhJ56ezvAkQ906YzoPPfgAAJs2baJnz57pDWh71oFMpWkNcrRc2Ky0OsLUIyXR70W0nFsx0bpWqS5NRESkpdeBZ81sNlAG4O53pDekbc9BXzyRu2/4CS8/8wiDdh7OyH0PAuBf/7iLw790Ojm5eWmOUDLR5NPP5Hv/cxn33XsPI0eO4rDD27yWla3Qkc707j6dMCBWa8+XQKgNizSfW/GAVta1SomKiIhIS69Hi5O+fqTbrPOvvgGA/B49mfbD37R4/otnnPs5RyTbkl69e3PzjFnpDqNb6OIPt0RzKyY132IDNf0SERFp6UFgNHAqMIow9L6IyPala4f9SjS3YrvzLTalGhUREZGWbgfuBW4DDgHuAE5OZ0AiIqnWxTPOP8CWcyseQqilbr6uVapRERERaamvu9/h7u+5++1Av3QHJCKSambJL8ly96LoZ/O5FTcmWtdWWapRERERaek1M7sZeIFwx+/VNMcjIpJyXd0Bz92L2TzKV6vrWqNERUREpBl3v9TMvgTsBTzg7o+kOyYRkVSzjlSVpIESFRERkQTc/WHg4XTHISLSVTI8T1GiIiIiIiLSHWV4nqJERUREpIGZ/c7dv2NmzxJGp4HwXe7uPiGNoYmIpF6GZypKVERERCLu/p3o59HpjkVEpKt18fDEW02JioiIiIhIN5TpfVQ0j4qIiEgzZtbTzIaaWW8zO9fMdk53TCIiqdYV86ikUpckKmY208yeN7MftvJ8bzN71MyeMLP7zSynK+IQERHppPuA3YAbgJHA3WmNRkSkC1gH/kuHlCcqZnYqEHf3Q4FdzWxUgs3OBn7n7scCq4BJqY5DRERkK2S7+3PAju5+FVCf7oBERFIt02tUuqKPShGbZ5t8AjgCWNx0A3f/U5NfBwKfdUEcIiIinfWpmb0K3G5m5wAr0h2QiEiqZXgXlS5JVAqB5dHj9cABrW1oZocCfd19QSvPXwhcCDBs2LAUhykiIpKYu59jZv3cfb2ZDQVmpzsmEZGUy/BMpSv6qJQC+dHjHq0dw8z6ATcC57VWkLtPd/dx7j5u4MCBKQ9UREQkETPrCRSYWW9gIrBjmkMSEUm5mFnSS1ri64IyXyY09wLYF1jSfIOo8/w9wJXuvrQLYhAREdka6kwvIts968CSDl2RqDwAnGNmvwPOABaZ2c+bbXM+oUnYVWY218zO7II4REREOkud6UVk+5fhmUrK+6i4e4mZFRGqyq9391XA6822uQm4KdXHFhERSRF1pheR7V63nJne3YvZPPKXiIjINkWd6UWkO8j0mem7JFERERHZlpmZAYeZ2SDgbULTL9WqiMh2JcPzlK6ZmV5ERGQbdzdwNHAR4bvyL+kNR0Qk9cws6SUdlKiIiIi0NNDd/wcodfd/o+9LEdkOpWpmejPLMrNPokGy5prZ3mY208yeN7MfdjY+ffCKiIi0tNjMbgV2NLOrgffTHZCISKqlcNCvfYDZ7l7k7kXAKCDu7ocCu5rZqM7Epz4qIiIizbj7hWZ2MvAu8B7w0zSHJCKSeh1o0WVmFwIXNlk13d2nR48PAU40s6OBN4EqNg+s9QRhjsXFHQ1PiYqIiEgC7v5gumMQEelKHRmeOEpKprfy9EvAMe6+0szuACYAN0fPrSfMn9hhavolIiLSjJm9lu4YRES6Wqr6qABvuPvK6PFCYACQH/3eg07mHEpUREREWrrNzC5NdxAiIl0pZskv7bjTzPY1szjwFeBiQnMvgH2BJZ2JT02/REREWjqZ0JH+LKACcHefkOaYRERSLGXDDv8U+FtU4D+AB4D5ZjYEOJ7Qh6XDlKiIiIg04+5HpzsGEZGulqrpUdz9LcLIX03KtiJgInC9u2/sTLlKVERERJoxsxihVmU48AHwT3f3tAYlIpJiXTmNo7sXs3nkr05RHxUREZGW7gK+CJQBJwB/TW84IiKpl8LO9F1CNSoiIiIt7eDuZzT8YmbPpjMYEZGuYOnKQJKkREVERKSlcjO7AngZOBjYaGbj3X1emuMSEUmZzE5T1PRLREQkkReAXOAwwk29V4GidAYkIpJqavolIiKyjXH3n5jZWGAn4BPgU3cvTXNYIiIp1ZGZ6dOhSxIVM5sJ7AU87O4/b2WbQcDf3f3IroihqfdXb2LJ2jJ65GZRkJtFYU6c2nqnrKqWTVW1VNXU0Ssvm76FOfQtyCE3K0ZZdS0V1XVU1NRRkJPFgB459M7PbrMtX119GBAmZpnf5k9ERFpnZjcCQ4ARwI+A64CT0hqUiEiqZfjlasoTFTM7FYi7+6FmdquZjXL3xc226QvcDhSm+viJ3P/qcm6a++FWl5MdN/oX5jK4dx479s5jcO88aurqWbK2nI/XlrFiYwUNg1dmxYy87DgDe+YysEcuA3rmkJ8dTrcZuENtfT01dfVU1zoxg1752fSOlrp6p6KmjvLqWmpqncLcLHrlZ9EzL5vSylo+WlvKh2tK+WRdOf0KcxjWv5Bd+hUwuHce2XEjHosRN6itd8qjhKuypq4xPjOo3+K5evoX5rDLgAKG9y9kUK88NlXWsKG8hvVl1dTU1ZOfEycvK05eTpwBhTkM6p3HoF55FObEKa2qZUN52D4rbvQvzKFvYQ7Z8Rjry6r5aE0pH60po7SqlsHRuduxdx5xM6pq66msqaO6rp6sWIzsuJEdj5GbHaMwJ4v87DixmFFf75RW17KpspbyqlrqHerdqat34jEjJytGTrRfr7xs8rLjCf+OHu1TW++NyaVZuKsQi0F2LEasnSlY3Z2NFTUs31DBig2VrCutYo/BPdl7p95kxTe3qKyvdz5ZXw7AgJ65FObElcSKbBv2dvciM3vW3R82s++nOyARkVTL9CuSrqhRKWLzmMlPAEcAi5ttUwecCTzYVkFmdiFwIcCwYcM6HdB5h4/gS3vvSFlVLeXVdZRW1ZIVMwpzsyjMzSI3K8amylo2lFdTXF5DVW1duEDOiZOfHaesupY1m6pYV1bNmk1VrC6pZPFnpcx7fw1Z8RjDBxQybnhfhvXbiex4jNp6p7aunvLqumifSt5dtYmqmvrGmNyd7OjCOjseo64+XPhurKihoqYOgLzsGAU5WWTFjLKqWsqq6xr336lPPrvt0IP9du5DcXkNn6wr59VPitlUWZvwHGTHjbyseEiSonUxMwpy4uTnxMnNivP6sg2sWVjV4fMbM6hvZXaB/Ox44+vZGnnZMapq6+nILAZ52TH65OeQmx2jsqaOiuqQkFXX1be7b8wgKxaSnvzsOHnZcbLiRkV1XeP7qDbBi+6Rm8VBw/uyU9983l25iXdWlmzxd8vLjtG/MLfxvOdlx4mbhYS1rp7q2nrysuP0yM2iMDdOz7xseuU1JLBZmBmlVbWUVdVSWlXLxoqQHG6sqKG0qpbq2pD8NiSW/QpCwtgzL5vKmrrG91F9vZOXHSMvem3uUFNXT219PTEzhvYtYJf+BQzvX0BudpyN5TVsKK9mY0UtVbXhtVfXhu1r67zxPZ+XHadXfja98kJSnZ8dbzyH9e4Ul9dQXF7NxvIanPC+zIrFGpPMnKwYuVkxaurqQ+JbUcOmyhr6FOSwQ89wk2BQrzz6FmTTtyDUclbX1bO6pIrPSipZU1pFeVVIvhved32i5L9XfjYbK2pYsaGC5Rsq2Fhew8BeuQzpnc+OvfMozM2isqauMXHeVBnOb0lFDXXuDOsXkvhd+hc01q7GogTXYuHD3syoratnU2UtJZU1lFSEz5X15dWsL62msraOUTv0ZK8hvdh1QOEWSa27497QFthwd+qjGxr19dG5arJ9TV09KzZU8Mn6ciqq69glii1Rgu7uVNfVh39D9dC7IDv5f0jd1xoz+zHQx8y+BqxKd0AiIqkWy/Cbp12RqBQCy6PH64EDmm/g7iXQfvMod58OTAcYN25cpyfaGtgzl4E9czu7++eupq6euFmLu/q1dfWUVtWSmxUucptzd6pq6xtrCurqPSQo2XGy48mNm1BaVcvSdWV8VlJFr/ws+hbk0C+qGamMLv4qqutYU1rFZyVVrCqpZFNlDb3zs+lTkEOf/Gxq6511ZdWsK62ipKKWIX3y2HVgIbsO6EGv/GxWl1SyamMlKzdWUu9OXnac3KzNCVttfbhgr6ytp6K6lrLowjMvK0av/Gx65mU1JnBm1lg7VF1XT1VNPVW1dZREieeG8hqqauvJz44SsuwYufEYWfEYWXEjHr0HnVDLVe9OTV09dU3Ka3jdtXVOQU6cwtwsCnLi9CvMYac++Qzpk0+fgmzeXL6R5z9cx/MfrWPhkmJG79iT08ftzF479iIeM9aWVkXnpZqKmtC0MCQ89eRmx+iRl9V4nsuqavlsU2W44K2o2SLZARoT7T4Fm2vhhvTJa0x8s6Jy1pdVs76smiVry8jLDrH3zs8mblBZEy6o12yqwszIiS6Ea+vqeXzFKtaXVSd8j2TFLDpG9DP6PR4zKmvqKKmsobKm7WQwJytGzGhMclpTmBOnR15W498xVQpy4vQtyGHNpqp2E9eC6N9aefXWJ9xNk/rcrBj9CnMa/00l8/pysmL0yM0iO26s2VSV8AbB4F555GSFv39D4tW07P127sMDFx++1a9le2Vm46OHM4E9CB3qewO3pSsmEZGukuF5SpckKqVAfvS4BxpZrMNaSyqy4jH6FOS0up+ZtdrcKVk9crMYM6Q3Y4a0fK4wd/PbZdSgnp0+Rr/CHPbcsVen989Uu/Qv5MR9Epy4FKipq6ekogagsRawq5uQlVSGmrqq2nr6FoREtFde1hZ39VtTVRtqJMIFeKjJMoO+BaEfWNNE292pqWtIDMOFdVbc6JOfQ05WrHGbjRU1rCqp5LOSKoqjJHR9WTW52TEG9cxjh17hhkRhTkgkC3KycLyx1mlDeQ298rMY2qeAXlHtVH2UVK/YUBGS4ex4qGnKijcmxdnxGO5hu6XryliytpzSqtpQA0JIPBpqQxwnHos11ij1ysuiT0EO/Xvk0Kcgm5gZH60p4+2VG1m0vISNFTUU5ITmlI01nr651jMrZsRjRiyqqSmtrqW8KiQgg3vnsXO/AnbpF2q9GmJbur4sqjGLN94EyG34mRVjcO+8Lni3bFeOjn4eBdQCDwCTgNOBR9MUk4hIt9QVicrLhOZeC4B9gfe64Bgi3Up2PEb/Hp9vrWCvvGzG7tS7U/vmZsXJ7ZFc0mxm5GRZY21Ba9v0KcihT0EOowd3LJaCnCx27J2f8LlYzJKqcTUzBvTIZUCPXA7cpV/HAmhmj8E92WNwT07Zf6uKaWG/nfuktsBuyt1/AmBmT7v7sQ3rNeGjiGyPumONygPAfDMbAhwPTDGzn7v7D7vgWCIiIl2h3swuBV4HxrC5oktEZLuR6cMTp7xZVtT/pIhQo3K0u7/eWpLi7kWpPr6IiEgKnA7kAVMIfVROT284IiKp1y0nfHT3YjaP/CUiIrJNcfcNwPXpjkNEpCt1x6ZfIiIiIiKS4TK96ZcSFRERERGRbkg1KiIiIiIiknEyPE9RoiIiIiIi0i1leKaiREVEREREpBuKZXjbL3PfNoaGN7M1wNKtKGIAsDZF4WyvdI7apvPTPp2jtm0P52cXdx+Y7iBk22JmF7r79HTHIdsWvW9km0lUtpaZLXT3cemOI5PpHLVN56d9Okdt0/mR7krvfekMvW8k5RM+ioiIiIiIbC0lKiIiIiIiknG6U6KiNo7t0zlqm85P+3SO2qbzI92V3vvSGXrfdHPdpo+KiIiIiIhsO7pTjYqIiIiIiGwjlKiIiIiIiEjG6RaJipnNNLPnzeyH6Y4lU5hZbzN71MyeMLP7zSxH56klMxtkZq9Gj3V+EjCzP5nZl6PHOkdNmFlfM3vEzBaa2c3ROp0j2eaZ2TVmVpQBcdyQ7hgkyIT3hN4P25/tPlExs1OBuLsfCuxqZqPSHVOGOBv4nbsfC6wCpqDzlMhvgHy9jxIzsyOBwe7+kM5RQucAf43mAehpZt9H50gkZdz9snTHIJlD74ftT1a6A/gcFAFzosdPAEcAi9MWTYZw9z81+XUg8FXghuh3nSfAzCYAZYRErgi9j7ZgZtnALcAjZnYyOkeJrAPGmlkfYGdgIzpH8jkys2uAbOBIoBdwMvArYAiwDDgX+EGzbSa5+6oEZfUF7gHigAFzzawAuAPYAXjT3S+OjvkFoABYA0xx91ozmws8CJzr7vuYmRFGddo92u5MoD4qbxegBjgVKGm+zt03RjHNdfeiJjHeCOwHbACmRq9332gZDJzh7m917mxKcwneEy+a2d/R+0FSZLuvUQEKgeXR4/XAoDTGknHM7FCgL/ApOk+NzCwH+BFwRbRK76OWpgJvA9cDBwMXo3PU3L8IX6aXAu8AOegcyedvpLuPB+4Dvga85e5HEZLk8xJsM8HMbjazuU2WHwMXAv9096MJF4hE696K9t3RzPaJ1s+PjrGacHEIsCPg7t6wzclAdrTdJ8CXgH7APsBRwM+A3q2sa8HMTgTy3P1I4F7g8uipg4DjCAnaSR09edKm5u+J3dH7QVKoOyQqpUB+9LgH3eM1J8XM+gE3Er6odJ62dAXwJ3ffEP2u89PS/sD06M7rX4B56Bw1dzXwDXf/KfAucBY6R/L5uyP6+Qnhs+2F6PcFwJ4Jtslx94vcvajJ8lNgBPB6tN3C6OcewCnR3fFdgZ2i9S9HP98AhkePNwJ/aBLXHsCh0b7jgUHuvg64DXiMUNuzKdG6Vl7nXq28ttnuXtPw2lrZVzqn+XviIvR+kBTqDl+SLxOaV0Co6luSvlAyR1RjcA9wpbsvReepuWOAi6MPzP2AL6Pz09wHhC8igHGELx+doy31BfY2szih6cOv0DmSz19Zk8ffAw6JHh8CLEqwTWs+AcZEj/eLfr4H3BA1t/lhtA2EWlYINzQ+iB6Xu3t9k/LeA+6K9r0MeNvMdgbWuftxhNrHUxOtayW+RVvx2qRzmr8nbkbvB0mh7tBH5QFgvpkNAY5n85u2uzsfOAC4ysyuAmYB5+g8BVG1NRDavBKqh/U+2tJM4FYzm0Jo314E/EPnaAvXEv5t7QI8D/wevY8kvWqBMWY2j9Dk95eEPirJmA7cY2aTCf/mIfRTm2Vm5xL6DpwVrT8o+uxcBfyzlfL+AXzJzJ4DnNBXcjXwZTO7iNDv4eaojObrWnD3h81skpn9Cyhmc58E6TrN3xOLga/p/SCp0i1mpo86e00E5iXqICiBzlPbdH7ap3PUPp0j2d5FnafnuvvcNIciGUDvB9ka3SJRERERERGRbUt36KMiIiIiIiLbGCUqIiIiIiKScZSoiIiIiIhIxlGiIrIVzOwaM3unyaRo+21lWUUpC05ERDrEzKaZWYWZrTazFWaW7IhobZU5t+lnu5l918y+m4JyU1KOSCbrDsMTi3S1X7j7X9IdhIiIpMSD7j7FzAYA/zazue7+n1QV7u6/6cj2ZnaNu1+zteWIbItUoyKSQmZ2m5ndZ2b/NrM/RutyzWy2mT1nZn81sxwzyzOzu8zsX2b2TzMriIqYaGbzzOw1MxucxpciItKtuftawpwfR6Y5lKvTfHyRtFGiIrL1rmpo+kWYfOrv7n44MMLMDgS+Drzl7kcRJsM6D7gQeN3djwDuBcZGZY2MJpu8D5jwOb8OERHZkgH10Wf8ZDN7wMyeaXzS7AQzezdqKnZNk/U/NrOVZvYo0GuLAkMz32uarTvbzD6K9vletO46M1sVPV5lZouSKOerZvaxmS0xs2nRumnRTbK/mtk6M7vXzGzrT41I11OiIrL1fuHuRe5eBNQBL0fr3wCGA3sBL0TrFgB7AqOBF6N1twEvRY/viH5+AuR0ZdAiItI6MxsKnAQ0JCa/BG4FTomeHwjcCBwLjARON7P9zexgwg2pvYAfA/u2c5w9gV8B4wk3rb5tZnu4++XuPhjA3Qe7+5h2yhndpJzDgJ+a2d7R06cBdwG7AIcD+yV5GkTSSn1URFLvYOA9whfB3cBg4BDgqejnIiAPOAh4GvgB8Fm0b9nnHKuIiGzp5Kgmowz4nbu/HFVA3Oru/2iy3SHATmy+6ZQLjAEGAA+7ezHwkpm92c7xjgH+6e7Lot+HdDLuiVE5nwKY2f3AccBaYKG7PxStfw/o3cljiHyulKiIbL2rzOyC6PEXgHwz+wbworu/ZmbvALeZ2TzgU8JduThwe9RcbB1wNnDF5x+6iIg086C7T0mwfkGz3w141t2PBzCzQqAe+AbgTbar78jBzexYYKm7v9eR/SLe7HHD7x+2so1IRlOiIrIVopFYrmn43cxuA65x9yVNtqkC/ivB7mc0+72xHHe/LWVBiohIV1gATDezPYCPgSeB6wg1LN8ys97AbsA+7ZTzNPA9MxsClAJ/BJomSuvMbBdgBVDg7htbKedJ4Ptm9nNCcnQKcCJwIEpOZBulREUkhdx9WrpjEBGRrufun0W16f8AegKz3f1BADO7C3if0Az47XbKedvMfgj8m1Db/nt3f6XJJt+PnssjJB/zWynnXTO7EvgXobbnand/MxrURWSbZO5KskVEREREJLNo1C8REREREck4SlRERERERCTjKFEREREREZGMo0RFREREREQyjhIVERERERHJOEpUREREREQk4yhRERERERGRjPP/AS/2VBw40lW1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u04': 0.01639344262295082, 'u10': 0.0, 'u14': 0.0, 'u17': 0.0, 'u33': 0.041666666666666664, 'u35': 0.0, 'u43': 0.0, 'u44': 0.05714285714285714}\n",
      "mse of score by users:  0.24024210360765735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "AC 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' B.DNN '''\n",
    "\n",
    "###############  Training ,prediction #################################\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer = regularizers.l2(0.01))) # input_dim = numb of features, 1D number\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu')) # regularizer in hidden layers\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))  # 5 classes\n",
    "model.summary()\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam',loss='mse', metrics=['mae', 'acc']) \n",
    "history=model.fit(X_train, y_train_dummy, epochs=epochs,validation_split=0.33, batch_size=32, verbose = 0)\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "y_predicted = y_predict.argmax(axis = 1)\n",
    "\n",
    "###############################################below######################################################\n",
    "########### plot traing acc and loss, f1 score##########################\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_t = y_test_dummy.argmax(axis =1)\n",
    "\n",
    "print(classification_report(y_t,y_predicted))\n",
    "print(\"AC\",accuracy_score(y_t,y_predicted))\n",
    "\n",
    "### plotting ####\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "plt.title('Training Loss/Accuracy',fontsize=14,fontweight='bold')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(history.history['loss'][10:], label='Train loss')\n",
    "plt.plot(history.history['val_loss'][10:], label ='Val loss')\n",
    "plt.plot(history.history['acc'][10:], label='Train accuracy')\n",
    "plt.plot(history.history['val_acc'][10:], label ='Val accuracy')\n",
    "plt.legend(['train loss','validation loss', 'Train accuracy','Validation accuracy'], loc='center right')\n",
    "\n",
    "#####confusion matrix#####\n",
    "plt.subplot(122)\n",
    "plt.title('Confusion Matrix',fontsize=14,fontweight='bold')\n",
    "ticks=['non-depression','depression']\n",
    "conf_mat = confusion_matrix( y_t,y_predicted)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=ticks, yticklabels=ticks, cmap=\"Blues\")\n",
    "plt.ylabel('Actual',fontsize=12)\n",
    "plt.xlabel('Prediction',fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "########################################above###################################################################\n",
    "\n",
    "predicted_df = pd.DataFrame(y_predicted, index =X_test.index) # dataframe of predected label + user index\n",
    "predictions_temp, score_user  = classifier_slice2uid(predicted_df, trigger= trigger) # predictions output in df formate , by user id\n",
    "predictions =  predictions_temp.iloc[:,-1].to_list()  # convert to array \n",
    "y_t = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list())  # pickup the associated test's depression label by user\n",
    "\n",
    "mse = np.sum((y_t-list(score_user.values()))**2/len(y_t))\n",
    "\n",
    "print(score_user)\n",
    "print(\"mse of score by users: \",mse) \n",
    "print(classification_report(y_t,predictions))\n",
    "print(\"AC\",accuracy_score(y_t,predictions))\n",
    "\n",
    "fpr_dnn, tpr_dnn, thresholds_dnn = roc_curve(y_t,predictions)\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title('ROC',fontsize=14,fontweight='bold')\n",
    "plt.xlabel('False Postive Rate')\n",
    "plt.ylabel('True Postive Rate')\n",
    "plt.plot(fpr_dnn,tpr_dnn,linewidth=2, linestyle=\"-\",color='red')\n",
    "\n",
    "plt.subplot(122)\n",
    "ticks=['Normal','Depression']\n",
    "conf_mat = confusion_matrix(y_t, predictions)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=ticks, yticklabels=ticks,cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix',fontsize=14,fontweight='bold')\n",
    "plt.ylabel('Actual',fontsize=12)\n",
    "plt.xlabel('Prediction',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51bf395c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "10. KNN\n",
      "sugmentation factor = 2\n",
      "feature number = 10\n",
      "{'u07': 0.0, 'u09': 0.0, 'u15': 0.057971014492753624, 'u17': 0.08571428571428572, 'u20': 0.1016949152542373, 'u27': 0.09859154929577464, 'u33': 0.08333333333333333, 'u58': 0.04411764705882353}\n",
      "0.000000000, 0.000000000, 0.057971014, 0.085714286, 0.101694915, 0.098591549, 0.083333333, 0.044117647, [0, 0, 0, 0, 1, 1, 0, 0]\n",
      "mse of score by users:  0.21269566247614374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.50         8\n",
      "   macro avg       0.33      0.33      0.33         8\n",
      "weighted avg       0.50      0.50      0.50         8\n",
      "\n",
      "AC 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:541: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:546: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEVCAYAAADTivDNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2f0lEQVR4nO3debxbVdn3/8+3pQilCAVrkRkU0QqUoSAg4ik3s+AAItyMZaoDIsrj8wOkjIIotw8OQNHKXIZbBgVkRmSoSoEyD4KilEmQQltKGYrQ6/fH3oeTpjnJzmmSvZN837z26+zsrOxcJ4ekV9Za+1qKCMzMzMyssQblHYCZmZlZJ3KSZWZmZtYETrLMzMzMmsBJlpmZmVkTOMkyMzMzawInWWZmZmZN4CTLciFpnKRItzcl3SbpU+l9O0n6m6S3Jd0iaeWSx60q6Y+S3pD0J0lr5/dbmHU2ScMlXZm+3x6QtGmDzjtM0jXpeV+StEWDzhuSVm/Euao8R0/6PD9Jb49Kb5/fzOdNn2u6pJ5mP481jpMsy9NfgeHAOsBLwJWSPgpcCVyQHl8cuBhA0iDgauA1YDRwK3BFetzMGu98YEVgI2AicI2kpRpw3nHASsBaQA/wjwacE5LPk2cbdK5aPlX2MxNJxy9CQrYe8KcBPtZy4H+cLE/zI2J2RPwT+C6wNrAn8EJEnBwRTwFHA1tKWpXkw3g9YHx63w9JPqTXzSV6sw4maU3gC8AhEfFERPwaeBPYugGn/xDwSET8Kz33iw04J+nnyfxGnKuG9+hLrtZJbzddRMyJiHdb8VzWGE6yrCjeTH9uCDxacvzx9OengM8A/4iIGQARMQ/4GeAPHbPG+wwwD3iw5NiZwCwASeukQ/avSbq+d1g/HU6bLukLkp6RNFPSt9L79pYUwHHAfukw2/T0vtXT+95XOjwmaQNJ96TTCx6TtGV5wJWGC9Mhz0slzUqHPD9b1n5bSfdKmivpQknK8No8DwyR9EGSz6Z7S875kfT1mCvpn5K+XOt3T+8fJ+l2SRumr+sVFX6/hYYLJV0u6ex0f5V0CHZMht/BWsBJluUu/VA7jGTIcFmS4cBevfvDSYYtZpQ+NiKOjIjHWhCmWbdZEXi1tGcoIv4nIu6UNAy4GbiFZOj+BeDqkqH75YEjgB2BY4H/J2kJ4Dck7+UfA5em++tljOcs4B7gY8Bk4FcZH3chsBTJkOcvgeslrVRy/y9Iesy3AXYDxmY87yPAKJKerIdLjp9G0rP1ceB7wAWSFiPb7z6SZHrEOcAJGeP4LvAVSaOAHwAXRsS0jI+1Jlss7wCsq31S0mzgAyTfjvcGjixrU/qtcggt6pY3s6rvt51Jep9PjIiQ9G3gFWCT9P5hwNcj4jFJfwdOB0ZGxDPAbElvA+9ExOw64nkrjektkkTlp7UeIOkjwE7AqhHxHPArSXsA+wA/SpudGRE3p+0fAFbJGM/DJD3vywAvAqunx79F8tqsDKwALA18JH3+Wr/7J4DPRkTmeVcR8bykk0mSyZWBT2Z9rDWfe7IsT08B65NMrn00Im4FXiXpzer1wfTnq8DssvtIhyM+39wwzbrSbBZ+v90p6RCSROTpiAiAiHiLJNFYNW06KyIeSe97p/fhA4hhaMn+N0l6ep4C7ge2y/D4VYB30wSn1z9K4gS4o2T/nTrifBjYlb4pDb22Ap4ErgE2SI8NznjO++pJsEqcTdKjeHlEzBrA461JnGRZnv4TEdNJusW3kLQ58BBJ93uv3v1HgQeAj/de3SRpKMmQxvMti9isezwALC3pYyXHPkbyfnsWWKP3oKQlgY8Az6SH5gzg+SI916D05yrAiJJjI4HdSSbN/wz4TfoZUM2zwGIqKQMDrFkS50BjhSTJ2oqSocJ0SPRC4JsR8UmSaRDl5tN/Ijd3gLEcC0wB9pa0Rq3G1jpOsix3EfESyVyJE4BLgBUkTUg/3E8Gbo2IF0jKN7wCnJl+kPyI5Fvpw5XPbGaL4C8kidaZkj4m6QiSof1bgWuBoZKOk7QaybymJyiZAD4ALwP/ATaTNJhkbtN/ANJ5YReTzD9amSRJqTndJf1suTb9HdaQ9DVgY+CiRYiz1+NpfKWfP4uTvEZDldTwuzg9XppUPQV8WtLKktZf1KRI0obAAcC+JPPUzlqU81ljOcmyovgxsDnJB+iuJB8Yj5LMv9gXICLmAtuSfJt+jKR7fOfeIQsza5z0fbUzybysR4CvAttHxNz0vbgdyfvxEZL37RcXpXxCOuR4BHAZyXDgLcC/SprsAXyFZCjueODAiHiT2vYj+Rx5APgGsGP6pW2RpMOgT1KSZEXEHOD/knxp/ANwJ8nFOxuUPPQykmT0CeAmknphA5Imo5OAUyPieZKJ7+tI2mug57TGkv99MjMzM2s892SZmZmZNYGTLDMzM7MmcJJlZmZmBkgamdZL6+/+cyTdJWlClvM5yTIzMzNL/ARYstIdknYBBkfEZsCaktaqdbK2qPj+oQ99KFZfffW8wzCzFrrvvvteiYgRecfRCEtu8C1fYVQwBxx7SN4hWJkzv/zJugvW1vPeevvBM78GjC85NCkiJvXekLQV8AbJEm+V9JBcHQrJslJbAH+v9pxtkWStvvrqTJvmpZjMuomkZ2q3MjPLJk2oJlW6T9LiwDHAl4Gr+jnFUiTrdALMJFlWqaq2SLLMzMzMFqKGzXo6EpgYEbOlfjvU5tI3lDiMDFOuPCfLzMzM2tOgwdm36rYGDpF0O7C+pLMrtLmPZIgQkmLY02ud1D1ZZmZm1p7673WqS0Rs2XdK3Q6cJumkiCi9ivAqYIqkFYEdgE1rnddJlpmZmbWnxg0Xvi8ietLdCWXH50jqAbYhWcrotVrncpJlZmZm7alBPVlZRcQs+q4wrMlJlpmZmbWnJvRkNVJToksrpk6p0aauqqlm1ubmzcs7AjPrNFL2LQcNT7IkDQcuIKkn0V+buqummlkbO/dc2HBDeOKJvCMxs07SuKsLmxNeE875HrA7MKdKmx4Wrpq6AEnjJU2TNG3GjBkND9LMWuT+++Gb34THH4e77847GjPrJBqUfctBw581IuZkmHFfXjV1ZIXzTIqIMRExZsSIjlhZw6z7zJwJu+6aDBWOHw/77Zd3RGbWSQo+XJjXxPe6q6aaWZuZPx/23humT4cxY+DnP887IjPrNN048T2Duqummlmb+cEP4IYbYPnl4YorYIkl8o7IzDpNwYcLm96TJWkUsOeiVk01szZy441wwglJF/0ll8Bqq+UdkZl1osH5TGjPqmmpXW/F1Ih4vCzBIiLmkEx+nwqMzVI11czaxPTpsOeeEAEnngjbbpt3RGbWqTwnq7J6q6aaWRt4+234yldg1izYaSf4/vfzjsjMOlnB52S54ruZNc6hh8J998Gaa8KFF8KgYn8Amlmby6mHKisnWWbWGOeeC2efnUxwv/JKGD4874jMrNO5J8vMOl5vwVGAs86C9dfPNRwz6xLuyTKzjlZecHTcuLwjMrNukdNyOVk5yTKzgZs/H/bZxwVHzSwfHi40s4510klw/fWw3HIuOGpmrefhQjPrSDfeCMcfn3zIXXqpC46aWeu5J8vMOo4LjppZETQwyZK0HLAR8EBEvNKIcxY7BTSz4iktOPr5z8PRR+cdkZl1q0GDs29VSBoOXAtsAtwmaUSFNotJelbS7em2bq3w3JNlZvUpLTg6ebILjppZfho3J2s94PCImJomXBsCN1Voc2lEHJH1pP50NLPsXHDUzIpEg7JvVUTEHWmCtSVJb9ZdFZptCuwk6R5J50iq2VHlJMvMsnHBUTMrmjoWiJY0XtK0km38gqeSgN2BWcB/KjzbvcDWEbEJMATYsVZ4Hi40s9pccNTMCkh1DBdGxCRgUpX7AzhE0g+ALwC/KWvycETMS/enAWvVek73ZJlZdS44amYFpaSHKtNW4zxHSNo3vbksMLtCs8mSRksaDHwJeKhWfE6yzKw6Fxw1s4LSIGXeapgE7CPpTmAw8Lykk8ranAhMBh4E7oqIP9Q6qYcLzax/LjhqZgVWz3BhNRExC9im7PCEsjaPklxhmJmTLDOrbPp02GsvFxw1s8JqVJLVLE6yzGxhvQVHZ850wVEzKywnWWbWfr797aTg6BpruOComRVXsXMsJ1lmVubcc+HXv3bBUTMrPPdkmVn7KC04OnEibLBBvvGYmVUxqOC97E6yzCxRWnD04INh//3zjsjMrCr3ZJlZ8ZUWHN1oI/jFL/KOyMystmLnWE6yzIwFC45eeaULjppZW3BPlpkVmwuOmlmbcpJlZsXlgqNm1sYyLJeTKydZZt3KBUfNrM25J8vMiskFR82szTnJMrPiOe88Fxw1s7bnJMvMiuWBB1xw1Mw6gpMsMyuOWbOSgqNvv+2Co2bW/oqdY9GUSRiSzpF0l6QJ/dw/XNL1kqZJ+lUzYjCzMvPnw957w9NPu+ComXWEQYMGZd5yia/RJ5S0CzA4IjYD1pS0VoVm+wAXR8QYYGlJYxodh5mVccFRM+swkjJveWhGatcDXJbu3wxsUaHNq8A6kpYFVgGeK28gaXza0zVtxowZTQjTrIuUFhy95BIXHDWzzqA6thw0I8laCngh3Z8JjKzQ5k/AasC3gb+m7RYQEZMiYkxEjBkxYkQTwjTrEqUFR48/HrbbLu+IzMwaoht7suYCS6b7w/p5juOAr0fEicATgGffmjVDacHRHXeECRWnSZpl8uHlluauS4/IOwxLLbHYIL652Sp8a/NVOPjTKzO44JPAm6GRSZak5SRtI+lDjYqvGUnWffQNEY4GpldoMxxYV9Jg4NNANCEOM+stOLr66i44aovslO9+mSU/MCTvMCy18SrL8MenZnLGX57j9bffZdTIYXmH1HKNSrIkDQeuBTYBbpNUcQit1oV95ZrxiXsVsI+k04CvAo9JOqmszSnAJOA1YDng0ibEYdbdSguO/va3yYR3swH63MYf54233+Hfr76edyiWmvL0LJ6Y8QYAwz4wmNfnvZtzRK2nQcq81bAecHhEnAzcBGy40HNlu7BvAQ1PsiJiDsnk96nA2Ih4KCImlLW5JyI+FRHDImKbiJjb6DjMupoLjuau9OKdd195LO9wFsmQxQZz1MHbc8zPr847FKtgjeWWZOiQwUyf9XbeobRcPT1Zpe/JdBvfe56IuCMipkrakqQ3664KT9dD7Qv7FtCUYqQRMaskEDNrJRccbThJt7HwtAYBERFbVXpMREwi6bFnyQ2+1dZTIr53wDZMumwKr819K+9QrMzQIYPYbb2RnH3383mHkot6JrSXvif7OZeA3YFZwH8qNCm/sG+h3q5yrvhu1klccLQpImJs3jHkaatPf4Kejdfma7tvyXprr8TEY/fkmydekndYXW+w4KBNVuaax2Yw863uGyqEpCpNo0REAIdI+gHwBeA3ZU2yXNi3ACdZZp3k5JNdcNQabpsDf/b+/k2/PswJVkFsvvqyrLLsEmy39vJst/byTHl6Fve/0F1z5hpVmkHSEcCLEXEhsCwwu0Kz3gv7ppJc2PdkrfM6yTLrFDfdBMcd54KjLSTpIxHxYt5xtNJ2B/887xAsNeXp2Ux5enbeYeRqUO0J7VlNAi6TdBDwKPC8pJPK5pRfBUyRtCKwA7BprZM6yTLrBM88A3vumRQcPeEEFxxtkvRK6Z1JhgoA3iC5KsnMctCo4cJ0Lvk2ZYfLL9qbI6knbXdqRLxW67wummPW7lxwtJU+C2wO3EOSXHnNL7McDRqkzFsjRMSsiLgsIl7K0t49WWbt7tvfhmnTXHC0NQaRzMUYRpJkec0vsxzltFpOZk6yzNpZb8HRD3wgmejugqPN9lVgReAY4LvAD/INx6y75bUmYVZOsszaVXnB0Q1rlmyxRbczfZNd55NMfr08v3DMulvBcywnWWZtqbTg6EEHwQEH5B1Rt9gf2JMkwWrrAqNmnWBQwadHOMkyazflBUdPPz3viLrJv4E/AM+QVnwHKlZ8N7Pmc0+WmTVWacHRK65wwdHWGgKsGxFv5h2ImXXInCxJw0kme84E/h0R85salZlVVl5wdPXV846o26wA3Cvp370H+lu70Myar+A5Vu0kKy01/2VgKPBjYDtg3ybHZWblXHA0dxGxUd4xmFmfovdkZZkxtnNEbAq8GhEXA2s2OSYzK+eCo4Ug6Ya8YzCzPlL2LQ9Zkqw5kvYFlpD0OSovmmhmzXTYYS44WgyPSPpi3kGYWaLVFd/rlWVO1jjgKGAW8EXgwGYGZGZlzj8fJk1ywdFi2Bg4VNIjJOsWhudkmeWn6MOFNZOsiHiZpLIxAJI8XGjWKg8+CN/4RrLvgqO5i4ixecdgZn0KnmPVHi6UNLns0EVNisXMSs2aBbvs4oKjBSJpMUnjJf1U0kGSXAbHLEeSMm956PcDQtKqwBrApyRtmR5eCvhPKwIz62ouOFpU5wFPATeSLK9zHrBPrhGZdbGi92RV+xa2BtADDE9/CngL8Ndps2ZzwdGiWiUiepOqmyTdkWs0Zl0urwntWfWbZEXEHcAdklaLiBNbGJNZdystOHrxxS44Wiz/knQkcA9JT9bzOcdj1tU6YeL7Aj1Xkj4SES82LySzLlZacPT442H77fOOyABJx6a704H/A/wG2A04O6+YzKwDkixJPwC+AAxLD70BrNfMoMy6UmnB0R12gGOOyTsi69P7ST4POINkCsUIYLO8AjKzxs3JkrQM8L/AYJI8Z/eIeKeszWLAP9MN4NCIeKTaebNUNNwS2Jyke3w9YEZ9oZtZJqUFRy+6yAVHCyQiTgBOAZ4DdgSeATZ0jSyzfDXw6sK9gNMiYlvgJaDSMMJ6wKUR0ZNuVRMsyFaMdBAwmqQnaz2Sb29m1kguOFpokk4GDgIeJVnD9SVgqKTNI+IvuQZn1sUa1ZMVERNLbo4AXq7QbFNgJ0ljgUeAr0XEu9XOmyXJ+iqwEnAMcDjwg0wRm1k2LjjaDlYErk/3dy45HoCTLLOc1HN1oaTxwPiSQ5MiYlJZm82A4RExtcIp7gW2jogXJV1I0qt9TbXnrFYnazCwNfBORNyWHhsH7JLhdzGzLFxwtC1ExP55x2BmCxtUR1dWmlBN6u9+ScsBpwO79tPk4YiYl+5PA9aqGV+V+y4Bdge+IekXkg4DHgK2qHVSM8tg/nzYZx8XHDUzGyAp+1b9PFocuBw4KiKe6afZZEmj006oL5HkRFVVGy5cJSI2VzJb7GlgIvDZiJhd66RmlsEPfwjXXeeCo2ZmA9TAEg4HAhsCR0s6GrgNGBIRE0ranEjSASXgmoj4Q62TVkuylkjHJgXMBP4EjJKEJ3qaLaKbb4Zjj3XBUTOzRdCogu8RcRZwVo02j1JnCatqSdZD9E0Qewg4uPd5qDHRU9I5wCjguog4qUq7icANEfH7zBGbtTsXHDUza4h2XlZnQBM9Je0CDI6IzSSdK2mtiPh7hXafBVZwgmVdpbfg6KuvuuComdkiEsVOsppR7bAHuCzdv5kKE+UlDQF+DUyX9MVKJ5E0XtI0SdNmzHD9U+sQLjhqZtYwg5R9yyW+JpxzKeCFdH8mMLJCm32Bx4FTgU0kHVreICImRcSYiBgzYoTrn1oHcMFRM7OGamDF96ZoRpI1F1gy3R/Wz3NsQFIE7CXgImBsE+IwK47SgqNnnumCo2ZmDdCoEg7N0owk6z76hghHk6xaX+4pYM10fwzJOmBmnam04OiBByabmZktskFS5i0PWZbVQdI6JEvrPAs8FxFzqzS/CpgiaUVgB2APSSeV1Zo4BzhX0h7AEOArAwnerPBKC45uuCGccUbeEZmZdYy2vbqwl6TTSdbtWoNk/cIfA1/or31EzJHUA2wDnJoOCT5U1uZ1YLcBR23WLnoLjg4f7oKjZmYNltcwYFZZhgvXjYhdgdkRcR2wTK0HRMSsiLgsTbDMulN5wdE11sg7IjOzjtIJw4UzJB0LDJe0H+DEyayW8oKjO+yQd0RdL/0cqyoiTmxFLGbWGAXvyMrUk7Uv8BpwF0kvllejN6tm3jzYbTcXHC0eZdjMrI0UvYRDlp6sHUnKLbzV7GDMOsJhh8G997rgaMFExAl5x2BmjVXwee+Zkqy1gCslzQKuAa6NiDeaG5ZZm7rgAvjVr1xw1MysBYp+dWHNr9gR8aOI2BH4OvBxXNPKrLIHH4Svfz3Zd8HRtiPpY3nHYGb1afvhQklfIKl3tTJwD/DZZgdl1nZccLStSNob+B/gwyWH55Lh6mkzK46Cd2RlGi5cBzgtIv7e7GDM2pILjrajk4ANgYuBccDngU/kGZCZ1S+vHqqssgwX/tAJllkVLjjajhYDZgO3AZ8BfgV8Nc+AzKx+WS4ZzvPS4UzL6phZP1xwtF2dA/we+C7wO2AnYEauEZlZ3QYXfLyw3yRL0mkRcbik24DoPQxERGzVkujMiqy04Ohxx7ngaBuJiOMkrRIRz0kaB3wa+H7OYZlZnYo+XNhvkhURh6c/x7YuHLM2UVpwdPvtk94saxuStkx/9nY93gushq+eNmsrBc+xPFxoNiC9BUdXW80FR9tTb2FSASsBawJ/Aj6XW0RmVrdGrUkoaRngf4HBwBvA7hHxToV25wCjgOsi4qSa8Q0gkC3qfYxZRykvOLr88nlHZHWKiLHp1hMRawE7Aw/lHZeZ1UfKvtWwF0klhW1J1mjefuHn0i7A4IjYDFhT0lq1TpqlTtYtEbFNyaFTcK0s61blBUc32ijXcKwxIuJ6Sd9r1vkPOPaQZp3aBuj/7fzJvEOwBqhnTpak8cD4kkOTImISQERMLDk+Ani5wil6gMvS/ZuBLYCq1ReqTXxfD9gAWEnSvunhpYC3q53QrGPNmgW77uqCox1A0nn0XdADsAqePmHWdgbXkWSlCdWkam0kbQYMj4ipFe5eCngh3Z9JUmuvqmofKqrw81VcS8a60fz5sO++8M9/uuBoZ7i97PYc4MYc4jCzRdDICg6SlgNOB3btp8lcYMl0fxgZplxVu7rwIeAhSWtHxIV1xmrWWU45Ba691gVHO0REXFB6W9ISwArA9FwCMrMBaVSSJWlx4HLgqIjo7yrj+0iGCKcCo4Ena503S/f4BEkfBN4kmYs1LSJezxS1WSe4+WY45hgXHO0gkq5PF75//xBwF/CRnEIyswFoYJ2sA0mG/46WdDTJahBDImJCSZurgCmSViRZ03nTWifNkmRdDpwHbAcsBxwNbF1X6GbtygVHO9X6FY5FhWNmVmCN6smKiLOAs2q0mSOpB9gGODUiXqt13iwlHJaPiGuBtSJiL/rGI806mwuOdhxJh0l6Ghgh6Z+9G8kl255oZ9ZmGljCIZOImBURl0XES1naZ+nJel3SVcB9knYEPFRo3cEFRzvR+cDVwD1A6WoWr0bE3FwiMrMBW6zgJd+zJFm7AaMi4n5Jo4HdmxyTWf5ccLQjpd37r6X1/7yEjlmbK3iOlSnJehcYI2kf4LF0M+tcpQVHzzjDBUc7UETsJWloRLwpaQVgfkRUKj5oZgXWqGV1miXL+Md5JFfc3Eiyxtd5TY3ILE+lBUcPOAAOOijviKwJJO0FvJje/AzwoCTXADRrM62ek1WvLD1ZK0fEPun+TZJub2I8ZvkpLTi6wQYuONrZfgisBxARV0q6C7iTviUzzKwNNLIYaTNkSbJelHQUcDdJTYh/NTcks5yUFhy98kpY0hfSdriZJftzgSF5BWJmAzO44FlWliRrHHAwSZn5x9LbZp3llltccLS7nEFSVPDi9PaeuISDWdspeI5VdYHoFYBvk1R6/7mrvFvHevZZ+O//dsHRLhIR/yPpAWB7YBngp8BW+UZlZvUSxc6yqvVkTSapKTMcmAjsU6WtWXuaNw++8hUXHO0ikoaT1MjamiSxWppkNYt78ozLzOrXtj1ZwOIRcTGApK+0KB6z1vrOd1xwtEtIOhX4L2BZYArJRPddI2JknnGZ2cC1c5I1QtKeJAunfjjdByAiLml6ZGbNdsEF8MtfuuBo91iJpBzNsyTlG14E5ucakZktkgYuEN0U1ZKs3wBrVdivuYiqpHOAUcB1EXFSlXYjgRsjYoNs4Zo1iAuOdp107VUkjSLp0RoPLC7pXpKrp++OiMk5hmhmdRpc8MGHfpOsiDhhICeUtAswOCI2k3SupLUi4u/9NP8JXnDaWs0FR7taRDwOPA6cLmkQsBHJ3Ky9SeaimlmbKHrF9ywlHOrVQ19Bv5uBLYCFkixJWwFvABVXspY0nuSbJquuumoTwrSu5IKjViIi5gP3ptuPcw7HzOpU9DlZzehoWwp4Id2fCSw0qVTS4sAxwJH9nSQiJkXEmIgYM2LEiCaEaV3JBUfNzDpGJyyrU6+59A0BDqNyInckMDEiZhd90pp1kNKCoxdd5IKjZmZtblDB62Q1oyfrPpIhQoDRwPQKbbYGDknXQVxf0tlNiMOsT2nB0WOOgR13zDsiMzNbRB3RkyVpHZLLn58FnouIuVWaX0WyXMWKwA7AHpJOiogJvQ0iYsuSc98eEZ55bM1TWnB0u+1ccNTMrEMsVvBJWTV7siSdDpwAnAKsCVStkRURc0gmv08FxkbEQ6UJVoX2PXXEa1a/0oKjF18MgwfnHZGZmTVAo3uyJI2UNKXK/YtJelbS7em2brXzZRkuXDcidgVmR8R1JOt8VRURsyLisoioeOWgWctceGFScHTxxeGKK1xw1MysgwySMm+1pEtuXUByAV9/1gMujYiedHukanwZfocZko4Fhkvaj35KLpgVzkMPwde+luyfeSaMGZNvPGZm1lD19GRJGi9pWsk2vux07wG7A3OqPOWmwE6S7pF0jqSq066yJFn7Aq8Bd5H0Yo3L8BizfM2e7YKjZmYdblAdW2lpqHSbVHquiJgTEa/VeMp7ga0jYhNgCFD1KqosSdZuwCySZSdmp7fNiqu34Og//uGCo2ZmHayRw4UZPRwRL6b70+hbcrByfBlOqHRbEtgF2LJ6c7OcnXIK/P73LjhqZtbhckiyJksaLWkw8CXgoWqNa5ZwiIgLSm7+UtLERYvPrIlccNTMrGs0s4BDupj8nmUVEk4kqbIg4JqI+EO1c9RMsiSV9lyNAEYNIFaz5istOHrssS44ambW4ZpRZLS3tFS6mPyEsvseJbnCMJMsxUjHluy/AxyS9eRmLeOCo2ZmXafoS/NlGS48oRWBmC0SFxw1M+s6zVgbsJGyVHy/oRWBmA2YC46amXWlHCa+1xdfhjaPSPpi0yMxG4jSgqNnnOGCo2ZmXURS5i0PWeZkbQwcKukR4A0gImKr5oZllkFpwdH993fBUTOzLlP04cIsc7LG1mpj1nLlBUfPPLM5l5mYmVlhFX3ie79JoIcIrdB+9KOk4OiyyybzsFxw1Mys66iOLQ/VetoOa1kUZvX4wx+SgqOQXEm45pr5xmNmZrkYLGXe8lBtuHBTSX8rOyaSOVkfb2JMZv177rmk4Oj8+S44ambW5Qo+Wlg1ybrb87GsUHoLjr7yiguOmpkZym0gMJtqSdYVLYvCLIvvfAfuuccFR83MDGjjnqyIOLOVgZhV5YKjZmZWZlAb92SZFYMLjpqZWQVt25NlVgguOGpmZv3Ia7mcrJxkWXG54KiZmVUxqOD/JDjJsuJywVEzM6uina8uNMtPacHRiy5ywVEzM1tI0Qc3nGRZ8ZQWHD3mGPj85/OOyMzMCsg9WWb1KC04uu22cNxxeUdk1vWWWGwQB2y8EoME894Lzr3ned6LvKMyK/6crGprF5q13ne/mxQcXXVVuOQSFxw1K4CNV1mGPz41kzP+8hyvv/0uo0YOyzskSx13zPfZZ8/dmfTLiXmHkotBUuYtC0kjJU2p0eYcSXdJmlAzvoy/h1nzTZ4MZ53lgqNmBTPl6Vk8MeMNAIZ9YDCvz3s354gM4A+33Mz89+Yz+ZLf8Pzzz/HMM9PzDqnlVMdW81zScOACYKkqbXYBBkfEZsCaktaqdk4nWVYMDz+8YMHRjTfONx6zCiR9WNIekvbt3aq0HS9pmqRpj918WSvDbJo1lluSoUMGM33W23mHYsC0e+9h2+13AGCzzbfggfvvyzmi1qunJ6v0PZlu48tO9x6wOzCnylP2AL1v6JuBLarF5zlZlr/Zs2GXXeCtt1xw1IruRuC3wHO1GkbEJGASwCG/+2vbz2AaOmQQu603krPvfj7vUCz11ltv8uEPjwRgmWWW4a+Pv5BzRK1Xz5Ss0vdkP/fPAVD1ocWlgN4XeiawYbXGTrIsXy44au3l9Yg4Ke8gWm2w4KBNVuaax2Yw8y0PFRbF0KFDmTcv6VV88803iZifc0Q5aP0/F3OB3qKNw6gxItiU4cJak8IkLSPpBkk3S/qdpMWbEYe1ARcctfYyRdKlknaQtKWkLfMOqBU2X31ZVll2CbZbe3kO22JVNlxp6bxDMmDUqHXeHyL825NPsOKKK+UcUes1euJ7BvfRN0Q4GpherXHDe7JKJ4VJOlfSWhHx97JmewGnRcQtks4CtgeuaXQsVnAuOGrt5z/AE8DGJN+hA7gz14haYMrTs5ny9Oy8w7AyY/9ra/bfZ09efvll/vynO5l8SWfM/atHMzuyJI0C9oyI0g6jq0i+bK0I7ABsWu0czejJ6qHGpLCImBgRt6Q3RwAvl7cpnaA2Y8aMJoRpuXLBUWtPPwReApYjmZfxw3zDsW42bNgwzjl/MuuNHs3Z517I0kt3YQ9jIy8vTEVET/rz8bIEq3feVg8wFRgbEa9VO1czkqzySWEj+2soaTNgeERMLb8vIiZFxJiIGDNixIgmhGm5ccFRa1/nknym3QCsBJyXbzjW7T64zDJst/2OfKhL/51UHf81SkTMiojLIuKlWm2bMfE906QwScsBpwO7NiEGKzIXHLX2tUpE7JPu3yTpjlyjMetyRb9OqhlJVu+ksKkkk8KeLG+QTnS/HDgqIp5pQgxWVC44au3tX5KOAu4mmYvRfdfMmxVIwXOspgwXXgXsI+k04KvAY5LKL3k+kKS2xNGSbpe0exPisKIpLTh6+ukuOGrtaBxJocJdgdnpbTPLiZIio5m2PDS8Jysi5kjqAbYBTk3HLB8qa3MWcFajn9sKrLTg6LhxcPDBeUdkVreIeAc4M+84zCzRjcOFRMQs+q4wtG43fz7st19ScHT99WHixOK/M8zMrPCK/i+JK75b8/34x3DNNUnB0SuvdMFRazuSTouIwyXdRlIbC9I6WRGxVY6hmXW3gmdZTrKsuW69FSakZUYmT3bBUWtLEXF4+nNs3rGYWZ9GlmZoBidZ1jzPPQd77NFXcHSnnfKOyMzMOkjRZ540Ze1CMxcctU4kaZCkD0paTNJYSV1YYtusOKTsWx6cZFlzuOCodabLgS2BnwIHAb/LNxyz7pZHxfd6OMmyxnPBUetcy0fEtcBaEbEXfatbmFkOit6T5TlZ1lguOGqd7XVJVwH3SdoReD3neMy6WsGnZDnJsgZywVHrfLsBoyLifkmjAa9WYZangmdZTrKsMVxw1LrDO8BTkhYDlgP+mXM8Zl1tUMH/nfGcLGsMFxy17uCJ72YFojq2PDjJskXngqPWPTzx3axICp5lebjQFk1pwdEJE1xw1DqdJ76bFYgrvlvnmjcPdtutr+Do8cfnHZFZs3niu1mBFHxKlocLbREcfjjcfXdScPTii11w1DpeRLwNvCNpO5JJ8O/lHJJZVyv4aKGTLBugyZOTKwh7C45+6EN5R2TWdJJOB04ATgHWBC7JNyKz7iYp85bhXOdIukvShH7uX0zSs5JuT7d1a53TSZbVzwVHrXutGxG7ArMj4jpgmbwDMutmjar4LmkXYHBEbAasKWmtCs3WAy6NiJ50e6RWfE6yrD4uOGrdbYakY4HhkvYDXso7ILNuVs9woaTxkqaVbONLTtUDXJbu3wxsUeHpNgV2knRP2utVc167kyzLzgVHzfYFXgPuIunF2j/fcMy6XB1ZVkRMiogxJdukkjMtBbyQ7s8ERlZ4tnuBrSNiE2AIsGOt8Hx1oWXngqPW5SLiLeDnecdhZokGlnCYS1/du2FU7oR6OCLmpfvTgEpDigtwT5Zl44KjZki6Ie8YzKxPo+ZkAffRN0Q4Gpheoc1kSaMlDQa+BDxU66ROsqw2Fxw16/WIpC/mHYSZJQYp+1bDVcA+kk4Dvgo8JumksjYnApOBB4G7IuIPtU7q4UKrzgVHzUptDBwq6RHgDSAiYqucYzLrYo0ZLoyIOZJ6gG2AUyPiJcp6qiLiUZIrDDNzkmXVueCo2fsiYmzeMZhZn0ZeexURs+i7wrAhnGRZ/y66yAVHzcpI2gBYHfhHRDycczhmXa3o17c7ybLKHn4YxqclRH7xCxccNQMk/YKk0vvDwMGS/hoR/yfnsMy6VtGrCDnJsoXNng277poUHN1vv75ky8w2jIj3ixRKmpJnMGbdLstyOXlykmULmj8/qeT+1FMwerQLjpot6N+SdgfuBzYBnpe0akQ8m3NcZl2p6P86OcmyBZ16Klx9NSyzTFJwdOjQvCMyK5I5wPbpBvA2cDxwQF4BmXWzovcBOMmyPrfeCkcfnexfdBF89KP5xmNWMBGxv6ThwIrALOCliJifc1hmXauBFd+bwsVILeGCo2Y1SToCuAG4lGRB2fPzjMes69WzQnQOnGSZC46aZbdzRGwKvBoRl5BcaWhmOSl4jtWc4UJJ5wCjgOsiorwsfeY2dXvnHXjttYacqqscd5wLjpplM0fSvsASkj4HzM45HrOuNqjgk7IanmRJ2gUYHBGbSTpX0loR8fd62wzIHXckPTFWPxccNatK0jrAVOAcklGAI4BxecZk1u0KnmM1pSerh76y9DeTrGpdnkDVbCNpPDAeYNVVV832zEOGOEkYiKFD4Uc/csFRs35IOgg4DrgOOBVYnuQKw88Bl+cYmpkVWDOSrKWAF9L9mcCGA2kTEZOASQBjxoyJTM/c0wMzZtQXrZlZbeOB0RExs/eApGWB63GSZZabbuzJmgssme4Po/Lk+ixtzMyKYgiwthYuL/2BPIIxs0TRSzg0I8m6j2T4byowGnhygG3MzIriQdLpC2W8QLRZjrqxJ+sqYIqkFYEdgD0knRQRE6q02bQJcZiZNURE7J93DGa2sKInWQ0fpouIOSQT26cCYyPiobIEq1Ib110wMzOzuqiO//LQlDpZETGLvqsHB9zGzMzMrD9F78ny2oVmZmbWlgqeY/mqPjMzM2tTDVxXR9I5ku6SNGFR2pRykmVmZmZtaZCUeaumdCUaYE1Jaw2kzUKPichW5zNPkmYAz9TxkA8BrzQpnGZy3K3luFur3rhXi4gRzQrG6idpfFoo2grEf5dsSleSSU3qfd0k/QK4MSKul7QHsGREnFf2+JptyrXFnKx6P2glTYuIMc2Kp1kcd2s57tZq17htAeNJV+KwQvHfJYPSlWQqaMhqNeU8XGhmZmbdrimr1TjJMjMzs27XuxINJCvRTB9gmwW0xXDhALRrt6njbi3H3VrtGrf18d+wmPx3WXRX0YTVatpi4ruZmZlZM0kaDmwD3BkRLw20zQLtnWSZmZmZNZ7nZJmZmZk1QVsnWc2oztoKtWKStIykGyTdLOl3khZvdYyVZH0tJY2U9ECr4qqljrgnStq5VXHVkuH/k+GSrpc0TdKvWh1fNen/A1NqtCnce7NbSDpf0g/T/eMlHd+k57m9GedtJ+lr/WD6Pj24APH8LO8YuknbJlnNqs7abBlj2gs4LSK2BV4Ctm9ljJXU+Vr+hL7LXHOVNW5JnwVWiIjftzTAfmSMex/g4rT21NKSClGDKp2zcAFJTZn+2hTuvdmFDpa0RN5BdIlvAdsBx0laL89AIuI7eT5/t2nbJAvoAS5L92+m77LKetu0Wg81YoqIiRFxS3pzBPBya0KrqocMr6WkrYA3SJLDIuihRtyShgC/BqZL+mLrQquqh9qv96vAOpKWBVYBnmtJZLW9B+wOzKnSpofivTe7zaMkX+gAPiDpUkl3SLq4t/dc0u2S/kfSTent+9Je9qsl3S3p65JWlPQnSVMknZzXL1N0EfEqcB3w5fQ1/Iuko+D93q7fSvqzpDN6H1Ph9R8q6QpJd0o6Mz22pKRr02O/k7RYpWOl5yzZX+jvnvZsnpw+9kFJK7ToJepI7ZxklVdeHTnANq2WOSZJmwHDI2JqKwKroWbc6QfzMcCRLYyrliyv977A48CpwCaSDm1RbNVkiftPwGrAt4G/pu1yFxFzIuK1Gs2K+N7sNmcCX0v3DwYejYjPAX8HDkiPbwrcFRHbpbeHArsB6wF7Ap8GViJ5z+8AFGa4vaBeJfmM/E1EbA58SdLy6X1XRMRngDUkbZQeK3/9x5P8nbYEPpL2io0C5qfHziMpklnpWCX9/d0/lj72t8BWDfnNu1Q7J1lNqc7aAplikrQccDp9/9PnLUvcRwITI2J2q4LKIEvcG5CsYfUScBEwtkWxVZMl7uOAr0fEicATwP4tiq0Rivje7DYvkfx/00PSq3h3enwq8Ml0/9GI+G3JY/4dEXNJ1pJ9DxDwLsl7/2xg6eaH3daWAwYD30h7lJYCVkzvuy/9+TCwerpf/vqvTdITdjuwJkmCez/wqKSbSYYk3+znWCWjqPx3vzD9+SxQiDnB7aqdP9iaUp21BWrGlPYIXQ4cFRH1LIzdTFley62BQ9IPgPUlnd2a0KrKEvdTJB9YAGOobzHyZskS93BgXUmDSXoU2qkeSxHfm93op8DnSHqnegsrbgo8lu7PzXCOw4FTgINor/8HWyod1t8BuAk4MiJ6gB/R1wO9SfpzfeAf6X756/8k8LP0sRNIkqDRwJ/TObzDgc/2c6ySx6j8d3+j3t/PKmvnJOsqYB9JpwFfBR6TdFKNNte1NMLKrqJ23AeSLDx5dDomv3uLY6zkKmrEHRFbRkRP+gHwYEQc1PowF3IVtV/vc4Cxku4EvkkycT9vV1E77lNIKj2/RvIN+dKWRpiRpFFt8t7sOhHxAHAHSS/Up9L3wFrA+XWc5lrgl8A1wJuSVmp0nB3gdOBG4AhgHPA9SX8muajp32mbndJjT0TEg/2c59fADunf6esk8zCnA9+W9BdgBWBaP8cqWZS/u2XQ1sVI1YTqrK1QxJiycNyt1a5xZ9Xpv59ZVpLOB46PiOk5h2IN1tZJlpmZmVlRtfNwoZmZmVlhOckyMzMzawInWWZmZmZN4CSri6SVfP+aXrF4u6Rv1Wh/e4Of905Jt0pasfajFnj8+pLWr3D8Zw2K73xJDyhZR+9yJRXg+2vbI2n1RjyvmSUkjZP0lqR/S/qXpO834Jy3S+opuf09Sd9rwHkbch7rDk6yus/JvWUWIuKM2s0b+ry91Yfrrai+frotoMFrcB2arqM3l6TeV3966CsUaGaNc3VEjCSpJr+fpM0befKI+ElEZC7Pon4Wza73PNbdnGR1OUnDJN2Yrjt2XpV2ldbHWmgdrQyGA2/1s2ZWpec4haSa9JGSbi2L6faS/aMlfSndP0rSbvXGJ0kk1cffUYX12NLXZxzwM0kXp8dGqmwdMjMbuIh4haT2Vn8FNFvluJyf3zqAk6zu01vgdGJ6+yMkhfK2BlaX1N8acpXWwqq0jla1572TpKrwz6m8ZtZCzxERR5FURf5RRPxXlfNfTlJNGWBL4Po64zudpIDfv4E/UmE9tojYn6RY33ciondh3aOovA6ZmQ2cgPnpZ9VXJF0l6Y/v3yntKOmJdHjx+JLjx0p6UdINwAcXOGEybeH4smN7Sfpn+pj/mx77saSX0v2XJD2W4Tx7S3pa0nRJ49Jj49IvkBdLelXSlekXOesii9VuYh3m5Ii4qOT2f0iWw9ifpGr4khUfteBaWH8nqV68NrB5Ou9hWZLE5OEszytpFMnio5CsmbUD8KsKz5FJRPxN0sqSPgjMjog3JNUT36Eky7zMi4iQ9C7JN9m5VF+PbW1gs/SDtXcdslezxm1mC5K0MvAFYHfg88APge+RVKZH0giSL0VjgVnAVElXA0Po+7L2MZLPlWrP80mSL3CbAW8Bj0i6JiKOAI6QFBGxQoZ4P1FynvfSeHrXIdyVZMmir5Es37U+8ECmF8I6gnuy7EDgCuC/qb5eVaW1sCqto5VVpTWz+ltv6y1gKLw/pNefe4DvkCzvwQDi+xVwoJK1APtbj608liepvA6ZmdXni2kP0h3ATyOiN1E5NyKuiYjX0tubknxhuofky9iKwKeAzYHrImJWRNwLPFLj+bYGro2I5yPi1YhYMSKeHEDc26TneS4i/gX8jmRRZoBpEfH7dFHtJ4FlBnB+a2NOsuwWkiGv3q74/tYdm87Ca2FVWkcrq0prZlV6jt4Yd1Gyrle1eRqXkyRZ16a364ovImaRvA670v96bFeSzA+bCnyUJLGqtA6ZmdXn6ohYISI+GhGlcyjLe6QE3Ja2XQFYmeR9KRb8QjS/nieXtG3a+z0QUbbfe/sf/bSxLuFldczMLFfpcPv2EbFH2fHbSdb0u73k2IdJhv0/BzwN3A78GHgFuBDYkOQL0D3A1r2P7Z1HFRHHp7dHkUxJ2JRkWsA0YI+IuD+9/xVgI+BfwNDenrQK5/kEyRfBTUkSu6nATuljeyJiXH+/i3U+z8kyM7O2EREvSzqIpJd5aeDSiLgaQNL/An8jGZp7vMZ5Hpc0AfgzMJhkiPL+kib/X3rfEsCXgSn9nOeJ9MriP5H0ph0XEY9I2mgRfk3rEO7JMjMzM2sCz8kyMzMzawInWWZmZmZN4CTLzMzMrAmcZJmZmZk1gZMsMzMzsyZwkmVmZmbWBE6yzMzMzJrg/wdpTU1fT7wgUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Knn with stage 1+ stage2\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif']=[u'simHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "# STUDENT-t\n",
    "def outlier_t(data):\n",
    "    n=len(data)\n",
    "    mu= np.mean(data)\n",
    "    S= np.std(data,ddof=1)#variance\n",
    "    t_av= 1.440 #https:// 90% confidence, 1 side, en.wikipedia.org/wiki/Student%27s_t-distribution\n",
    "#     t_av =0.906 # 80%\n",
    "    t_av= 1.943 #  95% confidentce, 1 side\n",
    "    X_left = mu-t_av*S/np.sqrt(n)\n",
    "    X_right = mu +t_av*S/np.sqrt(n)\n",
    "#     print(X_left,\",\",X_right)\n",
    "    result=[]\n",
    "    for i in range(len(data)):\n",
    "        if data[i] >= X_right:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "# ===========================machine learning algorithms to predict and plot====================#\n",
    "\n",
    "### 10. knn Classifier \n",
    "print(\"=============================================================================================\") \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"10. KNN\")\n",
    "print('sugmentation factor =', agm_factor)\n",
    "print('feature number =', len(col_slct))\n",
    "\n",
    "# knn=KNeighborsClassifier(n_neighbors= 5)\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train,y_train_dummy)\n",
    "# pred = knn.predict(X_test) ####\n",
    "# predictions_10 = pred.argmax(axis = 1) ####\n",
    "\n",
    "############ LR\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# clf = LogisticRegression(penalty='l2')\n",
    "# clf.fit(X_train,y_train_dummy)  ####\n",
    "# pred = clf.predict(X_test) ####\n",
    "# predictions_2 = pred.argmax(axis = 1) ####\n",
    "\n",
    "############# svm\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "# clf.fit(X_train,y_train)\n",
    "# clf.fit(X_train,y_train_dummy)  ####\n",
    "# pred = clf.predict(X_test) ####\n",
    "# predictions_9 = pred.argmax(axis = 1) ####\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#     print(\"=============================================================================================\")     \n",
    "#     print(\"1. RF\")\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=10,oob_score= True) # after tuning, n_estimators =100 is too large\n",
    "RF.fit(X_train,y_train_dummy) ###\n",
    "pred = RF.predict(X_test) ####\n",
    "predictions_10 = pred.argmax(axis = 1) ####\n",
    "\n",
    "predicted_df = pd.DataFrame(predictions_10, index =X_test.index) \n",
    "predictions_temp, score_user_10  = classifier_slice2uid(predicted_df, trigger= trigger) \n",
    "# predictions_10 =  predictions_temp.iloc[:,-1].to_list() \n",
    "\n",
    "predictions_10 = outlier_t(list(score_user_10.values()))\n",
    "y_test = np.array(phq.loc[list(predictions_temp.index),\"dp_class\"].to_list()) \n",
    "mse_10 = np.sum((y_test-list(score_user_10.values()))**2/len(y_test))\n",
    "\n",
    "print(score_user_10)\n",
    "for value in score_user_10.values():\n",
    "    print('%.9f, '%value, end='')\n",
    "print(predictions_10)\n",
    "print(\"mse of score by users: \",mse_10)\n",
    "print(classification_report(y_test,predictions_10))\n",
    "print(\"AC\",accuracy_score(y_test,predictions_10))\n",
    "\n",
    "####################### Plot ############################\n",
    "fpr_10, tpr_10, thresholds_10 = \\\n",
    "roc_curve(y_test,predictions_10)\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "# plt.xlim(0,1) \n",
    "# plt.ylim(0.0,1.1) \n",
    "plt.title('ROC',fontsize=14,fontweight='bold')\n",
    "plt.xlabel('False Postive Rate')\n",
    "plt.ylabel('True Postive Rate')\n",
    "plt.plot(fpr_10,tpr_10,linewidth=2, linestyle=\"-\",color='red')\n",
    "\n",
    "plt.subplot(122)\n",
    "ticks=['Normal','Depression']\n",
    "conf_mat = confusion_matrix(y_test, predictions_10)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=ticks, yticklabels=ticks,cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix',fontsize=14,fontweight='bold')\n",
    "plt.ylabel('Actual',fontsize=12)\n",
    "plt.xlabel('Prediction',fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dc829d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>aug_factor</th>\n",
       "      <th>u00_prob</th>\n",
       "      <th>u05_prob</th>\n",
       "      <th>u09_prob</th>\n",
       "      <th>u16_prob</th>\n",
       "      <th>u27_prob</th>\n",
       "      <th>u42_prob</th>\n",
       "      <th>u49_prob</th>\n",
       "      <th>u52_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>u16gth</th>\n",
       "      <th>u27gth</th>\n",
       "      <th>u42gth</th>\n",
       "      <th>u49gth</th>\n",
       "      <th>u52gth</th>\n",
       "      <th>mse</th>\n",
       "      <th>entropy</th>\n",
       "      <th>model</th>\n",
       "      <th>feature num</th>\n",
       "      <th>random seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224969</td>\n",
       "      <td>2.375875</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191014</td>\n",
       "      <td>2.874400</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222878</td>\n",
       "      <td>2.342320</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226434</td>\n",
       "      <td>2.074218</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185365</td>\n",
       "      <td>2.075278</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232030</td>\n",
       "      <td>1.064389</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178116</td>\n",
       "      <td>2.289864</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.246555</td>\n",
       "      <td>1.584831</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219659</td>\n",
       "      <td>2.734879</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231033</td>\n",
       "      <td>2.640595</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.239152</td>\n",
       "      <td>0.905005</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.168626</td>\n",
       "      <td>2.811295</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222646</td>\n",
       "      <td>2.361843</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220520</td>\n",
       "      <td>2.246193</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185001</td>\n",
       "      <td>2.113702</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>1.201020</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183887</td>\n",
       "      <td>2.396664</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.243661</td>\n",
       "      <td>2.038236</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203662</td>\n",
       "      <td>2.856602</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.204719</td>\n",
       "      <td>2.819642</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.239152</td>\n",
       "      <td>0.905005</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171766</td>\n",
       "      <td>2.932636</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221018</td>\n",
       "      <td>2.260754</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219138</td>\n",
       "      <td>2.419168</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>2.170705</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>1.201020</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.186472</td>\n",
       "      <td>2.366592</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.243958</td>\n",
       "      <td>2.041109</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203258</td>\n",
       "      <td>2.925474</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215882</td>\n",
       "      <td>2.573117</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232030</td>\n",
       "      <td>1.064389</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.197176</td>\n",
       "      <td>2.778944</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.207733</td>\n",
       "      <td>2.643550</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214687</td>\n",
       "      <td>2.355108</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185975</td>\n",
       "      <td>2.166100</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208939</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.177799</td>\n",
       "      <td>2.361876</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238135</td>\n",
       "      <td>2.237985</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.197575</td>\n",
       "      <td>2.898052</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>2.658155</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221847</td>\n",
       "      <td>2.174271</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.176617</td>\n",
       "      <td>2.819239</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211802</td>\n",
       "      <td>2.610343</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>2.315509</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.177675</td>\n",
       "      <td>2.301252</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.199851</td>\n",
       "      <td>2.048969</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.167550</td>\n",
       "      <td>2.457002</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228393</td>\n",
       "      <td>2.307667</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.192379</td>\n",
       "      <td>2.920931</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.212635</td>\n",
       "      <td>2.685503</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211932</td>\n",
       "      <td>2.142285</td>\n",
       "      <td>LR</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.194334</td>\n",
       "      <td>2.892268</td>\n",
       "      <td>DT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213847</td>\n",
       "      <td>2.573614</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>2.578679</td>\n",
       "      <td>AB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173110</td>\n",
       "      <td>2.283457</td>\n",
       "      <td>GNB</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.197025</td>\n",
       "      <td>2.095157</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.159353</td>\n",
       "      <td>2.501585</td>\n",
       "      <td>QDA</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222219</td>\n",
       "      <td>2.300669</td>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.662599e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191388</td>\n",
       "      <td>2.922385</td>\n",
       "      <td>KNN</td>\n",
       "      <td>10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time  aug_factor  u00_prob  u05_prob  u09_prob  u16_prob  \\\n",
       "0   1.662599e+09           0  0.013793  0.029851  0.000000  0.089552   \n",
       "1   1.662599e+09           0  0.000000  0.000000  0.000000  0.000000   \n",
       "2   1.662599e+09           0  0.062069  0.194030  0.101449  0.253731   \n",
       "3   1.662599e+09           0  0.013793  0.000000  0.014493  0.104478   \n",
       "4   1.662599e+09           0  0.006897  0.014925  0.000000  0.104478   \n",
       "5   1.662599e+09           0  0.041379  0.000000  0.000000  0.313433   \n",
       "6   1.662599e+09           0  0.006897  0.000000  0.000000  0.059701   \n",
       "7   1.662599e+09           0  0.048276  0.029851  0.000000  0.343284   \n",
       "8   1.662599e+09           0  0.000000  0.000000  0.000000  0.000000   \n",
       "9   1.662599e+09           0  0.020690  0.029851  0.014493  0.059701   \n",
       "10  1.662599e+09           1  0.020690  0.014925  0.014493  0.059701   \n",
       "11  1.662599e+09           1  0.000000  0.000000  0.000000  0.029851   \n",
       "12  1.662599e+09           1  0.027586  0.089552  0.144928  0.298507   \n",
       "13  1.662599e+09           1  0.000000  0.014925  0.014493  0.104478   \n",
       "14  1.662599e+09           1  0.013793  0.014925  0.000000  0.119403   \n",
       "15  1.662599e+09           1  0.048276  0.000000  0.000000  0.313433   \n",
       "16  1.662599e+09           1  0.006897  0.000000  0.000000  0.104478   \n",
       "17  1.662599e+09           1  0.041379  0.029851  0.000000  0.298507   \n",
       "18  1.662599e+09           1  0.000000  0.000000  0.000000  0.014925   \n",
       "19  1.662599e+09           1  0.027586  0.044776  0.086957  0.104478   \n",
       "20  1.662599e+09           2  0.027586  0.044776  0.028986  0.134328   \n",
       "21  1.662599e+09           2  0.000000  0.000000  0.000000  0.029851   \n",
       "22  1.662599e+09           2  0.075862  0.194030  0.130435  0.253731   \n",
       "23  1.662599e+09           2  0.006897  0.014925  0.028986  0.134328   \n",
       "24  1.662599e+09           2  0.006897  0.044776  0.000000  0.119403   \n",
       "25  1.662599e+09           2  0.062069  0.000000  0.000000  0.313433   \n",
       "26  1.662599e+09           2  0.006897  0.000000  0.000000  0.104478   \n",
       "27  1.662599e+09           2  0.048276  0.029851  0.000000  0.298507   \n",
       "28  1.662599e+09           2  0.000000  0.000000  0.000000  0.014925   \n",
       "29  1.662599e+09           2  0.048276  0.089552  0.101449  0.119403   \n",
       "30  1.662599e+09           3  0.020690  0.044776  0.028986  0.149254   \n",
       "31  1.662599e+09           3  0.006897  0.000000  0.000000  0.059701   \n",
       "32  1.662599e+09           3  0.082759  0.164179  0.072464  0.253731   \n",
       "33  1.662599e+09           3  0.020690  0.014925  0.028986  0.149254   \n",
       "34  1.662599e+09           3  0.013793  0.029851  0.000000  0.134328   \n",
       "35  1.662599e+09           3  0.068966  0.000000  0.000000  0.328358   \n",
       "36  1.662599e+09           3  0.006897  0.000000  0.000000  0.164179   \n",
       "37  1.662599e+09           3  0.068966  0.029851  0.000000  0.388060   \n",
       "38  1.662599e+09           3  0.013793  0.000000  0.000000  0.029851   \n",
       "39  1.662599e+09           3  0.055172  0.119403  0.115942  0.149254   \n",
       "40  1.662599e+09           4  0.013793  0.044776  0.014493  0.089552   \n",
       "41  1.662599e+09           4  0.013793  0.000000  0.014493  0.089552   \n",
       "42  1.662599e+09           4  0.055172  0.194030  0.086957  0.268657   \n",
       "43  1.662599e+09           4  0.027586  0.014925  0.028986  0.134328   \n",
       "44  1.662599e+09           4  0.013793  0.044776  0.000000  0.179104   \n",
       "45  1.662599e+09           4  0.062069  0.029851  0.000000  0.358209   \n",
       "46  1.662599e+09           4  0.013793  0.000000  0.014493  0.194030   \n",
       "47  1.662599e+09           4  0.075862  0.044776  0.000000  0.417910   \n",
       "48  1.662599e+09           4  0.013793  0.000000  0.000000  0.044776   \n",
       "49  1.662599e+09           4  0.068966  0.164179  0.144928  0.149254   \n",
       "50  1.662599e+09           5  0.020690  0.044776  0.028986  0.134328   \n",
       "51  1.662599e+09           5  0.013793  0.000000  0.014493  0.134328   \n",
       "52  1.662599e+09           5  0.075862  0.149254  0.144928  0.194030   \n",
       "53  1.662599e+09           5  0.027586  0.029851  0.028986  0.164179   \n",
       "54  1.662599e+09           5  0.013793  0.074627  0.014493  0.149254   \n",
       "55  1.662599e+09           5  0.068966  0.029851  0.000000  0.388060   \n",
       "56  1.662599e+09           5  0.020690  0.000000  0.014493  0.208955   \n",
       "57  1.662599e+09           5  0.082759  0.044776  0.000000  0.447761   \n",
       "58  1.662599e+09           5  0.013793  0.000000  0.000000  0.044776   \n",
       "59  1.662599e+09           5  0.068966  0.164179  0.159420  0.149254   \n",
       "\n",
       "    u27_prob  u42_prob  u49_prob  u52_prob  ...  u16gth  u27gth  u42gth  \\\n",
       "0   0.140845  0.014493  0.072464  0.028169  ...     1.0     0.0     0.0   \n",
       "1   0.000000  0.000000  0.000000  0.000000  ...     1.0     0.0     0.0   \n",
       "2   0.281690  0.173913  0.217391  0.126761  ...     1.0     0.0     0.0   \n",
       "3   0.056338  0.014493  0.072464  0.014085  ...     1.0     0.0     0.0   \n",
       "4   0.084507  0.014493  0.043478  0.000000  ...     1.0     0.0     0.0   \n",
       "5   0.098592  0.043478  0.231884  0.028169  ...     1.0     0.0     0.0   \n",
       "6   0.000000  0.000000  0.000000  0.014085  ...     1.0     0.0     0.0   \n",
       "7   0.154930  0.043478  0.217391  0.042254  ...     1.0     0.0     0.0   \n",
       "8   0.014085  0.000000  0.014493  0.014085  ...     1.0     0.0     0.0   \n",
       "9   0.098592  0.101449  0.115942  0.084507  ...     1.0     0.0     0.0   \n",
       "10  0.112676  0.072464  0.028986  0.028169  ...     1.0     0.0     0.0   \n",
       "11  0.000000  0.000000  0.000000  0.014085  ...     1.0     0.0     0.0   \n",
       "12  0.239437  0.159420  0.173913  0.154930  ...     1.0     0.0     0.0   \n",
       "13  0.056338  0.014493  0.057971  0.014085  ...     1.0     0.0     0.0   \n",
       "14  0.112676  0.014493  0.057971  0.014085  ...     1.0     0.0     0.0   \n",
       "15  0.112676  0.043478  0.217391  0.028169  ...     1.0     0.0     0.0   \n",
       "16  0.000000  0.000000  0.014493  0.014085  ...     1.0     0.0     0.0   \n",
       "17  0.126761  0.086957  0.188406  0.042254  ...     1.0     0.0     0.0   \n",
       "18  0.028169  0.028986  0.072464  0.014085  ...     1.0     0.0     0.0   \n",
       "19  0.154930  0.115942  0.130435  0.126761  ...     1.0     0.0     0.0   \n",
       "20  0.098592  0.057971  0.086957  0.070423  ...     1.0     0.0     0.0   \n",
       "21  0.000000  0.000000  0.000000  0.014085  ...     1.0     0.0     0.0   \n",
       "22  0.140845  0.144928  0.159420  0.169014  ...     1.0     0.0     0.0   \n",
       "23  0.084507  0.014493  0.101449  0.000000  ...     1.0     0.0     0.0   \n",
       "24  0.154930  0.043478  0.072464  0.028169  ...     1.0     0.0     0.0   \n",
       "25  0.112676  0.057971  0.231884  0.028169  ...     1.0     0.0     0.0   \n",
       "26  0.000000  0.014493  0.000000  0.014085  ...     1.0     0.0     0.0   \n",
       "27  0.169014  0.057971  0.217391  0.042254  ...     1.0     0.0     0.0   \n",
       "28  0.056338  0.028986  0.072464  0.014085  ...     1.0     0.0     0.0   \n",
       "29  0.169014  0.115942  0.159420  0.126761  ...     1.0     0.0     0.0   \n",
       "30  0.211268  0.057971  0.086957  0.028169  ...     1.0     0.0     0.0   \n",
       "31  0.000000  0.000000  0.000000  0.014085  ...     1.0     0.0     0.0   \n",
       "32  0.239437  0.173913  0.057971  0.056338  ...     1.0     0.0     0.0   \n",
       "33  0.084507  0.043478  0.101449  0.042254  ...     1.0     0.0     0.0   \n",
       "34  0.140845  0.028986  0.043478  0.028169  ...     1.0     0.0     0.0   \n",
       "35  0.126761  0.057971  0.260870  0.028169  ...     1.0     0.0     0.0   \n",
       "36  0.000000  0.000000  0.028986  0.014085  ...     1.0     0.0     0.0   \n",
       "37  0.197183  0.101449  0.275362  0.042254  ...     1.0     0.0     0.0   \n",
       "38  0.070423  0.028986  0.115942  0.028169  ...     1.0     0.0     0.0   \n",
       "39  0.239437  0.115942  0.202899  0.154930  ...     1.0     0.0     0.0   \n",
       "40  0.112676  0.028986  0.086957  0.028169  ...     1.0     0.0     0.0   \n",
       "41  0.028169  0.014493  0.000000  0.028169  ...     1.0     0.0     0.0   \n",
       "42  0.323944  0.101449  0.217391  0.183099  ...     1.0     0.0     0.0   \n",
       "43  0.140845  0.028986  0.072464  0.042254  ...     1.0     0.0     0.0   \n",
       "44  0.211268  0.072464  0.043478  0.028169  ...     1.0     0.0     0.0   \n",
       "45  0.126761  0.057971  0.260870  0.042254  ...     1.0     0.0     0.0   \n",
       "46  0.028169  0.014493  0.057971  0.028169  ...     1.0     0.0     0.0   \n",
       "47  0.211268  0.130435  0.260870  0.070423  ...     1.0     0.0     0.0   \n",
       "48  0.098592  0.028986  0.115942  0.056338  ...     1.0     0.0     0.0   \n",
       "49  0.239437  0.115942  0.217391  0.197183  ...     1.0     0.0     0.0   \n",
       "50  0.154930  0.043478  0.072464  0.042254  ...     1.0     0.0     0.0   \n",
       "51  0.028169  0.014493  0.014493  0.028169  ...     1.0     0.0     0.0   \n",
       "52  0.338028  0.173913  0.144928  0.169014  ...     1.0     0.0     0.0   \n",
       "53  0.154930  0.057971  0.101449  0.014085  ...     1.0     0.0     0.0   \n",
       "54  0.197183  0.057971  0.072464  0.042254  ...     1.0     0.0     0.0   \n",
       "55  0.126761  0.057971  0.260870  0.042254  ...     1.0     0.0     0.0   \n",
       "56  0.042254  0.014493  0.057971  0.028169  ...     1.0     0.0     0.0   \n",
       "57  0.225352  0.173913  0.304348  0.112676  ...     1.0     0.0     0.0   \n",
       "58  0.112676  0.028986  0.115942  0.084507  ...     1.0     0.0     0.0   \n",
       "59  0.239437  0.130435  0.231884  0.211268  ...     1.0     0.0     0.0   \n",
       "\n",
       "    u49gth  u52gth       mse   entropy  model  feature num  random seed  \n",
       "0      0.0     1.0  0.224969  2.375875     RF           10         42.0  \n",
       "1      0.0     1.0  0.250000  0.000000     LR           10         42.0  \n",
       "2      0.0     1.0  0.191014  2.874400     DT           10         42.0  \n",
       "3      0.0     1.0  0.222878  2.342320   GBDT           10         42.0  \n",
       "4      0.0     1.0  0.226434  2.074218     AB           10         42.0  \n",
       "5      0.0     1.0  0.185365  2.075278    GNB           10         42.0  \n",
       "6      0.0     1.0  0.232030  1.064389    LDA           10         42.0  \n",
       "7      0.0     1.0  0.178116  2.289864    QDA           10         42.0  \n",
       "8      0.0     1.0  0.246555  1.584831    SVM           10         42.0  \n",
       "9      0.0     1.0  0.219659  2.734879    KNN           10         42.0  \n",
       "10     0.0     1.0  0.231033  2.640595     RF           10         42.0  \n",
       "11     0.0     1.0  0.239152  0.905005     LR           10         42.0  \n",
       "12     0.0     1.0  0.168626  2.811295     DT           10         42.0  \n",
       "13     0.0     1.0  0.222646  2.361843   GBDT           10         42.0  \n",
       "14     0.0     1.0  0.220520  2.246193     AB           10         42.0  \n",
       "15     0.0     1.0  0.185001  2.113702    GNB           10         42.0  \n",
       "16     0.0     1.0  0.221781  1.201020    LDA           10         42.0  \n",
       "17     0.0     1.0  0.183887  2.396664    QDA           10         42.0  \n",
       "18     0.0     1.0  0.243661  2.038236    SVM           10         42.0  \n",
       "19     0.0     1.0  0.203662  2.856602    KNN           10         42.0  \n",
       "20     0.0     1.0  0.204719  2.819642     RF           10         42.0  \n",
       "21     0.0     1.0  0.239152  0.905005     LR           10         42.0  \n",
       "22     0.0     1.0  0.171766  2.932636     DT           10         42.0  \n",
       "23     0.0     1.0  0.221018  2.260754   GBDT           10         42.0  \n",
       "24     0.0     1.0  0.219138  2.419168     AB           10         42.0  \n",
       "25     0.0     1.0  0.186189  2.170705    GNB           10         42.0  \n",
       "26     0.0     1.0  0.221781  1.201020    LDA           10         42.0  \n",
       "27     0.0     1.0  0.186472  2.366592    QDA           10         42.0  \n",
       "28     0.0     1.0  0.243958  2.041109    SVM           10         42.0  \n",
       "29     0.0     1.0  0.203258  2.925474    KNN           10         42.0  \n",
       "30     0.0     1.0  0.215882  2.573117     RF           10         42.0  \n",
       "31     0.0     1.0  0.232030  1.064389     LR           10         42.0  \n",
       "32     0.0     1.0  0.197176  2.778944     DT           10         42.0  \n",
       "33     0.0     1.0  0.207733  2.643550   GBDT           10         42.0  \n",
       "34     0.0     1.0  0.214687  2.355108     AB           10         42.0  \n",
       "35     0.0     1.0  0.185975  2.166100    GNB           10         42.0  \n",
       "36     0.0     1.0  0.208939  1.102273    LDA           10         42.0  \n",
       "37     0.0     1.0  0.177799  2.361876    QDA           10         42.0  \n",
       "38     0.0     1.0  0.238135  2.237985    SVM           10         42.0  \n",
       "39     0.0     1.0  0.197575  2.898052    KNN           10         42.0  \n",
       "40     0.0     1.0  0.224609  2.658155     RF           10         42.0  \n",
       "41     0.0     1.0  0.221847  2.174271     LR           10         42.0  \n",
       "42     0.0     1.0  0.176617  2.819239     DT           10         42.0  \n",
       "43     0.0     1.0  0.211802  2.610343   GBDT           10         42.0  \n",
       "44     0.0     1.0  0.209037  2.315509     AB           10         42.0  \n",
       "45     0.0     1.0  0.177675  2.301252    GNB           10         42.0  \n",
       "46     0.0     1.0  0.199851  2.048969    LDA           10         42.0  \n",
       "47     0.0     1.0  0.167550  2.457002    QDA           10         42.0  \n",
       "48     0.0     1.0  0.228393  2.307667    SVM           10         42.0  \n",
       "49     0.0     1.0  0.192379  2.920931    KNN           10         42.0  \n",
       "50     0.0     1.0  0.212635  2.685503     RF           10         42.0  \n",
       "51     0.0     1.0  0.211932  2.142285     LR           10         42.0  \n",
       "52     0.0     1.0  0.194334  2.892268     DT           10         42.0  \n",
       "53     0.0     1.0  0.213847  2.573614   GBDT           10         42.0  \n",
       "54     0.0     1.0  0.211814  2.578679     AB           10         42.0  \n",
       "55     0.0     1.0  0.173110  2.283457    GNB           10         42.0  \n",
       "56     0.0     1.0  0.197025  2.095157    LDA           10         42.0  \n",
       "57     0.0     1.0  0.159353  2.501585    QDA           10         42.0  \n",
       "58     0.0     1.0  0.222219  2.300669    SVM           10         42.0  \n",
       "59     0.0     1.0  0.191388  2.922385    KNN           10         42.0  \n",
       "\n",
       "[60 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### ONLY for DNN experiment to find aug factor: ##############################\n",
    "##############  output: array of factor+ predicted probability list by user_test+ trueth list + mse  ############\n",
    "############### this section follows the function defination sections(before data process section)  ###############\n",
    "\n",
    "\n",
    "\n",
    "''' B.DNN '''\n",
    "import time\n",
    "###############  Training ,prediction #################################\n",
    "def dnn_predict_score_user(X_train, y_train_dummy, epochs):\n",
    "    epochs = epochs\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu')) # input_dim = numb of features, 1D number\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))  # 5 classes\n",
    "#     model.summary()\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam',loss='mse', metrics=['mae', 'acc']) \n",
    "    history=model.fit(X_train, y_train_dummy, epochs=epochs,validation_split=0.33, batch_size=32, verbose = 0)\n",
    "\n",
    "    y_predict = model.predict(X_test)\n",
    "    y_predicted = y_predict.argmax(axis = 1)\n",
    "    y_t = y_test_dummy.argmax(axis =1)\n",
    "\n",
    "    predicted_df = pd.DataFrame(y_predicted, index =X_test.index) # dataframe of predected label + user index\n",
    "    predictions_temp, score_user  = classifier_slice2uid(predicted_df, trigger= trigger) # predictions output in df formate , by user id\n",
    "    predictions =  predictions_temp.iloc[:,-1].to_list()  # convert to array \n",
    "    y_t = np.array(phq_score.loc[list(predictions_temp.index),\"dp_class_y\"].to_list())  # pickup the associated test's depression label by user\n",
    "    mse = np.sum((y_t-list(score_user.values()))**2/len(y_t))\n",
    "    return score_user, y_t, mse\n",
    "\n",
    "\n",
    "#############  pick up features， see what's picked in ML algorithm portion ###########################\n",
    "\n",
    "\n",
    "# 0. all features\n",
    "col_slct= full_col # ALL features, 78 cols after manully filter \"null\" and \"green\" columns\n",
    "\n",
    "# # 1. EMA 6 + sensing 4\n",
    "col_slct=['Mood_happyornot','PAM_picture_idx', 'Mood_sad','Stress_level','Sleep_social','Sleep_rate',\n",
    "            'dark_duration(sec)','audio_ audio inference',\n",
    "#             'phonecharge_duration(sec)','activity_ activity inference'] \n",
    "\n",
    "# # # 2. EMA  6 features\n",
    "# col_slct=['Mood_happyornot','PAM_picture_idx', 'Mood_sad','Stress_level','Sleep_social','Sleep_rate']\n",
    "\n",
    "# # # # 3. sensing 4 features\n",
    "# col_slct=['dark_duration(sec)','audio_ audio inference',\n",
    "#           'phonecharge_duration(sec)','activity_ activity inference'] \n",
    "\n",
    "\n",
    "''' Data processing 1:\n",
    "    - select columns\n",
    "    - normalization\n",
    "    - PCA (on demand)\n",
    "'''\n",
    "\n",
    "dataset_col = dataset[col_slct]\n",
    "\n",
    "data_norm= minmax_normalize(dataset_col)\n",
    "data_norm=data_norm.fillna(0) # choose -1 or 0\n",
    "\n",
    "## process PCA ############\n",
    "# dataset_np = data_norm.iloc[:,1:].to_numpy()  # transfer to numpy without index of 'user_date'\n",
    "# K=2\n",
    "# dataset_np=PCA(dataset_np, K, axis= 1)\n",
    "# dataset_np.shape\n",
    "# dataset_pca=pd.DataFrame(dataset_np, columns= np.arange(K), index= data_norm.index)\n",
    "\n",
    "\n",
    "# print(\"Below is data_norm:\")\n",
    "# data_norm\n",
    "\n",
    "''' Data processing 2:\n",
    "    - add lables\n",
    "    - clean data\n",
    "    '''\n",
    "\n",
    "\n",
    "########### 1.2 add label(dp_class) by merging phq ###############\n",
    "########## add uer and set as index for merging phq ###########\n",
    "\n",
    "\n",
    "# dataset_1=dataset   # no pca, no normalization\n",
    "dataset_1 = data_norm # with normalization , no pca\n",
    "# dataset_1= dataset_pca # with both normalization + pca\n",
    "\n",
    "dataset_1['user']=dataset_1.index  # copy 'user' column with index values\n",
    "def str_slice(str):\n",
    "    return str[:3]\n",
    "dataset_1['user']=dataset_1['user'].apply(str_slice)\n",
    "dataset_1 = dataset_1.reset_index().set_index('user') \n",
    "dataset_1 = dataset_1.drop(['user_date'], axis=1)# keep the index columns and set 'user' col as index\n",
    "# dataset_1.fillna(0, inplace= True)\n",
    "print(\"Dataset before labeling is:\", dataset_1.shape)\n",
    "\n",
    "## add label of dp_class\n",
    "dp_class_list=[]\n",
    "for user in dataset_1.index: \n",
    "    if user in phq_selected.index.to_list():\n",
    "        dp_class_list.append(phq_selected.loc[user]['dp_class_y'])\n",
    "    else:\n",
    "        dp_class_list.append(None) # for removing those users are not in the selected list\n",
    "dataset_1['dp_class']=dp_class_list\n",
    "\n",
    "dataset_1= dataset_1.dropna() #dataset_1= dataset_1.dropna(how='all')  will cause big issue\n",
    "dataset_1= dataset_1.fillna(0)\n",
    "\n",
    "# dataset_2 = minmax_normalize(dataset_1)\n",
    "print(\"After labeling and removing non-completed label, dataset shape is:\", dataset_1.shape)\n",
    "\n",
    "''' Data processing 3:\n",
    "    - Split training and testing dataset\n",
    "    - Data augment : pick the id with depression ,which stays in train dataset for augmenting'''\n",
    "\n",
    "# process training and testing data ########\n",
    "random.seed(seed_value)  # 1 depression :42, 0 ,3 ; 0 depression: 2,5, 6. see what's in ML portion\n",
    "trigger =0.2 # threshold for 2nd round classification\n",
    "dataset_full= dataset_1 #11/15 after norm\n",
    "id_list =list(np.unique(dataset_full.index))\n",
    "split = int(0.8*len(id_list))\n",
    "\n",
    "\n",
    "id_train= sorted(random.sample(id_list,split))\n",
    "id_test= sorted(list(set(id_list)- set(id_train)))\n",
    "\n",
    "id_dptrain=[]\n",
    "id_alldepression=['u17', 'u18','u23','u33','u52'] # users who has depression\n",
    "for id in id_alldepression:\n",
    "    if (id in id_train):\n",
    "        id_dptrain.append(id)  #add to augment id list for depression id in train dataset\n",
    "# print(id_dptrain)\n",
    "###############  start iteration ###################\n",
    "import time\n",
    "\n",
    "p_dnn =  pd.DataFrame()\n",
    "# nondepression_idnum_train=len(np.unique(y_train.index))-len(id_dptrain) # none depression id number in training\n",
    "nondepression_idnum_train=len(np.unique(y_train.index))\n",
    "\n",
    "for j in range(nondepression_idnum_train):       \n",
    "    p_temp=[]    \n",
    "    \n",
    "    agm_factor = j # add 4 times depressed user data, randomly        \n",
    "    id_augment = []\n",
    "    # for i in range(len(id_dptrain)* agm_factor): # add 4X users with depression \n",
    "    for i in range(agm_factor): # add users in train dataset with depression     \n",
    "        id_augment.append(random.sample(id_dptrain,1)[0])\n",
    "\n",
    "    id_agm =id_train+id_augment\n",
    "    random.shuffle(id_agm)\n",
    "\n",
    "    X_train= dataset_1.loc[id_agm,:].iloc[:,:-1] # X_train shape[0] will increase after augmenting\n",
    "    y_train= dataset_1.loc[id_agm,:].iloc[:,-1]\n",
    "#     print(\"id_augment\", id_agm)\n",
    "\n",
    "    X_test= dataset_1.loc[id_test,:].iloc[:,:-1]\n",
    "    y_test= dataset_1.loc[id_test,:].iloc[:,-1]\n",
    "    y_train_dummy = pd.get_dummies(y_train).values\n",
    "    y_test_dummy = pd.get_dummies(y_test).values\n",
    "\n",
    "    score_user, y_t, mse = dnn_predict_score_user(X_train, y_train_dummy, 200) # train and predict with CNN ================\n",
    "    p_temp.append(int(time.time()))\n",
    "    p_temp.append(j)\n",
    "    p_temp.extend(list(score_user.values()))\n",
    "    p_temp.extend(y_t)\n",
    "    p_temp.append(mse)\n",
    "    p_temp.append(entropy(list(score_user.values())))\n",
    "    p_temp.append(\"DNN\")\n",
    "    p_temp.append(len(col_slct))\n",
    "    p_temp.append(seed_value)\n",
    "#     print(\"score by user\", score_user)\n",
    "#     print(\"yt\",y_t)\n",
    "#     print(\"mse\",mse)\n",
    "    \n",
    "    p_series =  pd.Series(p_temp)\n",
    "    p_dnn = p_dnn.append(p_series, ignore_index=True)\n",
    "p_col = [\"time\",\"aug_factor\"]+list(score_user.keys())*2+[\"mse\",\"entropy\",\"model\",\"feature num\",\"random seed\"]\n",
    "p_dnn.columns= p_col\n",
    "p_dnn['feature num']=p_dnn['feature num'].astype(int) #format the feature type\n",
    "p_dnn['aug_factor']=p_dnn['aug_factor'].astype(int)\n",
    "# p_dnn.to_csv(dir_data+\"\"entropy_probability.csv\")\n",
    "p_dnn.to_csv(dir_data+\"entropy_probability.csv\", mode=\"a\")\n",
    "print(\"DNN entropy report saved:\", dir_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
